{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyO1Rp6Q/Qe1yCLFW2qq0J0z",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aliahmedd24/Predictive-Maintenance-LSTM/blob/main/pipeline_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install wandb pandas numpy scikit-learn statsmodels scipy torch pmdarima confluent-kafka pyarrow matplotlib seaborn joblib\n",
        "\n",
        "import os\n",
        "import sys\n",
        "from datetime import datetime\n",
        "\n",
        "# Create project structure\n",
        "!mkdir -p /content/predictive-maintenance/{src,data,configs,features,artifacts}\n",
        "!mkdir -p /content/predictive-maintenance/src/{utils,ingestion,cleaning,windowing,features,scaling,monitoring,model_input,model}\n",
        "\n",
        "# Change to project directory\n",
        "os.chdir('/content/predictive-maintenance')\n",
        "sys.path.append('/content/predictive-maintenance')"
      ],
      "metadata": {
        "id": "w7Ig5WVQzXGK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile requirements.txt\n",
        "wandb>=0.16.0\n",
        "pandas>=1.5.0\n",
        "numpy>=1.23.0\n",
        "scikit-learn>=1.3.0\n",
        "statsmodels>=0.14.0\n",
        "scipy>=1.10.0\n",
        "torch>=2.0.0\n",
        "pmdarima>=2.0.0\n",
        "confluent-kafka>=2.0.0\n",
        "pyarrow>=10.0.0\n",
        "matplotlib>=3.6.0\n",
        "seaborn>=0.12.0\n",
        "joblib>=1.2.0"
      ],
      "metadata": {
        "id": "AVitXRuz1qK-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile configs/wandb.yaml\n",
        "project: predictive-maintenance\n",
        "entity: aliahmedibrahim24906-constructor-university\n",
        "tags:\n",
        "  - time-series\n",
        "  - sensor-data\n",
        "  - preprocessing\n",
        "settings:\n",
        "  console: \"off\"\n",
        "  quiet: true"
      ],
      "metadata": {
        "id": "Wlo1DgU12T23"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile src/wandb_init.py\n",
        "import wandb\n",
        "import yaml\n",
        "import os\n",
        "from typing import Dict, Any\n",
        "\n",
        "def init_wandb(config_path: str = \"configs/wandb.yaml\", run_name: str = None) -> wandb.run:\n",
        "    \"\"\"Initialize W&B with project configuration.\"\"\"\n",
        "\n",
        "    # Load config\n",
        "    with open(config_path, 'r') as f:\n",
        "        config = yaml.safe_load(f)\n",
        "\n",
        "    # Set entity from environment if not specified\n",
        "    if 'entity' not in config or config['entity'] == '${WANDB_ENTITY}':\n",
        "        config['entity'] = os.getenv('WANDB_ENTITY', 'default-entity')\n",
        "\n",
        "    # Initialize run\n",
        "    run = wandb.init(\n",
        "        project=config['project'],\n",
        "        entity=config['entity'],\n",
        "        name=run_name,\n",
        "        tags=config.get('tags', []),\n",
        "        config=config.get('settings', {})\n",
        "    )\n",
        "\n",
        "    return run\n",
        "\n",
        "# Login with API key\n",
        "def setup_wandb_colab():\n",
        "    \"\"\"Setup W&B for Google Colab environment.\"\"\"\n",
        "    from google.colab import userdata\n",
        "    import wandb\n",
        "\n",
        "    # Try to get API key from Colab secrets\n",
        "    try:\n",
        "        wandb_key = userdata.get('WANDB_API_KEY')\n",
        "        wandb.login(key=wandb_key)\n",
        "    except:\n",
        "        # Fallback to manual login\n",
        "        wandb.login()\n",
        "\n",
        "    return True"
      ],
      "metadata": {
        "id": "Gi44yN7x2ZfG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile src/ingestion/loader.py\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import wandb\n",
        "import argparse\n",
        "import logging\n",
        "from pathlib import Path\n",
        "from typing import List, Dict, Union\n",
        "import pyarrow.parquet as pq\n",
        "\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "class DataLoader:\n",
        "    \"\"\"Handles data ingestion and time alignment with W&B logging.\"\"\"\n",
        "\n",
        "    def __init__(self, wandb_run: wandb.run = None):\n",
        "        self.run = wandb_run or wandb.init(project=\"predictive-maintenance\")\n",
        "\n",
        "    def load_csv_files(self, file_paths: List[str], timestamp_col: str = 'timestamp') -> pd.DataFrame:\n",
        "        \"\"\"Load and concatenate multiple CSV files.\"\"\"\n",
        "        dfs = []\n",
        "\n",
        "        for path in file_paths:\n",
        "            logger.info(f\"Loading {path}\")\n",
        "            df = pd.read_csv(path)\n",
        "\n",
        "            # Parse timestamp\n",
        "            df[timestamp_col] = pd.to_datetime(df[timestamp_col])\n",
        "            df = df.set_index(timestamp_col)\n",
        "\n",
        "            # Log file stats\n",
        "            self.run.log({\n",
        "                f\"file_{Path(path).stem}_rows\": len(df),\n",
        "                f\"file_{Path(path).stem}_cols\": len(df.columns)\n",
        "            })\n",
        "\n",
        "            dfs.append(df)\n",
        "\n",
        "        # Combine all dataframes\n",
        "        combined_df = pd.concat(dfs, axis=0)\n",
        "        combined_df = combined_df.sort_index()\n",
        "\n",
        "        return combined_df\n",
        "\n",
        "    def resample_to_frequency(self, df: pd.DataFrame, freq: str = '1T') -> pd.DataFrame:\n",
        "        \"\"\"Resample data to uniform frequency.\"\"\"\n",
        "        logger.info(f\"Resampling to {freq} frequency\")\n",
        "\n",
        "        # Log original stats\n",
        "        self.run.log({\n",
        "            \"original_rows\": len(df),\n",
        "            \"original_start\": df.index.min().isoformat(),\n",
        "            \"original_end\": df.index.max().isoformat()\n",
        "        })\n",
        "\n",
        "        # Resample numeric columns with mean, categorical with mode\n",
        "        numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
        "        categorical_cols = df.select_dtypes(include=['object', 'category']).columns\n",
        "\n",
        "        resampled_numeric = df[numeric_cols].resample(freq).mean()\n",
        "\n",
        "        if len(categorical_cols) > 0:\n",
        "            resampled_categorical = df[categorical_cols].resample(freq).agg(lambda x: x.mode()[0] if len(x) > 0 else np.nan)\n",
        "            resampled_df = pd.concat([resampled_numeric, resampled_categorical], axis=1)\n",
        "        else:\n",
        "            resampled_df = resampled_numeric\n",
        "\n",
        "        # Log resampled stats\n",
        "        self.run.log({\n",
        "            \"resampled_rows\": len(resampled_df),\n",
        "            \"resampling_freq\": freq,\n",
        "            \"data_loss_ratio\": 1 - (len(resampled_df) / len(df))\n",
        "        })\n",
        "\n",
        "        return resampled_df\n",
        "\n",
        "    def save_and_log_artifact(self, df: pd.DataFrame, output_path: str = \"data/unified.parquet\"):\n",
        "        \"\"\"Save DataFrame to Parquet and log as W&B artifact.\"\"\"\n",
        "        # Create directory if needed\n",
        "        Path(output_path).parent.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "        # Save to parquet\n",
        "        df.to_parquet(output_path, engine='pyarrow')\n",
        "        logger.info(f\"Saved unified data to {output_path}\")\n",
        "\n",
        "        # Create and log W&B artifact\n",
        "        artifact = wandb.Artifact(\"raw-data\", type=\"dataset\")\n",
        "        artifact.add_file(output_path)\n",
        "\n",
        "        # Add metadata\n",
        "        artifact.metadata = {\n",
        "            \"rows\": len(df),\n",
        "            \"columns\": list(df.columns),\n",
        "            \"start_time\": df.index.min().isoformat(),\n",
        "            \"end_time\": df.index.max().isoformat(),\n",
        "            \"frequency\": pd.infer_freq(df.index) or \"irregular\"\n",
        "        }\n",
        "\n",
        "        self.run.log_artifact(artifact)\n",
        "        logger.info(\"Logged raw-data artifact to W&B\")\n",
        "\n",
        "def main():\n",
        "    parser = argparse.ArgumentParser(description=\"Ingest and align time-series data\")\n",
        "    parser.add_argument(\"--input-files\", nargs=\"+\", required=True, help=\"Input CSV files\")\n",
        "    parser.add_argument(\"--timestamp-col\", default=\"timestamp\", help=\"Timestamp column name\")\n",
        "    parser.add_argument(\"--resample-freq\", default=\"1T\", help=\"Resampling frequency\")\n",
        "    parser.add_argument(\"--output-path\", default=\"data/unified.parquet\", help=\"Output path\")\n",
        "\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    # Initialize W&B\n",
        "    run = wandb.init(project=\"predictive-maintenance\", job_type=\"data-ingestion\")\n",
        "\n",
        "    # Process data\n",
        "    loader = DataLoader(run)\n",
        "    df = loader.load_csv_files(args.input_files, args.timestamp_col)\n",
        "    df = loader.resample_to_frequency(df, args.resample_freq)\n",
        "    loader.save_and_log_artifact(df, args.output_path)\n",
        "\n",
        "    run.finish()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "-ye7VF6Q26yH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile src/cleaning/quality_checks.py\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import wandb\n",
        "import json\n",
        "import logging\n",
        "from pathlib import Path\n",
        "from typing import Dict, Tuple, Any\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "class QualityChecker:\n",
        "    \"\"\"Performs initial data quality checks with W&B logging.\"\"\"\n",
        "\n",
        "    def __init__(self, wandb_run: wandb.run = None):\n",
        "        self.run = wandb_run or wandb.init(project=\"predictive-maintenance\")\n",
        "\n",
        "    def load_limits(self, limits_path: str = \"configs/limits.json\") -> Dict[str, Dict[str, float]]:\n",
        "        \"\"\"Load sensor limits configuration.\"\"\"\n",
        "        try:\n",
        "            with open(limits_path, 'r') as f:\n",
        "                return json.load(f)\n",
        "        except FileNotFoundError:\n",
        "            logger.warning(f\"Limits file not found at {limits_path}, using defaults\")\n",
        "            return {}\n",
        "\n",
        "    def remove_duplicates(self, df: pd.DataFrame) -> pd.DataFrame:\n",
        "        \"\"\"Remove duplicate timestamps.\"\"\"\n",
        "        initial_rows = len(df)\n",
        "        df = df[~df.index.duplicated(keep='first')]\n",
        "\n",
        "        duplicates_removed = initial_rows - len(df)\n",
        "        self.run.log({\"duplicates_removed\": duplicates_removed})\n",
        "        logger.info(f\"Removed {duplicates_removed} duplicate rows\")\n",
        "\n",
        "        return df\n",
        "\n",
        "    def filter_by_limits(self, df: pd.DataFrame, limits: Dict[str, Dict[str, float]]) -> pd.DataFrame:\n",
        "        \"\"\"Filter values outside physical limits.\"\"\"\n",
        "        out_of_spec_counts = {}\n",
        "\n",
        "        for col in df.columns:\n",
        "            if col in limits:\n",
        "                min_val = limits[col].get('min', -np.inf)\n",
        "                max_val = limits[col].get('max', np.inf)\n",
        "\n",
        "                # Count out-of-spec values\n",
        "                out_of_spec = (df[col] < min_val) | (df[col] > max_val)\n",
        "                out_of_spec_counts[col] = out_of_spec.sum()\n",
        "\n",
        "                # Replace with NaN\n",
        "                df.loc[out_of_spec, col] = np.nan\n",
        "\n",
        "        # Log out-of-spec counts\n",
        "        for col, count in out_of_spec_counts.items():\n",
        "            self.run.log({f\"out_of_spec_{col}\": count})\n",
        "\n",
        "        return df\n",
        "\n",
        "    def profile_missingness(self, df: pd.DataFrame) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
        "        \"\"\"Profile missing values and generate report.\"\"\"\n",
        "        missingness_report = pd.DataFrame({\n",
        "            'column': df.columns,\n",
        "            'missing_count': df.isnull().sum().values,\n",
        "            'missing_rate': (df.isnull().sum() / len(df)).values,\n",
        "            'data_type': df.dtypes.astype(str).values\n",
        "        })\n",
        "\n",
        "        # Sort by missing rate\n",
        "        missingness_report = missingness_report.sort_values('missing_rate', ascending=False)\n",
        "\n",
        "        # Log missing rates to W&B\n",
        "        missing_rate_dict = dict(zip(\n",
        "            missingness_report['column'],\n",
        "            missingness_report['missing_rate']\n",
        "        ))\n",
        "\n",
        "        # Log as table and metrics\n",
        "        table = wandb.Table(dataframe=missingness_report)\n",
        "        self.run.log({\"missingness_table\": table})\n",
        "\n",
        "        for col, rate in missing_rate_dict.items():\n",
        "            self.run.log({f\"missing_rate_{col}\": rate})\n",
        "\n",
        "        # Log summary statistics\n",
        "        self.run.log({\n",
        "            \"avg_missing_rate\": missingness_report['missing_rate'].mean(),\n",
        "            \"max_missing_rate\": missingness_report['missing_rate'].max(),\n",
        "            \"columns_with_missing\": (missingness_report['missing_rate'] > 0).sum()\n",
        "        })\n",
        "\n",
        "        return df, missingness_report\n",
        "\n",
        "    def save_quality_artifacts(self, df: pd.DataFrame, report: pd.DataFrame,\n",
        "                              output_dir: str = \"data\"):\n",
        "        \"\"\"Save cleaned data and quality report as W&B artifacts.\"\"\"\n",
        "        # Save report\n",
        "        report_path = f\"{output_dir}/missingness_report.csv\"\n",
        "        report.to_csv(report_path, index=False)\n",
        "\n",
        "        # Create quality metrics artifact\n",
        "        artifact = wandb.Artifact(\"quality-metrics\", type=\"dataset\")\n",
        "        artifact.add_file(report_path)\n",
        "\n",
        "        # Add cleaned data stats to metadata\n",
        "        artifact.metadata = {\n",
        "            \"total_rows\": len(df),\n",
        "            \"total_columns\": len(df.columns),\n",
        "            \"avg_missing_rate\": float(report['missing_rate'].mean()),\n",
        "            \"columns_with_missing\": int((report['missing_rate'] > 0).sum())\n",
        "        }\n",
        "\n",
        "        self.run.log_artifact(artifact)\n",
        "        logger.info(\"Logged quality-metrics artifact to W&B\")\n",
        "\n",
        "        return df\n",
        "\n",
        "def main():\n",
        "    import argparse\n",
        "\n",
        "    parser = argparse.ArgumentParser(description=\"Perform quality checks on time-series data\")\n",
        "    parser.add_argument(\"--input-artifact\", default=\"raw-data:latest\", help=\"Input W&B artifact\")\n",
        "    parser.add_argument(\"--limits-file\", default=\"configs/limits.json\", help=\"Sensor limits file\")\n",
        "    parser.add_argument(\"--output-dir\", default=\"data\", help=\"Output directory\")\n",
        "\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    # Initialize W&B\n",
        "    run = wandb.init(project=\"predictive-maintenance\", job_type=\"quality-checks\")\n",
        "\n",
        "    # Load data from artifact\n",
        "    artifact = run.use_artifact(args.input_artifact)\n",
        "    artifact_dir = artifact.download()\n",
        "\n",
        "    # Find parquet file\n",
        "    parquet_files = list(Path(artifact_dir).glob(\"*.parquet\"))\n",
        "    if not parquet_files:\n",
        "        raise ValueError(\"No parquet files found in artifact\")\n",
        "\n",
        "    df = pd.read_parquet(parquet_files[0])\n",
        "\n",
        "    # Run quality checks\n",
        "    checker = QualityChecker(run)\n",
        "\n",
        "    # Load limits\n",
        "    # limits = checker.load_limits(args.limits_file)\n",
        "\n",
        "    # Apply checks\n",
        "    df = checker.remove_duplicates(df)\n",
        "    df = checker.filter_by_limits(df, limits)\n",
        "    df, report = checker.profile_missingness(df)\n",
        "\n",
        "    # Save results\n",
        "    checker.save_quality_artifacts(df, report, args.output_dir)\n",
        "\n",
        "    # Save cleaned data\n",
        "    df.to_parquet(f\"{args.output_dir}/cleaned.parquet\")\n",
        "\n",
        "    run.finish()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "XlMOKdux3N3P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''%%writefile configs/limits.json\n",
        "{\n",
        "    \"sensor_0\": {\"min\": 0, \"max\": 200},\n",
        "    \"sensor_1\": {\"min\": 10, \"max\": 210},\n",
        "    \"sensor_2\": {\"min\": 20, \"max\": 220},\n",
        "    \"sensor_3\": {\"min\": 30, \"max\": 230},\n",
        "    \"sensor_4\": {\"min\": 40, \"max\": 240}\n",
        "}'''"
      ],
      "metadata": {
        "id": "IcjIM1DR3e1d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile src/utils/sensor_utils.py\n",
        "\"\"\"\n",
        "Centralized utility for identifying sensor columns as ANY numeric column.\n",
        "This replaces the hardcoded 'sensor_' prefix requirement throughout the codebase.\n",
        "\"\"\"\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from typing import List, Union\n",
        "\n",
        "\n",
        "def get_sensor_columns(df: pd.DataFrame,\n",
        "                      exclude_columns: List[str] = None,\n",
        "                      ensure_numeric: bool = True) -> List[str]:\n",
        "    \"\"\"\n",
        "    Identify sensor columns as ANY numeric column in the dataframe.\n",
        "\n",
        "    This function replaces all hardcoded sensor column identification throughout\n",
        "    the codebase to make it more flexible and not dependent on naming conventions.\n",
        "\n",
        "    Args:\n",
        "        df: Input dataframe\n",
        "        exclude_columns: List of column names to exclude from sensor identification\n",
        "        ensure_numeric: If True, only return columns with numeric dtypes\n",
        "\n",
        "    Returns:\n",
        "        List of column names identified as sensor columns\n",
        "    \"\"\"\n",
        "    if exclude_columns is None:\n",
        "        exclude_columns = [\n",
        "            'timestamp', 'time', 'datetime', 'date',\n",
        "            'window_id', 'window_start', 'window_end',\n",
        "            'label', 'target', 'class', 'category',\n",
        "            'id', 'index', 'row_id', 'sample_id',\n",
        "            'step', 'phase', 'stage', 'mode',\n",
        "            'quality', 'confidence', 'flag', 'status'\n",
        "        ]\n",
        "\n",
        "    # Get all column names\n",
        "    all_columns = df.columns.tolist()\n",
        "\n",
        "    # Filter out excluded columns (case insensitive and partial matching)\n",
        "    sensor_candidates = []\n",
        "    for col in all_columns:\n",
        "        col_lower = col.lower()\n",
        "        # Exclude if any excluded term is found in the column name\n",
        "        exclude = False\n",
        "        for excluded_term in exclude_columns:\n",
        "            if excluded_term.lower() in col_lower:\n",
        "                exclude = True\n",
        "                break\n",
        "\n",
        "        # Also exclude columns ending with metadata suffixes\n",
        "        metadata_suffixes = ['_quality', '_confidence', '_flag', '_gap_flag',\n",
        "                           '_is_spike', '_low_confidence', '_method', '_status']\n",
        "        for suffix in metadata_suffixes:\n",
        "            if col.endswith(suffix):\n",
        "                exclude = True\n",
        "                break\n",
        "\n",
        "        if not exclude:\n",
        "            sensor_candidates.append(col)\n",
        "\n",
        "    # If ensure_numeric is True, filter to only numeric columns\n",
        "    if ensure_numeric:\n",
        "        numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
        "        sensor_columns = [col for col in sensor_candidates if col in numeric_cols]\n",
        "    else:\n",
        "        sensor_columns = sensor_candidates\n",
        "\n",
        "    return sensor_columns\n",
        "\n",
        "\n",
        "def get_sensor_columns_safe(df: pd.DataFrame,\n",
        "                           exclude_columns: List[str] = None,\n",
        "                           fallback_pattern: str = None) -> List[str]:\n",
        "    \"\"\"\n",
        "    Safe version of get_sensor_columns with fallback to pattern matching.\n",
        "\n",
        "    Args:\n",
        "        df: Input dataframe\n",
        "        exclude_columns: List of column names to exclude\n",
        "        fallback_pattern: Fallback pattern to match if no numeric columns found\n",
        "\n",
        "    Returns:\n",
        "        List of column names identified as sensor columns\n",
        "    \"\"\"\n",
        "    sensor_cols = get_sensor_columns(df, exclude_columns)\n",
        "\n",
        "    # If no sensor columns found and fallback pattern provided\n",
        "    if not sensor_cols and fallback_pattern:\n",
        "        sensor_cols = [col for col in df.columns if fallback_pattern in col.lower()]\n",
        "\n",
        "    return sensor_cols\n",
        "\n",
        "\n",
        "def validate_sensor_columns(df: pd.DataFrame,\n",
        "                          sensor_cols: List[str],\n",
        "                          min_valid_ratio: float = 0.1) -> List[str]:\n",
        "    \"\"\"\n",
        "    Validate that sensor columns have sufficient non-null data for processing.\n",
        "\n",
        "    Args:\n",
        "        df: Input dataframe\n",
        "        sensor_cols: List of sensor column names\n",
        "        min_valid_ratio: Minimum ratio of non-null values required\n",
        "\n",
        "    Returns:\n",
        "        List of validated sensor column names\n",
        "    \"\"\"\n",
        "    validated_cols = []\n",
        "\n",
        "    for col in sensor_cols:\n",
        "        if col in df.columns:\n",
        "            valid_ratio = df[col].notna().sum() / len(df)\n",
        "            if valid_ratio >= min_valid_ratio:\n",
        "                validated_cols.append(col)\n",
        "\n",
        "    return validated_cols\n",
        "\n",
        "\n",
        "def get_feature_columns(df: pd.DataFrame,\n",
        "                       exclude_metadata: bool = True) -> List[str]:\n",
        "    \"\"\"\n",
        "    Get feature columns for machine learning (excludes IDs and metadata).\n",
        "\n",
        "    Args:\n",
        "        df: Input dataframe\n",
        "        exclude_metadata: Whether to exclude metadata columns\n",
        "\n",
        "    Returns:\n",
        "        List of feature column names suitable for ML\n",
        "    \"\"\"\n",
        "    # Start with all sensor columns\n",
        "    feature_cols = get_sensor_columns(df)\n",
        "\n",
        "    if exclude_metadata:\n",
        "        # Remove additional metadata columns that might be features\n",
        "        metadata_patterns = ['_id', '_start', '_end', '_timestamp']\n",
        "        feature_cols = [col for col in feature_cols\n",
        "                       if not any(pattern in col.lower() for pattern in metadata_patterns)]\n",
        "\n",
        "    return feature_cols\n",
        "\n",
        "\n",
        "def log_sensor_column_info(df: pd.DataFrame,\n",
        "                          sensor_cols: List[str],\n",
        "                          wandb_run=None) -> dict:\n",
        "    \"\"\"\n",
        "    Log information about identified sensor columns for debugging and monitoring.\n",
        "\n",
        "    Args:\n",
        "        df: Input dataframe\n",
        "        sensor_cols: List of identified sensor columns\n",
        "        wandb_run: Optional wandb run for logging\n",
        "\n",
        "    Returns:\n",
        "        Dictionary with sensor column statistics\n",
        "    \"\"\"\n",
        "    stats = {\n",
        "        'total_columns': len(df.columns),\n",
        "        'sensor_columns': len(sensor_cols),\n",
        "        'sensor_percentage': len(sensor_cols) / len(df.columns) * 100,\n",
        "        'sensor_column_names': sensor_cols[:20],  # First 20 for logging\n",
        "        'excluded_columns': [col for col in df.columns if col not in sensor_cols][:10]\n",
        "    }\n",
        "\n",
        "    # Calculate data availability for sensor columns\n",
        "    if sensor_cols:\n",
        "        sensor_data = df[sensor_cols]\n",
        "        stats.update({\n",
        "            'avg_null_percentage': sensor_data.isnull().mean().mean() * 100,\n",
        "            'min_null_percentage': sensor_data.isnull().mean().min() * 100,\n",
        "            'max_null_percentage': sensor_data.isnull().mean().max() * 100,\n",
        "            'total_sensor_values': sensor_data.size,\n",
        "            'total_null_values': sensor_data.isnull().sum().sum(),\n",
        "        })\n",
        "\n",
        "    if wandb_run:\n",
        "        wandb_run.log({\n",
        "            'sensor_identification_total_columns': stats['total_columns'],\n",
        "            'sensor_identification_sensor_count': stats['sensor_columns'],\n",
        "            'sensor_identification_percentage': stats['sensor_percentage'],\n",
        "            'sensor_identification_avg_null_pct': stats.get('avg_null_percentage', 0)\n",
        "        })\n",
        "\n",
        "    return stats"
      ],
      "metadata": {
        "id": "BRv9UovKcvS1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile src/cleaning/time_features.py\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import wandb\n",
        "import matplotlib.pyplot as plt\n",
        "from typing import Dict, List\n",
        "\n",
        "class TimeFeatureEngineer:\n",
        "    \"\"\"Add time-based features with W&B logging.\"\"\"\n",
        "\n",
        "    def __init__(self, wandb_run: wandb.run = None):\n",
        "        self.run = wandb_run or wandb.init(project=\"predictive-maintenance\")\n",
        "\n",
        "    def add_cyclical_features(self, df: pd.DataFrame) -> pd.DataFrame:\n",
        "        \"\"\"Add sin/cos encoded time features.\"\"\"\n",
        "        # Hour of day\n",
        "        df['hour'] = df.index.hour\n",
        "        df['hour_sin'] = np.sin(2 * np.pi * df['hour'] / 24)\n",
        "        df['hour_cos'] = np.cos(2 * np.pi * df['hour'] / 24)\n",
        "\n",
        "        # Day of week\n",
        "        df['day_of_week'] = df.index.dayofweek\n",
        "        df['dow_sin'] = np.sin(2 * np.pi * df['day_of_week'] / 7)\n",
        "        df['dow_cos'] = np.cos(2 * np.pi * df['day_of_week'] / 7)\n",
        "\n",
        "        # Day of month\n",
        "        df['day_of_month'] = df.index.day\n",
        "        df['dom_sin'] = np.sin(2 * np.pi * df['day_of_month'] / 31)\n",
        "        df['dom_cos'] = np.cos(2 * np.pi * df['day_of_month'] / 31)\n",
        "\n",
        "        # Month of year\n",
        "        df['month'] = df.index.month\n",
        "        df['month_sin'] = np.sin(2 * np.pi * df['month'] / 12)\n",
        "        df['month_cos'] = np.cos(2 * np.pi * df['month'] / 12)\n",
        "\n",
        "        # Log feature ranges\n",
        "        for feat in ['hour_sin', 'hour_cos', 'dow_sin', 'dow_cos', 'dom_sin', 'dom_cos', 'month_sin', 'month_cos']:\n",
        "            self.run.log({\n",
        "                f\"{feat}_min\": df[feat].min(),\n",
        "                f\"{feat}_max\": df[feat].max(),\n",
        "                f\"{feat}_mean\": df[feat].mean()\n",
        "            })\n",
        "\n",
        "        return df\n",
        "\n",
        "    def add_shift_labels(self, df: pd.DataFrame) -> pd.DataFrame:\n",
        "        \"\"\"Add shift labels based on hour.\"\"\"\n",
        "        def get_shift(hour):\n",
        "            if 6 <= hour < 14:\n",
        "                return 'morning'\n",
        "            elif 14 <= hour < 22:\n",
        "                return 'evening'\n",
        "            else:\n",
        "                return 'night'\n",
        "\n",
        "        df['shift'] = df.index.hour.map(get_shift)\n",
        "\n",
        "        # Log shift distribution\n",
        "        shift_counts = df['shift'].value_counts().to_dict()\n",
        "        for shift, count in shift_counts.items():\n",
        "            self.run.log({f\"shift_{shift}_count\": count})\n",
        "\n",
        "        # One-hot encode shifts\n",
        "        shift_dummies = pd.get_dummies(df['shift'], prefix='shift')\n",
        "        df = pd.concat([df, shift_dummies], axis=1)\n",
        "\n",
        "        return df\n",
        "\n",
        "    def log_feature_distributions(self, df: pd.DataFrame):\n",
        "        \"\"\"Log histograms and distributions to W&B.\"\"\"\n",
        "        # Create histograms for cyclical features\n",
        "        fig, axes = plt.subplots(2, 4, figsize=(16, 8))\n",
        "        axes = axes.ravel()\n",
        "\n",
        "        cyclical_features = ['hour_sin', 'hour_cos', 'dow_sin', 'dow_cos',\n",
        "                           'dom_sin', 'dom_cos', 'month_sin', 'month_cos']\n",
        "\n",
        "        for idx, feat in enumerate(cyclical_features):\n",
        "            axes[idx].hist(df[feat].dropna(), bins=50, alpha=0.7, color='skyblue', edgecolor='black')\n",
        "            axes[idx].set_title(f'Distribution of {feat}')\n",
        "            axes[idx].set_xlabel('Value')\n",
        "            axes[idx].set_ylabel('Frequency')\n",
        "\n",
        "            # Add vertical lines for mean and median\n",
        "            mean_val = df[feat].mean()\n",
        "            median_val = df[feat].median()\n",
        "            axes[idx].axvline(mean_val, color='red', linestyle='--', alpha=0.7, label=f'Mean: {mean_val:.2f}')\n",
        "            axes[idx].axvline(median_val, color='green', linestyle='--', alpha=0.7, label=f'Median: {median_val:.2f}')\n",
        "            axes[idx].legend()\n",
        "\n",
        "        plt.tight_layout()\n",
        "        self.run.log({\"cyclical_features_distribution\": wandb.Image(fig)})\n",
        "        plt.close()\n",
        "\n",
        "        # Create shift distribution pie chart\n",
        "        fig, ax = plt.subplots(figsize=(8, 8))\n",
        "        shift_counts = df['shift'].value_counts()\n",
        "        ax.pie(shift_counts.values, labels=shift_counts.index, autopct='%1.1f%%', startangle=90)\n",
        "        ax.set_title('Distribution of Shifts')\n",
        "        self.run.log({\"shift_distribution\": wandb.Image(fig)})\n",
        "        plt.close()\n",
        "\n",
        "        # Log feature statistics table\n",
        "        feature_stats = []\n",
        "        for feat in cyclical_features:\n",
        "            stats = {\n",
        "                'feature': feat,\n",
        "                'mean': df[feat].mean(),\n",
        "                'std': df[feat].std(),\n",
        "                'min': df[feat].min(),\n",
        "                'max': df[feat].max(),\n",
        "                'nulls': df[feat].isnull().sum()\n",
        "            }\n",
        "            feature_stats.append(stats)\n",
        "\n",
        "        stats_table = wandb.Table(dataframe=pd.DataFrame(feature_stats))\n",
        "        self.run.log({\"time_feature_statistics\": stats_table})\n",
        "\n",
        "    def add_time_features(self, df: pd.DataFrame) -> pd.DataFrame:\n",
        "        \"\"\"Main method to add all time features.\"\"\"\n",
        "        # Ensure we have a datetime index\n",
        "        if not isinstance(df.index, pd.DatetimeIndex):\n",
        "            raise ValueError(\"DataFrame must have a DatetimeIndex\")\n",
        "\n",
        "        # Add features\n",
        "        df = self.add_cyclical_features(df)\n",
        "        df = self.add_shift_labels(df)\n",
        "\n",
        "        # Log distributions\n",
        "        self.log_feature_distributions(df)\n",
        "\n",
        "        # Log total features added\n",
        "        time_features = ['hour', 'hour_sin', 'hour_cos', 'day_of_week', 'dow_sin', 'dow_cos',\n",
        "                        'day_of_month', 'dom_sin', 'dom_cos', 'month', 'month_sin', 'month_cos',\n",
        "                        'shift', 'shift_morning', 'shift_evening', 'shift_night']\n",
        "\n",
        "        self.run.log({\"time_features_added\": len(time_features)})\n",
        "\n",
        "        return df\n",
        "\n",
        "def main():\n",
        "    import argparse\n",
        "\n",
        "    parser = argparse.ArgumentParser(description=\"Add time features to sensor data\")\n",
        "    parser.add_argument(\"--input-file\", default=\"data/cleaned.parquet\", help=\"Input parquet file\")\n",
        "    parser.add_argument(\"--output-file\", default=\"data/time_features.parquet\", help=\"Output file\")\n",
        "\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    # Initialize W&B\n",
        "    run = wandb.init(project=\"predictive-maintenance\", job_type=\"feature-engineering\")\n",
        "\n",
        "    # Load data\n",
        "    df = pd.read_parquet(args.input_file)\n",
        "\n",
        "    # Add time features\n",
        "    engineer = TimeFeatureEngineer(run)\n",
        "    df = engineer.add_time_features(df)\n",
        "\n",
        "    # Save enhanced data\n",
        "    df.to_parquet(args.output_file)\n",
        "\n",
        "    # Log as artifact\n",
        "    artifact = wandb.Artifact(\"time-features\", type=\"dataset\")\n",
        "    artifact.add_file(args.output_file)\n",
        "    run.log_artifact(artifact)\n",
        "\n",
        "    run.finish()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "SEwt-Xy23ykN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile src/cleaning/impute.py\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import wandb\n",
        "from sklearn.experimental import enable_iterative_imputer\n",
        "from sklearn.impute import IterativeImputer\n",
        "from scipy import stats\n",
        "from typing import Dict, List, Tuple\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Import the new sensor column utility\n",
        "from src.utils.sensor_utils import get_sensor_columns, log_sensor_column_info, validate_sensor_columns\n",
        "\n",
        "\n",
        "class MissingValueImputer:\n",
        "    \"\"\"\n",
        "    ROBUST Missing Value Imputation with comprehensive gap handling.\n",
        "\n",
        "    Now uses DYNAMIC sensor column identification (any numeric column) instead of\n",
        "    hardcoded 'sensor_' prefix requirement.\n",
        "\n",
        "    Strategy:\n",
        "    1. Short gaps (≤ short_limit): Linear interpolation\n",
        "    2. Medium gaps (≤ medium_limit): Seasonal pattern matching\n",
        "    3. Long gaps: Multiple fallback strategies\n",
        "    4. Remaining NaNs: Forward fill, backward fill, then median\n",
        "\n",
        "    This ensures NO NaN values remain for downstream processing.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, wandb_run: wandb.run = None):\n",
        "        self.run = wandb_run or wandb.init(project=\"predictive-maintenance\")\n",
        "        self.gap_stats = {}\n",
        "        self.imputation_methods = {}\n",
        "        self.sensor_statistics = {}\n",
        "\n",
        "    def calculate_sensor_statistics(self, df: pd.DataFrame) -> Dict:\n",
        "        \"\"\"Pre-calculate statistics for each sensor for robust imputation.\"\"\"\n",
        "        stats = {}\n",
        "        # Use new dynamic sensor column identification\n",
        "        sensor_cols = get_sensor_columns(df)\n",
        "\n",
        "        # Log sensor identification info\n",
        "        log_sensor_column_info(df, sensor_cols, self.run)\n",
        "\n",
        "        for col in sensor_cols:\n",
        "            non_null_data = df[col].dropna()\n",
        "            if len(non_null_data) > 0:\n",
        "                stats[col] = {\n",
        "                    'mean': float(non_null_data.mean()),\n",
        "                    'median': float(non_null_data.median()),\n",
        "                    'std': float(non_null_data.std()),\n",
        "                    'min': float(non_null_data.min()),\n",
        "                    'max': float(non_null_data.max()),\n",
        "                    'q25': float(non_null_data.quantile(0.25)),\n",
        "                    'q75': float(non_null_data.quantile(0.75)),\n",
        "                    'valid_count': len(non_null_data),\n",
        "                    'null_count': df[col].isnull().sum(),\n",
        "                    'null_percentage': float(df[col].isnull().sum() / len(df) * 100)\n",
        "                }\n",
        "            else:\n",
        "                # Completely null column - use global statistics\n",
        "                stats[col] = {\n",
        "                    'mean': 0.0,\n",
        "                    'median': 0.0,\n",
        "                    'std': 1.0,\n",
        "                    'min': -1.0,\n",
        "                    'max': 1.0,\n",
        "                    'q25': -0.5,\n",
        "                    'q75': 0.5,\n",
        "                    'valid_count': 0,\n",
        "                    'null_count': len(df),\n",
        "                    'null_percentage': 100.0\n",
        "                }\n",
        "\n",
        "        self.sensor_statistics = stats\n",
        "        return stats\n",
        "\n",
        "    def identify_gap_segments(self, series: pd.Series) -> List[Dict]:\n",
        "        \"\"\"Identify continuous gaps in time series for targeted imputation.\"\"\"\n",
        "        null_mask = series.isnull()\n",
        "\n",
        "        # Find gap start and end points\n",
        "        gap_starts = null_mask & ~null_mask.shift(1).fillna(False)\n",
        "        gap_ends = null_mask & ~null_mask.shift(-1).fillna(False)\n",
        "\n",
        "        gaps = []\n",
        "        start_indices = series[gap_starts].index\n",
        "        end_indices = series[gap_ends].index\n",
        "\n",
        "        for start_idx, end_idx in zip(start_indices, end_indices):\n",
        "            gap_length = (series.index.get_loc(end_idx) - series.index.get_loc(start_idx)) + 1\n",
        "            gaps.append({\n",
        "                'start_idx': start_idx,\n",
        "                'end_idx': end_idx,\n",
        "                'length': gap_length,\n",
        "                'start_pos': series.index.get_loc(start_idx),\n",
        "                'end_pos': series.index.get_loc(end_idx)\n",
        "            })\n",
        "\n",
        "        return gaps\n",
        "\n",
        "    def iterative_impute(self, df: pd.DataFrame, sensor_cols: List[str],\n",
        "                        max_iter: int = 5) -> pd.DataFrame:\n",
        "        \"\"\"Apply multivariate iterative imputation for sensor columns.\"\"\"\n",
        "        try:\n",
        "            # Only use columns with reasonable amount of data\n",
        "            valid_cols = validate_sensor_columns(df, sensor_cols, min_valid_ratio=0.3)\n",
        "\n",
        "            if len(valid_cols) < 2:\n",
        "                self.run.log({\"multivariate_skip_reason\": \"insufficient_valid_columns\"})\n",
        "                return df\n",
        "\n",
        "            # Apply iterative imputation\n",
        "            imputer = IterativeImputer(\n",
        "                max_iter=max_iter,\n",
        "                random_state=42,\n",
        "                sample_posterior=False\n",
        "            )\n",
        "\n",
        "            imputed_data = imputer.fit_transform(df[valid_cols])\n",
        "            df[valid_cols] = imputed_data\n",
        "\n",
        "            self.run.log({\n",
        "                \"multivariate_imputation_applied\": True,\n",
        "                \"multivariate_columns_processed\": len(valid_cols)\n",
        "            })\n",
        "\n",
        "        except Exception as e:\n",
        "            self.run.log({\n",
        "                \"multivariate_imputation_failed\": str(e),\n",
        "                \"multivariate_fallback\": \"univariate_only\"\n",
        "            })\n",
        "\n",
        "        return df\n",
        "\n",
        "    def robust_impute_series(self, series: pd.Series, col_name: str,\n",
        "                           short_limit: int = 5, medium_limit: int = 60) -> pd.Series:\n",
        "        \"\"\"Robust imputation strategy for a single sensor column.\"\"\"\n",
        "        imputed = series.copy()\n",
        "        original_nulls = series.isnull().sum()\n",
        "        methods_used = []\n",
        "\n",
        "        if original_nulls == 0:\n",
        "            return imputed\n",
        "\n",
        "        # Initialize gap tracking\n",
        "        if col_name not in self.gap_stats:\n",
        "            self.gap_stats[col_name] = {\n",
        "                'total_gaps': 0,\n",
        "                'short_gaps': 0,\n",
        "                'medium_gaps': 0,\n",
        "                'long_gaps': 0\n",
        "            }\n",
        "\n",
        "        # Get sensor statistics\n",
        "        sensor_stats = self.sensor_statistics.get(col_name, {})\n",
        "\n",
        "        # Identify gap segments\n",
        "        gaps = self.identify_gap_segments(series)\n",
        "\n",
        "        for gap in gaps:\n",
        "            gap_length = gap['length']\n",
        "            start_pos = gap['start_pos']\n",
        "            end_pos = gap['end_pos']\n",
        "\n",
        "            # Update gap statistics\n",
        "            self.gap_stats[col_name]['total_gaps'] += 1\n",
        "\n",
        "            if gap_length <= short_limit:\n",
        "                # SHORT GAPS: Linear interpolation\n",
        "                self.gap_stats[col_name]['short_gaps'] += 1\n",
        "                imputed.iloc[start_pos:end_pos+1] = imputed.iloc[start_pos:end_pos+1].interpolate(method='linear')\n",
        "                methods_used.append('linear_interp')\n",
        "\n",
        "            elif gap_length <= medium_limit:\n",
        "                # MEDIUM GAPS: Seasonal/time-based interpolation\n",
        "                self.gap_stats[col_name]['medium_gaps'] += 1\n",
        "\n",
        "                # Try time-based interpolation if index is datetime\n",
        "                if isinstance(imputed.index, pd.DatetimeIndex):\n",
        "                    imputed.iloc[start_pos:end_pos+1] = imputed.iloc[start_pos:end_pos+1].interpolate(method='time')\n",
        "                    methods_used.append('time_interp')\n",
        "                else:\n",
        "                    # Fallback to spline interpolation\n",
        "                    try:\n",
        "                        imputed.iloc[start_pos:end_pos+1] = imputed.iloc[start_pos:end_pos+1].interpolate(method='spline', order=2)\n",
        "                        methods_used.append('spline_interp')\n",
        "                    except:\n",
        "                        imputed.iloc[start_pos:end_pos+1] = imputed.iloc[start_pos:end_pos+1].interpolate(method='linear')\n",
        "                        methods_used.append('linear_fallback')\n",
        "\n",
        "            else:\n",
        "                # LONG GAPS: Multiple strategies\n",
        "                self.gap_stats[col_name]['long_gaps'] += 1\n",
        "\n",
        "                # Strategy 1: Try polynomial interpolation for shorter long gaps\n",
        "                if gap_length <= 200:\n",
        "                    try:\n",
        "                        imputed.iloc[start_pos:end_pos+1] = imputed.iloc[start_pos:end_pos+1].interpolate(method='polynomial', order=3)\n",
        "                        methods_used.append('polynomial_interp')\n",
        "                    except:\n",
        "                        # Fallback to median\n",
        "                        median_val = sensor_stats.get('median', 0.0)\n",
        "                        imputed.iloc[start_pos:end_pos+1] = median_val\n",
        "                        methods_used.append('median_fill')\n",
        "                else:\n",
        "                    # Strategy 2: Bounded random walk for very long gaps\n",
        "                    median_val = sensor_stats.get('median', 0.0)\n",
        "                    std_val = sensor_stats.get('std', 1.0)\n",
        "\n",
        "                    # Generate bounded random walk\n",
        "                    random_values = np.random.normal(median_val, std_val * 0.5, gap_length)\n",
        "                    # Bound the values to reasonable range\n",
        "                    q25, q75 = sensor_stats.get('q25', median_val - std_val), sensor_stats.get('q75', median_val + std_val)\n",
        "                    random_values = np.clip(random_values, q25, q75)\n",
        "\n",
        "                    imputed.iloc[start_pos:end_pos+1] = random_values\n",
        "                    methods_used.append('bounded_random')\n",
        "\n",
        "        # FINAL CLEANUP: Handle any remaining NaNs\n",
        "        remaining_nulls = imputed.isnull().sum()\n",
        "        if remaining_nulls > 0:\n",
        "            # Forward fill\n",
        "            imputed = imputed.fillna(method='ffill')\n",
        "            if imputed.isnull().sum() > 0:\n",
        "                methods_used.append('forward_fill')\n",
        "\n",
        "            # Backward fill\n",
        "            imputed = imputed.fillna(method='bfill')\n",
        "            if imputed.isnull().sum() > 0:\n",
        "                methods_used.append('backward_fill')\n",
        "\n",
        "            # Final fallback to median\n",
        "            if imputed.isnull().sum() > 0:\n",
        "                median_val = sensor_stats.get('median', 0.0)\n",
        "                imputed = imputed.fillna(median_val)\n",
        "                methods_used.append('zero_fill')\n",
        "\n",
        "        final_nulls = imputed.isnull().sum()\n",
        "\n",
        "        # Store methods used\n",
        "        self.imputation_methods[col_name] = '+'.join(methods_used) if methods_used else 'none'\n",
        "\n",
        "        # Log per-column statistics\n",
        "        self.run.log({\n",
        "            f'imputation_{col_name}_original_nulls': int(original_nulls),\n",
        "            f'imputation_{col_name}_final_nulls': int(final_nulls),\n",
        "            f'imputation_{col_name}_filled_points': int(original_nulls - final_nulls),\n",
        "            f'imputation_{col_name}_null_percentage': float(original_nulls / len(series) * 100),\n",
        "            f'imputation_{col_name}_methods_used': self.imputation_methods[col_name]\n",
        "        })\n",
        "\n",
        "        return imputed\n",
        "\n",
        "    def impute_missing(self, df: pd.DataFrame,\n",
        "                      short_limit: int = 5,\n",
        "                      medium_limit: int = 60,\n",
        "                      use_multivariate: bool = True,\n",
        "                      drop_threshold: float = 80.0) -> pd.DataFrame:\n",
        "        \"\"\"\n",
        "        Main imputation method - drops high-missing columns then ensures ALL remaining missing values are handled.\n",
        "\n",
        "        Args:\n",
        "            df: Input dataframe\n",
        "            short_limit: Max gap for linear interpolation\n",
        "            medium_limit: Max gap for time-based interpolation\n",
        "            use_multivariate: Whether to try multivariate imputation first\n",
        "            drop_threshold: Drop columns with more than this percentage of missing values (default 80%)\n",
        "\n",
        "        Returns:\n",
        "            DataFrame with NO missing values in remaining sensor columns\n",
        "        \"\"\"\n",
        "        # Log initial state\n",
        "        initial_shape = df.shape\n",
        "\n",
        "        # Use new dynamic sensor column identification\n",
        "        sensor_cols = get_sensor_columns(df)\n",
        "        initial_nulls = df[sensor_cols].isnull().sum().sum() if sensor_cols else 0\n",
        "\n",
        "        self.run.log({\n",
        "            \"imputation_initial_shape\": str(initial_shape),\n",
        "            \"imputation_initial_nulls\": int(initial_nulls),\n",
        "            \"imputation_initial_sensor_count\": len(sensor_cols),\n",
        "            \"sensor_identification_method\": \"dynamic_numeric_columns\"\n",
        "        })\n",
        "\n",
        "        if not sensor_cols:\n",
        "            self.run.log({\"imputation_warning\": \"No sensor columns found using dynamic identification\"})\n",
        "            return df\n",
        "\n",
        "        # Calculate sensor statistics for all sensors\n",
        "        self.calculate_sensor_statistics(df)\n",
        "\n",
        "        # STEP 1: DROP columns with > drop_threshold% missing values\n",
        "        columns_to_drop = []\n",
        "        columns_to_keep = []\n",
        "        dropped_stats = {}\n",
        "\n",
        "        for col in sensor_cols:\n",
        "            null_percentage = self.sensor_statistics[col]['null_percentage']\n",
        "            if null_percentage > drop_threshold:\n",
        "                columns_to_drop.append(col)\n",
        "                dropped_stats[col] = {\n",
        "                    'null_percentage': null_percentage,\n",
        "                    'null_count': self.sensor_statistics[col]['null_count'],\n",
        "                    'valid_count': self.sensor_statistics[col]['valid_count']\n",
        "                }\n",
        "            else:\n",
        "                columns_to_keep.append(col)\n",
        "\n",
        "        # Drop high-missing columns\n",
        "        if columns_to_drop:\n",
        "            df = df.drop(columns=columns_to_drop)\n",
        "\n",
        "            # Log dropped columns\n",
        "            self.run.log({\n",
        "                \"columns_dropped\": len(columns_to_drop),\n",
        "                \"columns_dropped_list\": \", \".join(columns_to_drop[:20]),  # First 20\n",
        "                \"columns_retained\": len(columns_to_keep),\n",
        "                \"drop_threshold_used\": drop_threshold\n",
        "            })\n",
        "\n",
        "            # Create table of dropped columns\n",
        "            if dropped_stats:\n",
        "                dropped_df = pd.DataFrame.from_dict(dropped_stats, orient='index')\n",
        "                dropped_df.index.name = 'sensor'\n",
        "                dropped_df.reset_index(inplace=True)\n",
        "                dropped_table = wandb.Table(dataframe=dropped_df.head(50))\n",
        "                self.run.log({\"dropped_columns_details\": dropped_table})\n",
        "\n",
        "            print(f\"Dropped {len(columns_to_drop)} columns with >{drop_threshold}% missing values\")\n",
        "            print(f\"Retaining {len(columns_to_keep)} columns for imputation\")\n",
        "\n",
        "        # Update sensor_cols to only include retained columns\n",
        "        sensor_cols = columns_to_keep\n",
        "\n",
        "        if not sensor_cols:\n",
        "            self.run.log({\"imputation_warning\": \"No sensor columns remaining after dropping high-missing columns\"})\n",
        "            return df\n",
        "\n",
        "        # Recalculate nulls after dropping\n",
        "        remaining_nulls = df[sensor_cols].isnull().sum().sum()\n",
        "\n",
        "        self.run.log({\n",
        "            \"nulls_after_dropping\": int(remaining_nulls),\n",
        "            \"imputation_working_sensor_count\": len(sensor_cols),\n",
        "            \"columns_removed_percentage\": (len(columns_to_drop) / (len(columns_to_drop) + len(sensor_cols)) * 100) if columns_to_drop else 0\n",
        "        })\n",
        "\n",
        "        # Log sensors with moderate missing rates (for awareness)\n",
        "        moderate_missing_sensors = [\n",
        "            col for col in sensor_cols\n",
        "            if 50 < self.sensor_statistics[col]['null_percentage'] <= drop_threshold\n",
        "        ]\n",
        "        if moderate_missing_sensors:\n",
        "            self.run.log({\n",
        "                \"moderate_missing_sensors\": len(moderate_missing_sensors),\n",
        "                \"moderate_missing_sensor_list\": \", \".join(moderate_missing_sensors[:10])\n",
        "            })\n",
        "\n",
        "        # Step 2: Try multivariate imputation for remaining sensors\n",
        "        if use_multivariate and len(sensor_cols) > 1:\n",
        "            df = self.iterative_impute(df, sensor_cols)\n",
        "\n",
        "        # Step 3: Robust univariate imputation for each remaining sensor\n",
        "        for col in sensor_cols:\n",
        "            if df[col].isnull().any():\n",
        "                df[col] = self.robust_impute_series(\n",
        "                    df[col],\n",
        "                    col,\n",
        "                    short_limit=short_limit,\n",
        "                    medium_limit=medium_limit\n",
        "                )\n",
        "\n",
        "        # Step 4: Verify no NaNs remain in retained sensors\n",
        "        remaining_nulls = df[sensor_cols].isnull().sum().sum()\n",
        "        if remaining_nulls > 0:\n",
        "            # Emergency fallback - should never reach here\n",
        "            self.run.log({\"emergency_fallback_triggered\": True})\n",
        "            for col in sensor_cols:\n",
        "                if df[col].isnull().any():\n",
        "                    # Use sensor mean or 0\n",
        "                    fallback_value = self.sensor_statistics.get(col, {}).get('mean', 0)\n",
        "                    df[col] = df[col].fillna(fallback_value)\n",
        "\n",
        "        # Final verification\n",
        "        final_nulls = df[sensor_cols].isnull().sum().sum()\n",
        "\n",
        "        # Log results\n",
        "        self.run.log({\n",
        "            \"imputation_final_nulls\": int(final_nulls),\n",
        "            \"imputation_filled_total\": int(remaining_nulls),\n",
        "            \"imputation_success\": final_nulls == 0,\n",
        "            \"imputation_methods_summary\": self.get_methods_summary(),\n",
        "            \"final_sensor_count\": len(sensor_cols),\n",
        "        })\n",
        "\n",
        "        # Create detailed statistics\n",
        "        self.log_imputation_stats()\n",
        "\n",
        "        # Add imputation quality flags (for downstream awareness)\n",
        "        df = self.add_imputation_quality_flags(df, sensor_cols)\n",
        "\n",
        "        # Store list of dropped columns as metadata\n",
        "        if columns_to_drop:\n",
        "            df.attrs['dropped_columns'] = columns_to_drop\n",
        "            df.attrs['drop_threshold'] = drop_threshold\n",
        "\n",
        "        return df\n",
        "\n",
        "    def add_imputation_quality_flags(self, df: pd.DataFrame, sensor_cols: List[str]) -> pd.DataFrame:\n",
        "        \"\"\"\n",
        "        Add quality flags to indicate imputation confidence.\n",
        "        These flags can be used downstream for weighted processing.\n",
        "        \"\"\"\n",
        "        for col in sensor_cols:\n",
        "            if col in self.gap_stats:\n",
        "                stats = self.gap_stats[col]\n",
        "                original_null_pct = stats.get('null_percentage', 0)\n",
        "\n",
        "                # Create quality flag: 1.0 = no imputation, 0.0 = 100% imputed\n",
        "                quality_score = 1.0 - (original_null_pct / 100.0)\n",
        "\n",
        "                # Add as metadata column\n",
        "                df[f\"{col}_quality\"] = quality_score\n",
        "\n",
        "                # Add method flag\n",
        "                method = self.imputation_methods.get(col, 'none')\n",
        "                if 'bounded_random' in method or 'zero_fill' in method:\n",
        "                    df[f\"{col}_low_confidence\"] = True\n",
        "                else:\n",
        "                    df[f\"{col}_low_confidence\"] = False\n",
        "\n",
        "        return df\n",
        "\n",
        "    def get_methods_summary(self) -> Dict[str, int]:\n",
        "        \"\"\"Summarize imputation methods used.\"\"\"\n",
        "        summary = {}\n",
        "        for methods in self.imputation_methods.values():\n",
        "            for method in methods.split('+'):\n",
        "                if method:\n",
        "                    summary[method] = summary.get(method, 0) + 1\n",
        "        return summary\n",
        "\n",
        "    def log_imputation_stats(self):\n",
        "        \"\"\"Log comprehensive imputation statistics.\"\"\"\n",
        "        # Create gap statistics table\n",
        "        gap_data = []\n",
        "        for col, stats in self.gap_stats.items():\n",
        "            gap_data.append({\n",
        "                'sensor': col,\n",
        "                'total_gaps': stats['total_gaps'],\n",
        "                'short_gaps': stats['short_gaps'],\n",
        "                'medium_gaps': stats['medium_gaps'],\n",
        "                'long_gaps': stats['long_gaps']\n",
        "            })\n",
        "\n",
        "        if gap_data:\n",
        "            gap_df = pd.DataFrame(gap_data)\n",
        "            gap_table = wandb.Table(dataframe=gap_df)\n",
        "            self.run.log({\"gap_statistics\": gap_table})\n",
        "\n",
        "        # Log methods summary\n",
        "        methods_summary = self.get_methods_summary()\n",
        "        for method, count in methods_summary.items():\n",
        "            self.run.log({f\"method_count_{method}\": count})\n",
        "\n",
        "    def create_imputation_visualization(self):\n",
        "        \"\"\"Create visualization of imputation impact.\"\"\"\n",
        "        if not self.gap_stats:\n",
        "            return\n",
        "\n",
        "        # Prepare data for visualization\n",
        "        sensors = list(self.gap_stats.keys())[:20]  # First 20 sensors\n",
        "        gap_counts = [self.gap_stats[s]['total_gaps'] for s in sensors]\n",
        "        filled_points = [sum([\n",
        "            self.gap_stats[s]['short_gaps'],\n",
        "            self.gap_stats[s]['medium_gaps'],\n",
        "            self.gap_stats[s]['long_gaps']\n",
        "        ]) for s in sensors]\n",
        "\n",
        "        # Create visualization\n",
        "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
        "\n",
        "        # Gap counts by sensor\n",
        "        ax1.bar(range(len(sensors)), gap_counts)\n",
        "        ax1.set_xlabel('Sensor')\n",
        "        ax1.set_ylabel('Number of gaps')\n",
        "        ax1.set_title('Gap Counts by Sensor')\n",
        "        ax1.set_xticks(range(len(sensors)))\n",
        "        ax1.set_xticklabels(sensors, rotation=45)\n",
        "\n",
        "        # Points filled vs flagged\n",
        "        x = np.arange(len(sensors))\n",
        "        width = 0.35\n",
        "        flagged = [sum([\n",
        "            self.gap_stats[s].get('flagged_points', 0)\n",
        "        ]) for s in sensors]\n",
        "\n",
        "        ax2.bar(x - width/2, filled_points, width, label='Points filled')\n",
        "        ax2.bar(x + width/2, flagged, width, label='Points flagged')\n",
        "        ax2.set_xlabel('Sensor')\n",
        "        ax2.set_ylabel('Number of points')\n",
        "        ax2.set_title('Imputation Impact by Sensor')\n",
        "        ax2.set_xticks(x)\n",
        "        ax2.set_xticklabels(sensors, rotation=45)\n",
        "        ax2.legend()\n",
        "\n",
        "        plt.tight_layout()\n",
        "        self.run.log({\"imputation_visualization\": wandb.Image(fig)})\n",
        "        plt.close()\n",
        "\n",
        "\n",
        "def main():\n",
        "    import argparse\n",
        "\n",
        "    parser = argparse.ArgumentParser(description=\"Impute missing values in sensor data\")\n",
        "    parser.add_argument(\"--input-file\", default=\"data/time_features.parquet\", help=\"Input file\")\n",
        "    parser.add_argument(\"--output-file\", default=\"data/imputed.parquet\", help=\"Output file\")\n",
        "    parser.add_argument(\"--short-limit\", type=int, default=5, help=\"Max gap for linear interpolation\")\n",
        "    parser.add_argument(\"--medium-limit\", type=int, default=60, help=\"Max gap for time-based interpolation\")\n",
        "    parser.add_argument(\"--drop-threshold\", type=float, default=80.0, help=\"Drop columns with >% missing\")\n",
        "    parser.add_argument(\"--use-multivariate\", action=\"store_true\", help=\"Use multivariate imputation\")\n",
        "\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    # Initialize W&B\n",
        "    run = wandb.init(project=\"predictive-maintenance\", job_type=\"imputation\")\n",
        "\n",
        "    # Load data\n",
        "    print(f\"Loading data from {args.input_file}...\")\n",
        "    df = pd.read_parquet(args.input_file)\n",
        "\n",
        "    # Log initial information\n",
        "    print(f\"Initial shape: {df.shape}\")\n",
        "\n",
        "    # Use new dynamic sensor identification\n",
        "    from src.utils.sensor_utils import get_sensor_columns\n",
        "    sensor_cols = get_sensor_columns(df)\n",
        "    print(f\"Number of sensor columns (dynamic identification): {len(sensor_cols)}\")\n",
        "\n",
        "    if sensor_cols:\n",
        "        initial_nulls = df[sensor_cols].isnull().sum().sum()\n",
        "        print(f\"Total missing values in sensors: {initial_nulls:,}\")\n",
        "        print(f\"Average missing rate: {initial_nulls/(len(df)*len(sensor_cols))*100:.2f}%\")\n",
        "\n",
        "    # Perform robust imputation with column dropping\n",
        "    print(f\"\\nDropping columns with >{args.drop_threshold}% missing values...\")\n",
        "    print(\"Then performing robust imputation on remaining columns...\")\n",
        "\n",
        "    imputer = MissingValueImputer(run)\n",
        "    df = imputer.impute_missing(\n",
        "        df,\n",
        "        short_limit=args.short_limit,\n",
        "        medium_limit=args.medium_limit,\n",
        "        use_multivariate=args.use_multivariate,\n",
        "        drop_threshold=args.drop_threshold\n",
        "    )\n",
        "\n",
        "    # Verify results\n",
        "    print(\"\\nImputation complete!\")\n",
        "    print(f\"Final shape: {df.shape}\")\n",
        "\n",
        "    # Get remaining sensor columns using dynamic identification\n",
        "    remaining_sensor_cols = get_sensor_columns(df)\n",
        "    print(f\"Remaining sensor columns: {len(remaining_sensor_cols)}\")\n",
        "\n",
        "    if remaining_sensor_cols:\n",
        "        final_nulls = df[remaining_sensor_cols].isnull().sum().sum()\n",
        "        print(f\"Remaining NaNs in sensors: {final_nulls}\")\n",
        "\n",
        "        if final_nulls == 0:\n",
        "            print(\"✅ SUCCESS: All missing values have been imputed in retained columns!\")\n",
        "        else:\n",
        "            print(\"⚠️ WARNING: Some NaN values remain (this should not happen)\")\n",
        "\n",
        "    # Report dropped columns if any\n",
        "    if hasattr(df, 'attrs') and 'dropped_columns' in df.attrs:\n",
        "        dropped = df.attrs['dropped_columns']\n",
        "        print(f\"\\n📊 Dropped {len(dropped)} columns due to >{args.drop_threshold}% missing values\")\n",
        "        if len(dropped) <= 10:\n",
        "            print(f\"   Dropped columns: {', '.join(dropped)}\")\n",
        "        else:\n",
        "            print(f\"   Dropped columns (first 10): {', '.join(dropped[:10])}...\")\n",
        "\n",
        "    # Save results\n",
        "    print(f\"\\nSaving to {args.output_file}...\")\n",
        "    df.to_parquet(args.output_file)\n",
        "\n",
        "    # Log as artifact\n",
        "    artifact = wandb.Artifact(\"robust-imputed-data\", type=\"dataset\")\n",
        "    artifact.add_file(args.output_file)\n",
        "    artifact.metadata = {\n",
        "        \"short_gap_limit\": args.short_limit,\n",
        "        \"medium_gap_limit\": args.medium_limit,\n",
        "        \"drop_threshold\": args.drop_threshold,\n",
        "        \"multivariate_used\": args.use_multivariate,\n",
        "        \"final_shape\": df.shape,\n",
        "        \"initial_sensors\": len(sensor_cols),\n",
        "        \"final_sensors\": len(remaining_sensor_cols),\n",
        "        \"columns_dropped\": len(sensor_cols) - len(remaining_sensor_cols),\n",
        "        \"quality_flags_added\": len([col for col in df.columns if col.endswith('_quality')]),\n",
        "        \"confidence_flags_added\": len([col for col in df.columns if col.endswith('_low_confidence')]),\n",
        "        \"sensor_nulls_remaining\": 0,  # Should always be 0\n",
        "        \"sensor_identification_method\": \"dynamic_numeric_columns\"\n",
        "    }\n",
        "    run.log_artifact(artifact)\n",
        "\n",
        "    run.finish()\n",
        "    print(\"Done!\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "0VVlOGpyIg07"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''%%writefile src/cleaning/impute.py\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import wandb\n",
        "from typing import Dict, Tuple\n",
        "\n",
        "class MissingValueImputer:\n",
        "    \"\"\"Handle missing values with intelligent imputation and W&B logging.\"\"\"\n",
        "\n",
        "    def __init__(self, wandb_run: wandb.run = None):\n",
        "        self.run = wandb_run or wandb.init(project=\"predictive-maintenance\")\n",
        "        self.gap_stats = {}\n",
        "\n",
        "    def identify_gaps(self, series: pd.Series) -> Dict[str, list]:\n",
        "        \"\"\"Identify continuous gaps in a series.\"\"\"\n",
        "        is_null = series.isnull()\n",
        "        gap_groups = (is_null != is_null.shift()).cumsum()\n",
        "\n",
        "        gaps = []\n",
        "        for gap_id in gap_groups[is_null].unique():\n",
        "            gap_mask = (gap_groups == gap_id) & is_null\n",
        "            gap_length = gap_mask.sum()\n",
        "            gap_start = gap_mask.idxmax()\n",
        "            gaps.append({\n",
        "                'start': gap_start,\n",
        "                'length': gap_length,\n",
        "                'gap_id': gap_id\n",
        "            })\n",
        "\n",
        "        return gaps\n",
        "\n",
        "    def impute_series(self, series: pd.Series, short_limit: int = 5) -> Tuple[pd.Series, pd.Series]:\n",
        "        \"\"\"Impute a single series and return imputed series + gap flags.\"\"\"\n",
        "        gaps = self.identify_gaps(series)\n",
        "        gap_flag = pd.Series(False, index=series.index, name=f\"{series.name}_gap_flag\")\n",
        "\n",
        "        # Statistics\n",
        "        total_gaps = len(gaps)\n",
        "        short_gaps = sum(1 for g in gaps if g['length'] <= short_limit)\n",
        "        long_gaps = total_gaps - short_gaps\n",
        "\n",
        "        # Copy series for imputation\n",
        "        imputed = series.copy()\n",
        "\n",
        "        # Process each gap\n",
        "        for gap in gaps:\n",
        "            gap_slice = slice(gap['start'], gap['start'] + pd.Timedelta(minutes=gap['length']-1))\n",
        "\n",
        "            if gap['length'] <= short_limit:\n",
        "                # Interpolate short gaps\n",
        "                imputed[gap_slice] = imputed[gap_slice].interpolate(method='time')\n",
        "            else:\n",
        "                # Flag long gaps\n",
        "                gap_flag[gap_slice] = True\n",
        "\n",
        "        # Store stats\n",
        "        self.gap_stats[series.name] = {\n",
        "            'total_gaps': total_gaps,\n",
        "            'short_gaps': short_gaps,\n",
        "            'long_gaps': long_gaps,\n",
        "            'filled_points': (series.isnull() & ~imputed.isnull()).sum(),\n",
        "            'flagged_points': gap_flag.sum()\n",
        "        }\n",
        "\n",
        "        return imputed, gap_flag\n",
        "\n",
        "    def impute_missing(self, df: pd.DataFrame, short_limit: int = 5) -> pd.DataFrame:\n",
        "        \"\"\"Impute missing values in entire dataframe.\"\"\"\n",
        "        # Separate sensor columns from time features\n",
        "        sensor_cols = [col for col in df.columns if col.startswith('sensor_')]\n",
        "\n",
        "        # Store gap flags\n",
        "        gap_flags = pd.DataFrame(index=df.index)\n",
        "\n",
        "        # Impute each sensor column\n",
        "        for col in sensor_cols:\n",
        "            df[col], gap_flag = self.impute_series(df[col], short_limit)\n",
        "            gap_flags[f\"{col}_gap_flag\"] = gap_flag\n",
        "\n",
        "        # Add gap flags to dataframe\n",
        "        df = pd.concat([df, gap_flags], axis=1)\n",
        "\n",
        "        # Log imputation statistics\n",
        "        self.log_imputation_stats()\n",
        "\n",
        "        return df\n",
        "\n",
        "    def log_imputation_stats(self):\n",
        "        \"\"\"Log detailed imputation statistics to W&B.\"\"\"\n",
        "        # Aggregate statistics\n",
        "        total_filled = sum(stats['filled_points'] for stats in self.gap_stats.values())\n",
        "        total_flagged = sum(stats['flagged_points'] for stats in self.gap_stats.values())\n",
        "        total_gaps = sum(stats['total_gaps'] for stats in self.gap_stats.values())\n",
        "\n",
        "        # Log overall metrics\n",
        "        self.run.log({\n",
        "            \"total_gaps_found\": total_gaps,\n",
        "            \"total_points_filled\": total_filled,\n",
        "            \"total_points_flagged\": total_flagged,\n",
        "            \"filled_gaps_ratio\": total_filled / (total_filled + total_flagged) if (total_filled + total_flagged) > 0 else 0,\n",
        "            \"long_gaps_ratio\": total_flagged / (total_filled + total_flagged) if (total_filled + total_flagged) > 0 else 0\n",
        "        })\n",
        "\n",
        "        # Log per-sensor metrics\n",
        "        for sensor, stats in self.gap_stats.items():\n",
        "            self.run.log({\n",
        "                f\"{sensor}_gaps\": stats['total_gaps'],\n",
        "                f\"{sensor}_filled\": stats['filled_points'],\n",
        "                f\"{sensor}_flagged\": stats['flagged_points']\n",
        "            })\n",
        "\n",
        "        # Create detailed table\n",
        "        stats_df = pd.DataFrame.from_dict(self.gap_stats, orient='index')\n",
        "        stats_df.index.name = 'sensor'\n",
        "        stats_df.reset_index(inplace=True)\n",
        "\n",
        "        table = wandb.Table(dataframe=stats_df)\n",
        "        self.run.log({\"imputation_details\": table})\n",
        "\n",
        "        # Create visualization\n",
        "        import matplotlib.pyplot as plt\n",
        "\n",
        "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))\n",
        "\n",
        "        # Gap distribution\n",
        "        sensors = list(self.gap_stats.keys())\n",
        "        short_gaps = [self.gap_stats[s]['short_gaps'] for s in sensors]\n",
        "        long_gaps = [self.gap_stats[s]['long_gaps'] for s in sensors]\n",
        "\n",
        "        x = np.arange(len(sensors))\n",
        "        width = 0.35\n",
        "\n",
        "        ax1.bar(x - width/2, short_gaps, width, label='Short gaps (filled)')\n",
        "        ax1.bar(x + width/2, long_gaps, width, label='Long gaps (flagged)')\n",
        "        ax1.set_xlabel('Sensor')\n",
        "        ax1.set_ylabel('Number of gaps')\n",
        "        ax1.set_title('Gap Distribution by Sensor')\n",
        "        ax1.set_xticks(x)\n",
        "        ax1.set_xticklabels(sensors, rotation=45)\n",
        "        ax1.legend()\n",
        "\n",
        "        # Imputation impact\n",
        "        filled = [self.gap_stats[s]['filled_points'] for s in sensors]\n",
        "        flagged = [self.gap_stats[s]['flagged_points'] for s in sensors]\n",
        "\n",
        "        ax2.bar(x - width/2, filled, width, label='Points filled')\n",
        "        ax2.bar(x + width/2, flagged, width, label='Points flagged')\n",
        "        ax2.set_xlabel('Sensor')\n",
        "        ax2.set_ylabel('Number of points')\n",
        "        ax2.set_title('Imputation Impact by Sensor')\n",
        "        ax2.set_xticks(x)\n",
        "        ax2.set_xticklabels(sensors, rotation=45)\n",
        "        ax2.legend()\n",
        "\n",
        "        plt.tight_layout()\n",
        "        self.run.log({\"imputation_visualization\": wandb.Image(fig)})\n",
        "        plt.close()\n",
        "\n",
        "def main():\n",
        "    import argparse\n",
        "\n",
        "    parser = argparse.ArgumentParser(description=\"Impute missing values in sensor data\")\n",
        "    parser.add_argument(\"--input-file\", default=\"data/time_features.parquet\", help=\"Input file\")\n",
        "    parser.add_argument(\"--output-file\", default=\"data/imputed.parquet\", help=\"Output file\")\n",
        "    parser.add_argument(\"--short-limit\", type=int, default=5, help=\"Max gap size to interpolate\")\n",
        "\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    # Initialize W&B\n",
        "    run = wandb.init(project=\"predictive-maintenance\", job_type=\"imputation\")\n",
        "\n",
        "    # Load data\n",
        "    df = pd.read_parquet(args.input_file)\n",
        "\n",
        "    # Impute missing values\n",
        "    imputer = MissingValueImputer(run)\n",
        "    df = imputer.impute_missing(df, args.short_limit)\n",
        "\n",
        "    # Save results\n",
        "    df.to_parquet(args.output_file)\n",
        "\n",
        "    # Log as artifact\n",
        "    artifact = wandb.Artifact(\"imputed-data\", type=\"dataset\")\n",
        "    artifact.add_file(args.output_file)\n",
        "    artifact.metadata = {\n",
        "        \"short_gap_limit\": args.short_limit,\n",
        "        \"gap_flags_added\": len([col for col in df.columns if col.endswith('_gap_flag')])\n",
        "    }\n",
        "    run.log_artifact(artifact)\n",
        "\n",
        "    run.finish()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()'''"
      ],
      "metadata": {
        "id": "K2EkJBRU3_kf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile src/cleaning/outlier_flags.py\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import wandb\n",
        "import json\n",
        "from scipy import stats\n",
        "from typing import Dict, Any\n",
        "from src.utils.sensor_utils import get_sensor_columns, log_sensor_column_info\n",
        "\n",
        "class OutlierFlagger:\n",
        "    \"\"\"Flag outliers without removal, with W&B logging.\"\"\"\n",
        "\n",
        "    def __init__(self, wandb_run: wandb.run = None):\n",
        "        self.run = wandb_run or wandb.init(project=\"predictive-maintenance\")\n",
        "        self.spike_stats = {}\n",
        "\n",
        "    def load_config(self, config_path: str = \"configs/outlier_config.json\") -> Dict[str, Any]:\n",
        "        \"\"\"Load outlier detection configuration.\"\"\"\n",
        "        try:\n",
        "            with open(config_path, 'r') as f:\n",
        "                return json.load(f)\n",
        "        except FileNotFoundError:\n",
        "            # Default config\n",
        "            return {\n",
        "                \"zscore_threshold\": 3.5,\n",
        "                \"iqr_multiplier\": 1.5,\n",
        "                \"min_spike_duration\": 1,\n",
        "                \"use_rolling_stats\": True,\n",
        "                \"rolling_window\": 60\n",
        "            }\n",
        "\n",
        "    def flag_physical_limits(self, series: pd.Series, limits: Dict[str, float]) -> pd.Series:\n",
        "        \"\"\"Flag values outside physical limits.\"\"\"\n",
        "        flags = pd.Series(False, index=series.index)\n",
        "\n",
        "        if 'min' in limits:\n",
        "            flags |= series < limits['min']\n",
        "        if 'max' in limits:\n",
        "            flags |= series > limits['max']\n",
        "\n",
        "        return flags\n",
        "\n",
        "    def flag_statistical_spikes(self, series: pd.Series, config: Dict[str, Any]) -> pd.Series:\n",
        "        \"\"\"Flag statistical outliers using multiple methods.\"\"\"\n",
        "        flags = pd.Series(False, index=series.index)\n",
        "\n",
        "        # Remove NaN for calculations\n",
        "        clean_series = series.dropna()\n",
        "\n",
        "        if len(clean_series) < 10:\n",
        "            return flags\n",
        "\n",
        "        # Method 1: Z-score\n",
        "        if config.get('use_rolling_stats', False):\n",
        "            # Rolling statistics for non-stationary data\n",
        "            window = config.get('rolling_window', 60)\n",
        "            rolling_mean = series.rolling(window=window, center=True).mean()\n",
        "            rolling_std = series.rolling(window=window, center=True).std()\n",
        "            z_scores = np.abs((series - rolling_mean) / rolling_std)\n",
        "        else:\n",
        "            # Global statistics\n",
        "            z_scores = np.abs(stats.zscore(clean_series))\n",
        "            z_score_flags = pd.Series(z_scores > config['zscore_threshold'],\n",
        "                                    index=clean_series.index)\n",
        "            flags.loc[clean_series.index] |= z_score_flags\n",
        "\n",
        "        # Method 2: IQR\n",
        "        Q1 = clean_series.quantile(0.25)\n",
        "        Q3 = clean_series.quantile(0.75)\n",
        "        IQR = Q3 - Q1\n",
        "        lower_bound = Q1 - config['iqr_multiplier'] * IQR\n",
        "        upper_bound = Q3 + config['iqr_multiplier'] * IQR\n",
        "\n",
        "        iqr_flags = (clean_series < lower_bound) | (clean_series > upper_bound)\n",
        "        flags.loc[clean_series.index] |= iqr_flags\n",
        "\n",
        "        # Method 3: Isolation Forest (commented out for speed in basic version)\n",
        "        # Can be added for more sophisticated detection\n",
        "\n",
        "        return flags\n",
        "\n",
        "    def flag_spikes(self, df: pd.DataFrame, config: Dict[str, Any]) -> pd.DataFrame:\n",
        "        \"\"\"Flag spikes in all sensor columns.\"\"\"\n",
        "        # Load physical limits\n",
        "        limits = {}\n",
        "        try:\n",
        "            with open(\"configs/limits.json\", 'r') as f:\n",
        "                limits = json.load(f)\n",
        "        except FileNotFoundError:\n",
        "            pass\n",
        "\n",
        "        # Process each sensor\n",
        "        sensor_cols = get_sensor_columns(df)\n",
        "\n",
        "        for col in sensor_cols:\n",
        "            physical_flags = pd.Series(False, index=df.index)\n",
        "            statistical_flags = pd.Series(False, index=df.index)\n",
        "\n",
        "            # Check physical limits if available\n",
        "            if col in limits:\n",
        "                physical_flags = self.flag_physical_limits(df[col], limits[col])\n",
        "\n",
        "            # Check statistical outliers\n",
        "            statistical_flags = self.flag_statistical_spikes(df[col], config)\n",
        "\n",
        "            # Combine flags\n",
        "            df[f\"{col}_is_spike\"] = physical_flags | statistical_flags\n",
        "\n",
        "            # Calculate statistics\n",
        "            total_points = len(df)\n",
        "            spike_points = df[f\"{col}_is_spike\"].sum()\n",
        "            spike_rate = spike_points / total_points if total_points > 0 else 0\n",
        "\n",
        "            self.spike_stats[col] = {\n",
        "                'total_spikes': int(spike_points),\n",
        "                'spike_rate': float(spike_rate),\n",
        "                'physical_spikes': int(physical_flags.sum()),\n",
        "                'statistical_spikes': int(statistical_flags.sum())\n",
        "            }\n",
        "\n",
        "            # Log per-sensor metrics\n",
        "            self.run.log({\n",
        "                f\"spike_rate_{col}\": spike_rate,\n",
        "                f\"spike_count_{col}\": spike_points\n",
        "            })\n",
        "\n",
        "        # Log overall statistics\n",
        "        self.log_spike_statistics(df)\n",
        "\n",
        "        return df\n",
        "\n",
        "    def log_spike_statistics(self, df: pd.DataFrame):\n",
        "        \"\"\"Log comprehensive spike statistics to W&B.\"\"\"\n",
        "        # Create summary table\n",
        "        stats_df = pd.DataFrame.from_dict(self.spike_stats, orient='index')\n",
        "        stats_df.index.name = 'sensor'\n",
        "        stats_df.reset_index(inplace=True)\n",
        "\n",
        "        table = wandb.Table(dataframe=stats_df)\n",
        "        self.run.log({\"spike_statistics\": table})\n",
        "\n",
        "        # Log aggregate metrics\n",
        "        avg_spike_rate = np.mean([s['spike_rate'] for s in self.spike_stats.values()])\n",
        "        total_spikes = sum(s['total_spikes'] for s in self.spike_stats.values())\n",
        "\n",
        "        self.run.log({\n",
        "            \"avg_spike_rate_all_sensors\": avg_spike_rate,\n",
        "            \"total_spikes_all_sensors\": total_spikes\n",
        "        })\n",
        "\n",
        "        # Create visualization\n",
        "        import matplotlib.pyplot as plt\n",
        "\n",
        "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))\n",
        "\n",
        "        # Spike rates by sensor\n",
        "        sensors = list(self.spike_stats.keys())\n",
        "        spike_rates = [self.spike_stats[s]['spike_rate'] for s in sensors]\n",
        "\n",
        "        ax1.bar(sensors, spike_rates, color='coral')\n",
        "        ax1.set_xlabel('Sensor')\n",
        "        ax1.set_ylabel('Spike Rate')\n",
        "        ax1.set_title('Spike Rates by Sensor')\n",
        "        ax1.tick_params(axis='x', rotation=45)\n",
        "\n",
        "        # Add horizontal line for average\n",
        "        ax1.axhline(y=avg_spike_rate, color='red', linestyle='--',\n",
        "                   label=f'Average: {avg_spike_rate:.3f}')\n",
        "        ax1.legend()\n",
        "\n",
        "        # Spike types distribution\n",
        "        physical_spikes = [self.spike_stats[s]['physical_spikes'] for s in sensors]\n",
        "        statistical_spikes = [self.spike_stats[s]['statistical_spikes'] for s in sensors]\n",
        "\n",
        "        x = np.arange(len(sensors))\n",
        "        width = 0.35\n",
        "\n",
        "        ax2.bar(x - width/2, physical_spikes, width, label='Physical limit violations')\n",
        "        ax2.bar(x + width/2, statistical_spikes, width, label='Statistical outliers')\n",
        "        ax2.set_xlabel('Sensor')\n",
        "        ax2.set_ylabel('Number of spikes')\n",
        "        ax2.set_title('Spike Types by Sensor')\n",
        "        ax2.set_xticks(x)\n",
        "        ax2.set_xticklabels(sensors, rotation=45)\n",
        "        ax2.legend()\n",
        "\n",
        "        plt.tight_layout()\n",
        "        self.run.log({\"spike_analysis\": wandb.Image(fig)})\n",
        "        plt.close()\n",
        "\n",
        "def main():\n",
        "    import argparse\n",
        "\n",
        "    parser = argparse.ArgumentParser(description=\"Flag outliers in sensor data\")\n",
        "    parser.add_argument(\"--input-file\", default=\"data/imputed.parquet\", help=\"Input file\")\n",
        "    parser.add_argument(\"--output-file\", default=\"data/flagged.parquet\", help=\"Output file\")\n",
        "    parser.add_argument(\"--config\", default=\"configs/outlier_config.json\", help=\"Config file\")\n",
        "\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    # Initialize W&B\n",
        "    run = wandb.init(project=\"predictive-maintenance\", job_type=\"outlier-detection\")\n",
        "\n",
        "    # Load data\n",
        "    df = pd.read_parquet(args.input_file)\n",
        "\n",
        "    # Flag outliers\n",
        "    flagger = OutlierFlagger(run)\n",
        "    config = flagger.load_config(args.config)\n",
        "    df = flagger.flag_spikes(df, config)\n",
        "\n",
        "    # Save results\n",
        "    df.to_parquet(args.output_file)\n",
        "\n",
        "    # Log as artifact\n",
        "    artifact = wandb.Artifact(\"flagged-data\", type=\"dataset\")\n",
        "    artifact.add_file(args.output_file)\n",
        "    artifact.metadata = {\n",
        "        \"outlier_config\": config,\n",
        "        \"spike_flags_added\": len([col for col in df.columns if col.endswith('_is_spike')])\n",
        "    }\n",
        "    run.log_artifact(artifact)\n",
        "\n",
        "    run.finish()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "aip3_z-E4J4e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile src/windowing/segment.py\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import wandb\n",
        "from typing import Generator, Tuple, Optional\n",
        "import random\n",
        "from src.utils.sensor_utils import get_sensor_columns, log_sensor_column_info\n",
        "\n",
        "class WindowSegmenter:\n",
        "    \"\"\"Create overlapping windows with W&B logging.\"\"\"\n",
        "\n",
        "    def __init__(self, wandb_run: wandb.run = None):\n",
        "        self.run = wandb_run or wandb.init(project=\"predictive-maintenance\")\n",
        "        self.window_stats = {}\n",
        "\n",
        "    def generate_windows(self, df: pd.DataFrame, window_size: int = 60,\n",
        "                        stride: int = 30) -> Generator[pd.DataFrame, None, None]:\n",
        "        \"\"\"Generate overlapping windows from dataframe.\"\"\"\n",
        "        # Ensure index is datetime\n",
        "        if not isinstance(df.index, pd.DatetimeIndex):\n",
        "            raise ValueError(\"DataFrame must have DatetimeIndex\")\n",
        "\n",
        "        # Calculate number of windows\n",
        "        total_points = len(df)\n",
        "        num_windows = (total_points - window_size) // stride + 1\n",
        "\n",
        "        self.run.log({\n",
        "            \"total_windows\": num_windows,\n",
        "            \"window_size\": window_size,\n",
        "            \"stride\": stride,\n",
        "            \"overlap_ratio\": 1 - (stride / window_size)\n",
        "        })\n",
        "\n",
        "        # Generate windows\n",
        "        windows_generated = 0\n",
        "        for i in range(0, total_points - window_size + 1, stride):\n",
        "            window = df.iloc[i:i + window_size].copy()\n",
        "            window.attrs['window_id'] = windows_generated\n",
        "            window.attrs['start_idx'] = i\n",
        "            window.attrs['end_idx'] = i + window_size\n",
        "\n",
        "            windows_generated += 1\n",
        "            yield window\n",
        "\n",
        "        # Store stats\n",
        "        self.window_stats['total_windows'] = windows_generated\n",
        "        self.window_stats['window_size'] = window_size\n",
        "        self.window_stats['stride'] = stride\n",
        "\n",
        "    def generate_baseline_windows(self, df: pd.DataFrame, window_size: int = 60,\n",
        "                                 stride: int = 30, baseline_hours: list = None) -> Generator[pd.DataFrame, None, None]:\n",
        "        \"\"\"Generate windows only from baseline periods (e.g., night shift).\"\"\"\n",
        "        if baseline_hours is None:\n",
        "            baseline_hours = [22, 23, 0, 1, 2, 3, 4, 5]  # Night hours\n",
        "\n",
        "        # Filter baseline periods\n",
        "        baseline_mask = df.index.hour.isin(baseline_hours)\n",
        "        baseline_df = df[baseline_mask]\n",
        "\n",
        "        self.run.log({\n",
        "            \"baseline_data_points\": len(baseline_df),\n",
        "            \"baseline_data_ratio\": len(baseline_df) / len(df)\n",
        "        })\n",
        "\n",
        "        # Generate windows from baseline\n",
        "        yield from self.generate_windows(baseline_df, window_size, stride)\n",
        "\n",
        "    def log_sample_windows(self, df: pd.DataFrame, window_size: int = 60,\n",
        "                          stride: int = 30, num_samples: int = 3):\n",
        "        \"\"\"Log sample windows to W&B for inspection.\"\"\"\n",
        "        # Collect a few windows\n",
        "        windows = list(self.generate_windows(df, window_size, stride))\n",
        "\n",
        "        if len(windows) < num_samples:\n",
        "            num_samples = len(windows)\n",
        "\n",
        "        # Random sample\n",
        "        sample_indices = random.sample(range(len(windows)), num_samples)\n",
        "\n",
        "        # Create table data\n",
        "        table_data = []\n",
        "        for idx in sample_indices:\n",
        "            window = windows[idx]\n",
        "\n",
        "            # Calculate window statistics\n",
        "            sensor_cols = get_sensor_columns(df)\n",
        "\n",
        "            for sensor in sensor_cols:\n",
        "                stats = {\n",
        "                    'window_id': window.attrs['window_id'],\n",
        "                    'start_time': window.index[0].isoformat(),\n",
        "                    'end_time': window.index[-1].isoformat(),\n",
        "                    'sensor': sensor,\n",
        "                    'mean': float(window[sensor].mean()),\n",
        "                    'std': float(window[sensor].std()),\n",
        "                    'min': float(window[sensor].min()),\n",
        "                    'max': float(window[sensor].max()),\n",
        "                    'missing_rate': float(window[sensor].isnull().sum() / len(window))\n",
        "                }\n",
        "                table_data.append(stats)\n",
        "\n",
        "        # Log table\n",
        "        table = wandb.Table(dataframe=pd.DataFrame(table_data))\n",
        "        self.run.log({\"sample_windows\": table})\n",
        "\n",
        "        # Create visualization of sample windows\n",
        "        import matplotlib.pyplot as plt\n",
        "\n",
        "        fig, axes = plt.subplots(num_samples, 1, figsize=(12, 4*num_samples))\n",
        "        if num_samples == 1:\n",
        "            axes = [axes]\n",
        "\n",
        "        for i, idx in enumerate(sample_indices):\n",
        "            window = windows[idx]\n",
        "            sensor_cols = df.select_dtypes(include=[np.number]).columns.tolist()[:3]  # Plot first 3 sensors\n",
        "\n",
        "            for sensor in sensor_cols:\n",
        "                axes[i].plot(window.index, window[sensor], label=sensor, alpha=0.7)\n",
        "\n",
        "            axes[i].set_title(f\"Window {window.attrs['window_id']} - {window.index[0].strftime('%Y-%m-%d %H:%M')}\")\n",
        "            axes[i].set_xlabel('Time')\n",
        "            axes[i].set_ylabel('Sensor Value')\n",
        "            axes[i].legend()\n",
        "            axes[i].grid(True, alpha=0.3)\n",
        "\n",
        "        plt.tight_layout()\n",
        "        self.run.log({\"sample_windows_plot\": wandb.Image(fig)})\n",
        "        plt.close()\n",
        "\n",
        "    def save_windows_metadata(self, output_path: str = \"data/windows_metadata.csv\"):\n",
        "        \"\"\"Save window generation metadata.\"\"\"\n",
        "        metadata_df = pd.DataFrame([self.window_stats])\n",
        "        metadata_df.to_csv(output_path, index=False)\n",
        "\n",
        "        # Log as artifact\n",
        "        artifact = wandb.Artifact(\"windows-metadata\", type=\"dataset\")\n",
        "        artifact.add_file(output_path)\n",
        "        self.run.log_artifact(artifact)\n",
        "\n",
        "def main():\n",
        "    import argparse\n",
        "\n",
        "    parser = argparse.ArgumentParser(description=\"Generate windows from sensor data\")\n",
        "    parser.add_argument(\"--input-file\", default=\"data/flagged.parquet\", help=\"Input file\")\n",
        "    parser.add_argument(\"--window-size\", type=int, default=60, help=\"Window size in minutes\")\n",
        "    parser.add_argument(\"--stride\", type=int, default=30, help=\"Stride in minutes\")\n",
        "    parser.add_argument(\"--num-samples\", type=int, default=3, help=\"Number of sample windows to log\")\n",
        "\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    # Initialize W&B\n",
        "    run = wandb.init(project=\"predictive-maintenance\", job_type=\"windowing\")\n",
        "\n",
        "    # Load data\n",
        "    df = pd.read_parquet(args.input_file)\n",
        "\n",
        "    # Create windows\n",
        "    segmenter = WindowSegmenter(run)\n",
        "\n",
        "    # Log sample windows\n",
        "    segmenter.log_sample_windows(df, args.window_size, args.stride, args.num_samples)\n",
        "\n",
        "    # Save metadata\n",
        "    segmenter.save_windows_metadata()\n",
        "\n",
        "    run.finish()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "PCEzgIaWB3Ek"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile src/cleaning/window_clean.py\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import wandb\n",
        "from sklearn.ensemble import IsolationForest\n",
        "from sklearn.preprocessing import RobustScaler\n",
        "import joblib\n",
        "from typing import List, Tuple, Dict\n",
        "import matplotlib.pyplot as plt\n",
        "from src.utils.sensor_utils import get_sensor_columns, log_sensor_column_info\n",
        "\n",
        "class WindowCleaner:\n",
        "    \"\"\"Clean baseline windows and establish normal operating envelope.\"\"\"\n",
        "\n",
        "    def __init__(self, wandb_run: wandb.run = None):\n",
        "        self.run = wandb_run or wandb.init(project=\"predictive-maintenance\")\n",
        "        self.baseline_stats = {}\n",
        "        self.models = {}\n",
        "\n",
        "    def clean_window_mad(self, window: pd.DataFrame, threshold: float = 3.0) -> pd.DataFrame:\n",
        "        \"\"\"Clean window using Median Absolute Deviation.\"\"\"\n",
        "        cleaned_window = window.copy()\n",
        "\n",
        "        # FIXED: Use sensor_utils instead of hardcoded df.select_dtypes()\n",
        "        sensor_cols = get_sensor_columns(window)\n",
        "\n",
        "        for col in sensor_cols:\n",
        "            series = window[col].dropna()\n",
        "            if len(series) < 10:\n",
        "                continue\n",
        "\n",
        "            median = series.median()\n",
        "            mad = np.median(np.abs(series - median))\n",
        "\n",
        "            # Modified Z-score using MAD\n",
        "            if mad > 0:\n",
        "                modified_z_scores = 0.6745 * (series - median) / mad\n",
        "                outliers = np.abs(modified_z_scores) > threshold\n",
        "                cleaned_window.loc[outliers.index[outliers], col] = np.nan\n",
        "\n",
        "        return cleaned_window\n",
        "\n",
        "    def clean_window_percentile(self, window: pd.DataFrame,\n",
        "                               lower_percentile: float = 5.0,\n",
        "                               upper_percentile: float = 95.0) -> pd.DataFrame:\n",
        "        \"\"\"Clean window using percentile-based outlier removal.\"\"\"\n",
        "        cleaned_window = window.copy()\n",
        "\n",
        "        # Use dynamic sensor column identification\n",
        "        sensor_cols = get_sensor_columns(window)\n",
        "\n",
        "        for col in sensor_cols:\n",
        "            series = window[col].dropna()\n",
        "            if len(series) < 10:\n",
        "                continue\n",
        "\n",
        "            lower_bound = np.percentile(series, lower_percentile)\n",
        "            upper_bound = np.percentile(series, upper_percentile)\n",
        "\n",
        "            # Remove outliers\n",
        "            outliers = (series < lower_bound) | (series > upper_bound)\n",
        "            cleaned_window.loc[outliers.index[outliers], col] = np.nan\n",
        "\n",
        "        return cleaned_window\n",
        "\n",
        "    def fit_isolation_forest(self, windows: List[pd.DataFrame],\n",
        "                           contamination: float = 0.1) -> IsolationForest:\n",
        "        \"\"\"Fit isolation forest on baseline windows.\"\"\"\n",
        "        # Use dynamic sensor column identification\n",
        "        if not windows:\n",
        "            return None\n",
        "\n",
        "        sensor_cols = get_sensor_columns(windows[0])\n",
        "\n",
        "        # Combine all windows\n",
        "        combined_data = []\n",
        "        for window in windows:\n",
        "            window_data = window[sensor_cols].values\n",
        "            # Reshape to 1D per window (feature vector)\n",
        "            combined_data.append(window_data.flatten())\n",
        "\n",
        "        combined_array = np.array(combined_data)\n",
        "\n",
        "        # Fit isolation forest\n",
        "        iso_forest = IsolationForest(\n",
        "            contamination=contamination,\n",
        "            random_state=42,\n",
        "            n_jobs=-1\n",
        "        )\n",
        "        iso_forest.fit(combined_array)\n",
        "\n",
        "        self.models['isolation_forest'] = iso_forest\n",
        "\n",
        "        # Log statistics\n",
        "        self.run.log({\n",
        "            \"isolation_forest_contamination\": contamination,\n",
        "            \"isolation_forest_features\": combined_array.shape[1],\n",
        "            \"isolation_forest_samples\": combined_array.shape[0]\n",
        "        })\n",
        "\n",
        "        return iso_forest\n",
        "\n",
        "    def calculate_baseline_statistics(self, windows: List[pd.DataFrame]) -> Dict[str, Dict[str, float]]:\n",
        "        \"\"\"Calculate baseline statistics from clean windows.\"\"\"\n",
        "        if not windows:\n",
        "            return {}\n",
        "\n",
        "        # Use dynamic sensor column identification\n",
        "        sensor_cols = get_sensor_columns(windows[0])\n",
        "\n",
        "        stats = {}\n",
        "\n",
        "        for col in sensor_cols:\n",
        "            all_values = []\n",
        "            for window in windows:\n",
        "                values = window[col].dropna()\n",
        "                all_values.extend(values.tolist())\n",
        "\n",
        "            if all_values:\n",
        "                col_array = np.array(all_values)\n",
        "                stats[col] = {\n",
        "                    'mean': float(np.mean(col_array)),\n",
        "                    'std': float(np.std(col_array)),\n",
        "                    'median': float(np.median(col_array)),\n",
        "                    'q25': float(np.percentile(col_array, 25)),\n",
        "                    'q75': float(np.percentile(col_array, 75)),\n",
        "                    'min': float(np.min(col_array)),\n",
        "                    'max': float(np.max(col_array))\n",
        "                }\n",
        "\n",
        "        return stats\n",
        "\n",
        "    def clean_baseline_windows(self, df: pd.DataFrame, window_size: int = 60,\n",
        "                             stride: int = 30, baseline_hours: list = None,\n",
        "                             cleaning_method: str = 'mad') -> Tuple[List[pd.DataFrame], Dict]:\n",
        "        \"\"\"Clean baseline windows and return statistics.\"\"\"\n",
        "        from src.windowing.segment import WindowSegmenter\n",
        "\n",
        "        # Generate baseline windows\n",
        "        segmenter = WindowSegmenter(self.run)\n",
        "        baseline_windows = list(segmenter.generate_baseline_windows(\n",
        "            df, window_size, stride, baseline_hours\n",
        "        ))\n",
        "\n",
        "        self.run.log({\"baseline_windows_count\": len(baseline_windows)})\n",
        "\n",
        "        # Clean windows\n",
        "        cleaned_windows = []\n",
        "        cleaning_impact = []\n",
        "\n",
        "        for window in baseline_windows:\n",
        "            if cleaning_method == 'mad':\n",
        "                cleaned = self.clean_window_mad(window)\n",
        "            elif cleaning_method == 'percentile':\n",
        "                cleaned = self.clean_window_percentile(window)\n",
        "            else:\n",
        "                cleaned = window.copy()\n",
        "\n",
        "            # Calculate cleaning impact using dynamic sensor identification\n",
        "            sensor_cols = get_sensor_columns(window)\n",
        "\n",
        "            if sensor_cols:\n",
        "                original_nulls = window[sensor_cols].isnull().sum().sum()\n",
        "                cleaned_nulls = cleaned[sensor_cols].isnull().sum().sum()\n",
        "                points_removed = cleaned_nulls - original_nulls\n",
        "\n",
        "                cleaning_impact.append({\n",
        "                    'window_id': window.attrs.get('window_id', 0),\n",
        "                    'points_removed': points_removed,\n",
        "                    'removal_rate': points_removed / (len(window) * len(sensor_cols)) if sensor_cols else 0\n",
        "                })\n",
        "\n",
        "                cleaned_windows.append(cleaned)\n",
        "\n",
        "        # Log cleaning impact\n",
        "        if cleaning_impact:\n",
        "            avg_removal_rate = np.mean([c['removal_rate'] for c in cleaning_impact])\n",
        "            self.run.log({\n",
        "                \"avg_baseline_cleaning_rate\": avg_removal_rate,\n",
        "                \"total_points_removed\": sum(c['points_removed'] for c in cleaning_impact),\n",
        "                \"baseline_cleaning_method\": cleaning_method\n",
        "            })\n",
        "\n",
        "        # Calculate baseline statistics\n",
        "        self.baseline_stats = self.calculate_baseline_statistics(cleaned_windows)\n",
        "\n",
        "        # Fit isolation forest for anomaly detection\n",
        "        if len(cleaned_windows) > 5:\n",
        "            self.fit_isolation_forest(cleaned_windows)\n",
        "\n",
        "        return cleaned_windows, self.baseline_stats\n",
        "\n",
        "    def save_models_and_stats(self, output_dir: str = \"models\"):\n",
        "        \"\"\"Save trained models and baseline statistics.\"\"\"\n",
        "        import os\n",
        "        import json\n",
        "\n",
        "        os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "        # Save baseline statistics\n",
        "        stats_path = f\"{output_dir}/baseline_stats.json\"\n",
        "        with open(stats_path, 'w') as f:\n",
        "            json.dump(self.baseline_stats, f, indent=2)\n",
        "\n",
        "        # Save isolation forest if available\n",
        "        if 'isolation_forest' in self.models:\n",
        "            model_path = f\"{output_dir}/isolation_forest.joblib\"\n",
        "            joblib.dump(self.models['isolation_forest'], model_path)\n",
        "\n",
        "        # Log as artifacts\n",
        "        artifact = wandb.Artifact(\"baseline-models\", type=\"model\")\n",
        "        artifact.add_file(stats_path)\n",
        "        if 'isolation_forest' in self.models:\n",
        "            artifact.add_file(model_path)\n",
        "        self.run.log_artifact(artifact)\n",
        "\n",
        "def main():\n",
        "    import argparse\n",
        "\n",
        "    parser = argparse.ArgumentParser(description=\"Clean baseline windows\")\n",
        "    parser.add_argument(\"--input-file\", default=\"data/flagged.parquet\", help=\"Input file\")\n",
        "    parser.add_argument(\"--window-size\", type=int, default=60, help=\"Window size\")\n",
        "    parser.add_argument(\"--stride\", type=int, default=30, help=\"Stride\")\n",
        "    parser.add_argument(\"--method\", choices=['mad', 'percentile'], default='mad', help=\"Cleaning method\")\n",
        "    parser.add_argument(\"--output-dir\", default=\"models\", help=\"Output directory\")\n",
        "\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    # Initialize W&B\n",
        "    run = wandb.init(project=\"predictive-maintenance\", job_type=\"baseline-cleaning\")\n",
        "\n",
        "    # Load data\n",
        "    df = pd.read_parquet(args.input_file)\n",
        "\n",
        "    # Clean windows\n",
        "    cleaner = WindowCleaner(run)\n",
        "    cleaned_windows, stats = cleaner.clean_baseline_windows(\n",
        "        df, args.window_size, args.stride, cleaning_method=args.method\n",
        "    )\n",
        "\n",
        "    # Save results\n",
        "    cleaner.save_models_and_stats(args.output_dir)\n",
        "\n",
        "    run.finish()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "J0WidcMgG9Fk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''%%writefile src/cleaning/window_clean.py\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import wandb\n",
        "from sklearn.ensemble import IsolationForest\n",
        "from sklearn.preprocessing import RobustScaler\n",
        "import joblib\n",
        "from typing import List, Tuple, Dict\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "class WindowCleaner:\n",
        "    \"\"\"Clean baseline windows and establish normal operating envelope.\"\"\"\n",
        "\n",
        "    def __init__(self, wandb_run: wandb.run = None):\n",
        "        self.run = wandb_run or wandb.init(project=\"predictive-maintenance\")\n",
        "        self.baseline_stats = {}\n",
        "        self.models = {}\n",
        "\n",
        "    def clean_window_mad(self, window: pd.DataFrame, threshold: float = 3.0) -> pd.DataFrame:\n",
        "        \"\"\"Clean window using Median Absolute Deviation.\"\"\"\n",
        "        cleaned_window = window.copy()\n",
        "        sensor_cols = [col for col in window.columns if col.startswith('sensor_')]\n",
        "\n",
        "        for col in sensor_cols:\n",
        "            series = window[col].dropna()\n",
        "            if len(series) < 10:\n",
        "                continue\n",
        "\n",
        "            median = series.median()\n",
        "            mad = np.median(np.abs(series - median))\n",
        "\n",
        "            # Modified Z-score using MAD\n",
        "            if mad > 0:\n",
        "                modified_z_scores = 0.6745 * (series - median) / mad\n",
        "                outliers = np.abs(modified_z_scores) > threshold\n",
        "                cleaned_window.loc[outliers.index[outliers], col] = np.nan\n",
        "\n",
        "        return cleaned_window\n",
        "\n",
        "    def clean_window_percentile(self, window: pd.DataFrame,\n",
        "                               lower_percentile: float = 1,\n",
        "                               upper_percentile: float = 99) -> pd.DataFrame:\n",
        "        \"\"\"Clean window using percentile-based method.\"\"\"\n",
        "        cleaned_window = window.copy()\n",
        "        sensor_cols = [col for col in window.columns if col.startswith('sensor_')]\n",
        "\n",
        "        for col in sensor_cols:\n",
        "            series = window[col].dropna()\n",
        "            if len(series) < 10:\n",
        "                continue\n",
        "\n",
        "            lower_bound = series.quantile(lower_percentile / 100)\n",
        "            upper_bound = series.quantile(upper_percentile / 100)\n",
        "\n",
        "            mask = (series < lower_bound) | (series > upper_bound)\n",
        "            cleaned_window.loc[mask.index[mask], col] = np.nan\n",
        "\n",
        "        return cleaned_window\n",
        "\n",
        "    def fit_isolation_forest(self, baseline_windows: List[pd.DataFrame],\n",
        "                           contamination: float = 0.05) -> IsolationForest:\n",
        "        \"\"\"Fit Isolation Forest on baseline windows.\"\"\"\n",
        "        # Prepare data\n",
        "        sensor_cols = [col for col in baseline_windows[0].columns if col.startswith('sensor_')]\n",
        "\n",
        "        # Stack all baseline data\n",
        "        baseline_data = []\n",
        "        for window in baseline_windows:\n",
        "            window_data = window[sensor_cols].dropna()\n",
        "            if len(window_data) > 0:\n",
        "                baseline_data.append(window_data)\n",
        "\n",
        "        if not baseline_data:\n",
        "            return None\n",
        "\n",
        "        all_baseline = pd.concat(baseline_data, axis=0)\n",
        "\n",
        "        # Fit Isolation Forest\n",
        "        iso_forest = IsolationForest(\n",
        "            contamination=contamination,\n",
        "            random_state=42,\n",
        "            n_estimators=100\n",
        "        )\n",
        "        iso_forest.fit(all_baseline)\n",
        "\n",
        "        # Calculate and log performance metrics\n",
        "        predictions = iso_forest.predict(all_baseline)\n",
        "        anomaly_ratio = (predictions == -1).sum() / len(predictions)\n",
        "\n",
        "        self.run.log({\n",
        "            \"isolation_forest_anomaly_ratio\": anomaly_ratio,\n",
        "            \"isolation_forest_n_samples\": len(all_baseline)\n",
        "        })\n",
        "\n",
        "        return iso_forest\n",
        "\n",
        "    def calculate_baseline_statistics(self, cleaned_windows: List[pd.DataFrame]) -> Dict:\n",
        "        \"\"\"Calculate statistics from cleaned baseline windows.\"\"\"\n",
        "        sensor_cols = [\n",
        "            col for col in cleaned_windows[0].columns\n",
        "            if col.startswith('sensor_') and not col.endswith('_is_spike') and not col.endswith('_gap_flag')\n",
        "        ]\n",
        "        stats = {}\n",
        "\n",
        "        for col in sensor_cols:\n",
        "            col_data = []\n",
        "            for window in cleaned_windows:\n",
        "                col_data.extend(window[col].dropna().values)\n",
        "\n",
        "            if col_data:\n",
        "                col_array = np.array(col_data)\n",
        "                stats[col] = {\n",
        "                    'mean': float(np.mean(col_array)),\n",
        "                    'median': float(np.median(col_array)),\n",
        "                    'std': float(np.std(col_array)),\n",
        "                    'iqr': float(np.percentile(col_array, 75) - np.percentile(col_array, 25)),\n",
        "                    'p5': float(np.percentile(col_array, 5)),\n",
        "                    'p95': float(np.percentile(col_array, 95)),\n",
        "                    'min': float(np.min(col_array)),\n",
        "                    'max': float(np.max(col_array))\n",
        "                }\n",
        "\n",
        "        return stats\n",
        "\n",
        "    def clean_baseline_windows(self, df: pd.DataFrame, window_size: int = 60,\n",
        "                             stride: int = 30, baseline_hours: list = None,\n",
        "                             cleaning_method: str = 'mad') -> Tuple[List[pd.DataFrame], Dict]:\n",
        "        \"\"\"Clean baseline windows and return statistics.\"\"\"\n",
        "        from src.windowing.segment import WindowSegmenter\n",
        "\n",
        "        # Generate baseline windows\n",
        "        segmenter = WindowSegmenter(self.run)\n",
        "        baseline_windows = list(segmenter.generate_baseline_windows(\n",
        "            df, window_size, stride, baseline_hours\n",
        "        ))\n",
        "\n",
        "        self.run.log({\"baseline_windows_count\": len(baseline_windows)})\n",
        "\n",
        "        # Clean windows\n",
        "        cleaned_windows = []\n",
        "        cleaning_impact = []\n",
        "\n",
        "        for window in baseline_windows:\n",
        "            if cleaning_method == 'mad':\n",
        "                cleaned = self.clean_window_mad(window)\n",
        "            elif cleaning_method == 'percentile':\n",
        "                cleaned = self.clean_window_percentile(window)\n",
        "            else:\n",
        "                cleaned = window.copy()\n",
        "\n",
        "            # Calculate cleaning impact\n",
        "            sensor_cols = [col for col in window.columns if col.startswith('sensor_')]\n",
        "            original_nulls = window[sensor_cols].isnull().sum().sum()\n",
        "            cleaned_nulls = cleaned[sensor_cols].isnull().sum().sum()\n",
        "            points_removed = cleaned_nulls - original_nulls\n",
        "\n",
        "            cleaning_impact.append({\n",
        "                'window_id': window.attrs.get('window_id', 0),\n",
        "                'points_removed': points_removed,\n",
        "                'removal_rate': points_removed / (len(window) * len(sensor_cols))\n",
        "            })\n",
        "\n",
        "            cleaned_windows.append(cleaned)\n",
        "\n",
        "        # Log cleaning impact\n",
        "        avg_removal_rate = np.mean([c['removal_rate'] for c in cleaning_impact])\n",
        "        self.run.log({\n",
        "            \"avg_baseline_cleaning_rate\": avg_removal_rate,\n",
        "            \"total_points_removed\": sum(c['points_removed'] for c in cleaning_impact)\n",
        "        })\n",
        "\n",
        "        # Calculate baseline statistics\n",
        "        self.baseline_stats = self.calculate_baseline_statistics(cleaned_windows)\n",
        "\n",
        "        # Fit models\n",
        "        if len(cleaned_windows) > 10:\n",
        "            self.models['isolation_forest'] = self.fit_isolation_forest(cleaned_windows)\n",
        "\n",
        "        # Log baseline statistics\n",
        "        self.log_baseline_statistics()\n",
        "\n",
        "        return cleaned_windows, self.baseline_stats\n",
        "\n",
        "    def log_baseline_statistics(self):\n",
        "        \"\"\"Log comprehensive baseline statistics to W&B.\"\"\"\n",
        "        # Create statistics table\n",
        "        stats_data = []\n",
        "        for sensor, stats in self.baseline_stats.items():\n",
        "            row = {'sensor': sensor}\n",
        "            row.update(stats)\n",
        "            stats_data.append(row)\n",
        "\n",
        "        stats_df = pd.DataFrame(stats_data)\n",
        "        table = wandb.Table(dataframe=stats_df)\n",
        "        self.run.log({\"baseline_statistics\": table})\n",
        "\n",
        "        # Log individual metrics\n",
        "        for sensor, stats in self.baseline_stats.items():\n",
        "            self.run.log({\n",
        "                f\"baseline_{sensor}_median\": stats['median'],\n",
        "                f\"baseline_{sensor}_iqr\": stats['iqr']\n",
        "            })\n",
        "\n",
        "        # Create visualization\n",
        "        fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
        "        axes = axes.ravel()\n",
        "\n",
        "        sensors = list(self.baseline_stats.keys())[:4]  # Plot first 4 sensors\n",
        "\n",
        "        for i, sensor in enumerate(sensors):\n",
        "            stats = self.baseline_stats[sensor]\n",
        "\n",
        "            # Box plot representation\n",
        "            ax = axes[i]\n",
        "            positions = [1]\n",
        "            box_data = [[stats['min'], stats['p5'], stats['median'], stats['p95'], stats['max']]]\n",
        "\n",
        "            bp = ax.boxplot(box_data, positions=positions, widths=0.6,\n",
        "                           patch_artist=True, showfliers=False)\n",
        "            bp['boxes'][0].set_facecolor('lightblue')\n",
        "\n",
        "            # Add mean line\n",
        "            ax.hlines(stats['mean'], 0.7, 1.3, colors='red', linestyles='--', label='Mean')\n",
        "\n",
        "            # Labels\n",
        "            ax.set_ylabel('Value')\n",
        "            ax.set_title(f'Baseline Statistics: {sensor}')\n",
        "            ax.set_xticklabels([''])\n",
        "            ax.legend()\n",
        "\n",
        "            # Add text annotations\n",
        "            ax.text(1.4, stats['max'], f\"Max: {stats['max']:.2f}\", fontsize=9)\n",
        "            ax.text(1.4, stats['p95'], f\"P95: {stats['p95']:.2f}\", fontsize=9)\n",
        "            ax.text(1.4, stats['median'], f\"Median: {stats['median']:.2f}\", fontsize=9)\n",
        "            ax.text(1.4, stats['p5'], f\"P5: {stats['p5']:.2f}\", fontsize=9)\n",
        "            ax.text(1.4, stats['min'], f\"Min: {stats['min']:.2f}\", fontsize=9)\n",
        "\n",
        "        plt.tight_layout()\n",
        "        self.run.log({\"baseline_envelope\": wandb.Image(fig)})\n",
        "        plt.close()\n",
        "\n",
        "    def save_models_and_stats(self, output_dir: str = \"models\"):\n",
        "        \"\"\"Save fitted models and statistics as W&B artifacts.\"\"\"\n",
        "        import os\n",
        "        os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "        # Save Isolation Forest if fitted\n",
        "        if 'isolation_forest' in self.models and self.models['isolation_forest'] is not None:\n",
        "            model_path = f\"{output_dir}/isolation_forest.joblib\"\n",
        "            joblib.dump(self.models['isolation_forest'], model_path)\n",
        "\n",
        "            # Log as artifact\n",
        "            artifact = wandb.Artifact(\"baseline-isolation-forest\", type=\"model\")\n",
        "            artifact.add_file(model_path)\n",
        "            self.run.log_artifact(artifact)\n",
        "\n",
        "        # Save baseline statistics\n",
        "        stats_path = f\"{output_dir}/baseline_stats.json\"\n",
        "        import json\n",
        "        with open(stats_path, 'w') as f:\n",
        "            json.dump(self.baseline_stats, f, indent=2)\n",
        "\n",
        "        # Log as artifact\n",
        "        artifact = wandb.Artifact(\"baseline-statistics\", type=\"dataset\")\n",
        "        artifact.add_file(stats_path)\n",
        "        self.run.log_artifact(artifact)\n",
        "\n",
        "def main():\n",
        "    import argparse\n",
        "\n",
        "    parser = argparse.ArgumentParser(description=\"Clean baseline windows\")\n",
        "    parser.add_argument(\"--input-file\", default=\"data/flagged.parquet\", help=\"Input file\")\n",
        "    parser.add_argument(\"--window-size\", type=int, default=60, help=\"Window size\")\n",
        "    parser.add_argument(\"--stride\", type=int, default=30, help=\"Stride\")\n",
        "    parser.add_argument(\"--cleaning-method\", default=\"mad\", choices=[\"mad\", \"percentile\"],\n",
        "                       help=\"Cleaning method\")\n",
        "\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    # Initialize W&B\n",
        "    run = wandb.init(project=\"predictive-maintenance\", job_type=\"baseline-cleaning\")\n",
        "\n",
        "    # Load data\n",
        "    df = pd.read_parquet(args.input_file)\n",
        "\n",
        "    # Clean baseline windows\n",
        "    cleaner = WindowCleaner(run)\n",
        "    cleaned_windows, stats = cleaner.clean_baseline_windows(\n",
        "        df, args.window_size, args.stride, cleaning_method=args.cleaning_method\n",
        "    )\n",
        "\n",
        "    # Save models and statistics\n",
        "    cleaner.save_models_and_stats()\n",
        "\n",
        "    run.finish()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()'''"
      ],
      "metadata": {
        "id": "M7sbX1dsEhiU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''%%writefile src/features/decompose.py\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import wandb\n",
        "from statsmodels.tsa.seasonal import STL\n",
        "from statsmodels.tsa.stattools import adfuller\n",
        "import matplotlib.pyplot as plt\n",
        "from typing import Dict, Tuple, Optional\n",
        "\n",
        "class TimeSeriesDecomposer:\n",
        "    \"\"\"Perform seasonal and trend decomposition with W&B logging.\"\"\"\n",
        "\n",
        "    def __init__(self, wandb_run: wandb.run = None):\n",
        "        self.run = wandb_run or wandb.init(project=\"predictive-maintenance\")\n",
        "        self.decomposition_stats = {}\n",
        "\n",
        "    def test_stationarity(self, series: pd.Series) -> Dict:\n",
        "        \"\"\"Perform Augmented Dickey-Fuller test for stationarity.\"\"\"\n",
        "        result = adfuller(series.dropna())\n",
        "\n",
        "        return {\n",
        "            'adf_statistic': float(result[0]),\n",
        "            'p_value': float(result[1]),\n",
        "            'critical_values': result[4],\n",
        "            'is_stationary': result[1] < 0.05\n",
        "        }\n",
        "\n",
        "    def apply_stl(self, window: pd.DataFrame, period: int = 24,\n",
        "                  seasonal: int = 7, trend: Optional[int] = None,\n",
        "                  robust: bool = True) -> Dict[str, pd.DataFrame]:\n",
        "        \"\"\"Apply STL decomposition to window.\"\"\"\n",
        "        sensor_cols = [col for col in window.columns if col.startswith('sensor_')]\n",
        "        decomposition_results = {}\n",
        "\n",
        "        for col in sensor_cols:\n",
        "            series = window[col].dropna()\n",
        "\n",
        "            if len(series) < 2 * period:\n",
        "                self.run.log({f\"skipped_{col}_insufficient_data\": True})\n",
        "                continue\n",
        "\n",
        "            # STL decomposition\n",
        "            stl = STL(series, period=period, seasonal=seasonal,\n",
        "                     trend=trend, robust=robust)\n",
        "            result = stl.fit()\n",
        "\n",
        "            # Store components\n",
        "            decomposition_results[col] = pd.DataFrame({\n",
        "                'original': series,\n",
        "                'trend': result.trend,\n",
        "                'seasonal': result.seasonal,\n",
        "                'residual': result.resid\n",
        "            })\n",
        "\n",
        "            # Calculate component strengths\n",
        "            var_total = np.var(series)\n",
        "            var_resid = np.var(result.resid)\n",
        "            var_seasonal = np.var(result.seasonal)\n",
        "\n",
        "            seasonal_strength = max(0, 1 - var_resid / (var_resid + var_seasonal))\n",
        "            trend_strength = max(0, 1 - var_resid / var_total)\n",
        "\n",
        "            # Test stationarity of residuals\n",
        "            residual_stationarity = self.test_stationarity(result.resid)\n",
        "\n",
        "            # Store statistics\n",
        "            self.decomposition_stats[col] = {\n",
        "                'seasonal_strength': float(seasonal_strength),\n",
        "                'trend_strength': float(trend_strength),\n",
        "                'residual_variance': float(var_resid),\n",
        "                'residual_stationary': residual_stationarity['is_stationary'],\n",
        "                'residual_adf_pvalue': residual_stationarity['p_value']\n",
        "            }\n",
        "\n",
        "            # Log metrics\n",
        "            self.run.log({\n",
        "                f\"{col}_seasonal_strength\": seasonal_strength,\n",
        "                f\"{col}_trend_strength\": trend_strength,\n",
        "                f\"{col}_residual_stationary\": residual_stationarity['is_stationary']\n",
        "            })\n",
        "\n",
        "        return decomposition_results\n",
        "\n",
        "    def create_decomposition_plot(self, decomposition: pd.DataFrame,\n",
        "                                 sensor_name: str) -> plt.Figure:\n",
        "        \"\"\"Create decomposition visualization.\"\"\"\n",
        "        fig, axes = plt.subplots(4, 1, figsize=(12, 10))\n",
        "\n",
        "        # Original\n",
        "        axes[0].plot(decomposition.index, decomposition['original'], 'b-', alpha=0.7)\n",
        "        axes[0].set_ylabel('Original')\n",
        "        axes[0].set_title(f'STL Decomposition: {sensor_name}')\n",
        "        axes[0].grid(True, alpha=0.3)\n",
        "\n",
        "        # Trend\n",
        "        axes[1].plot(decomposition.index, decomposition['trend'], 'r-', alpha=0.7)\n",
        "        axes[1].set_ylabel('Trend')\n",
        "        axes[1].grid(True, alpha=0.3)\n",
        "\n",
        "        # Seasonal\n",
        "        axes[2].plot(decomposition.index, decomposition['seasonal'], 'g-', alpha=0.7)\n",
        "        axes[2].set_ylabel('Seasonal')\n",
        "        axes[2].grid(True, alpha=0.3)\n",
        "\n",
        "        # Residual\n",
        "        axes[3].plot(decomposition.index, decomposition['residual'], 'm-', alpha=0.7)\n",
        "        axes[3].set_ylabel('Residual')\n",
        "        axes[3].set_xlabel('Time')\n",
        "        axes[3].grid(True, alpha=0.3)\n",
        "\n",
        "        # Add strength annotations\n",
        "        if sensor_name in self.decomposition_stats:\n",
        "            stats = self.decomposition_stats[sensor_name]\n",
        "            fig.text(0.02, 0.98,\n",
        "                    f\"Seasonal Strength: {stats['seasonal_strength']:.3f}\\n\"\n",
        "                    f\"Trend Strength: {stats['trend_strength']:.3f}\\n\"\n",
        "                    f\"Residual Stationary: {stats['residual_stationary']}\",\n",
        "                    transform=fig.transFigure, fontsize=10,\n",
        "                    verticalalignment='top',\n",
        "                    bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
        "\n",
        "        plt.tight_layout()\n",
        "        return fig\n",
        "\n",
        "    def decompose_windows(self, df: pd.DataFrame, window_size: int = 720,\n",
        "                         stride: int = 360, num_examples: int = 3):\n",
        "        \"\"\"Decompose multiple windows and log examples.\"\"\"\n",
        "        from src.windowing.segment import WindowSegmenter\n",
        "\n",
        "        segmenter = WindowSegmenter(self.run)\n",
        "        windows = list(segmenter.generate_windows(df, window_size, stride))\n",
        "\n",
        "        # Select example windows\n",
        "        example_indices = np.linspace(0, len(windows)-1, num_examples, dtype=int)\n",
        "\n",
        "        all_decompositions = []\n",
        "\n",
        "        for i, window_idx in enumerate(example_indices):\n",
        "            window = windows[window_idx]\n",
        "\n",
        "            # Apply STL decomposition\n",
        "            decomposition_results = self.apply_stl(window)\n",
        "            all_decompositions.append(decomposition_results)\n",
        "\n",
        "            # Create and log plots for first sensor\n",
        "            if decomposition_results:\n",
        "                first_sensor = list(decomposition_results.keys())[0]\n",
        "                fig = self.create_decomposition_plot(\n",
        "                    decomposition_results[first_sensor],\n",
        "                    first_sensor\n",
        "                )\n",
        "                self.run.log({f\"stl_plot_window_{i}\": wandb.Image(fig)})\n",
        "                plt.close()\n",
        "\n",
        "        # Log decomposition statistics table\n",
        "        stats_df = pd.DataFrame.from_dict(self.decomposition_stats, orient='index')\n",
        "        stats_df.index.name = 'sensor'\n",
        "        stats_df.reset_index(inplace=True)\n",
        "\n",
        "        table = wandb.Table(dataframe=stats_df)\n",
        "        self.run.log({\"decomposition_statistics\": table})\n",
        "\n",
        "        # Create summary visualization\n",
        "        self.create_summary_visualization()\n",
        "\n",
        "        return all_decompositions\n",
        "\n",
        "    def create_summary_visualization(self):\n",
        "        \"\"\"Create summary visualization of decomposition results.\"\"\"\n",
        "        if not self.decomposition_stats:\n",
        "            return\n",
        "\n",
        "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))\n",
        "\n",
        "        # Seasonal vs Trend strength scatter\n",
        "        sensors = list(self.decomposition_stats.keys())\n",
        "        seasonal_strengths = [self.decomposition_stats[s]['seasonal_strength'] for s in sensors]\n",
        "        trend_strengths = [self.decomposition_stats[s]['trend_strength'] for s in sensors]\n",
        "\n",
        "        scatter = ax1.scatter(seasonal_strengths, trend_strengths,\n",
        "                            c=range(len(sensors)), cmap='viridis', s=100, alpha=0.7)\n",
        "\n",
        "        for i, sensor in enumerate(sensors):\n",
        "            ax1.annotate(sensor, (seasonal_strengths[i], trend_strengths[i]),\n",
        "                        xytext=(5, 5), textcoords='offset points', fontsize=8)\n",
        "\n",
        "        ax1.set_xlabel('Seasonal Strength')\n",
        "        ax1.set_ylabel('Trend Strength')\n",
        "        ax1.set_title('Component Strength Analysis')\n",
        "        ax1.grid(True, alpha=0.3)\n",
        "        ax1.set_xlim(-0.1, 1.1)\n",
        "        ax1.set_ylim(-0.1, 1.1)\n",
        "\n",
        "        # Stationarity results\n",
        "        stationary_counts = sum(1 for s in self.decomposition_stats.values()\n",
        "                              if s['residual_stationary'])\n",
        "        non_stationary_counts = len(self.decomposition_stats) - stationary_counts\n",
        "\n",
        "        ax2.pie([stationary_counts, non_stationary_counts],\n",
        "               labels=['Stationary', 'Non-stationary'],\n",
        "               autopct='%1.1f%%', startangle=90,\n",
        "               colors=['lightgreen', 'lightcoral'])\n",
        "        ax2.set_title('Residual Stationarity Results')\n",
        "\n",
        "        plt.tight_layout()\n",
        "        self.run.log({\"decomposition_summary\": wandb.Image(fig)})\n",
        "        plt.close()\n",
        "\n",
        "    def save_decomposition_artifacts(self, decompositions: list,\n",
        "                                   output_dir: str = \"features\"):\n",
        "        \"\"\"Save decomposition results as artifacts.\"\"\"\n",
        "        import os\n",
        "        import json\n",
        "\n",
        "        os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "        # Save statistics\n",
        "        stats_path = f\"{output_dir}/decomposition_stats.json\"\n",
        "        with open(stats_path, 'w') as f:\n",
        "            json.dump(self.decomposition_stats, f, indent=2)\n",
        "\n",
        "        # Log as artifact\n",
        "        artifact = wandb.Artifact(\"decomposition-stats\", type=\"dataset\")\n",
        "        artifact.add_file(stats_path)\n",
        "        artifact.metadata = {\n",
        "            \"num_sensors\": len(self.decomposition_stats),\n",
        "            \"avg_seasonal_strength\": float(np.mean([s['seasonal_strength']\n",
        "                                                   for s in self.decomposition_stats.values()])),\n",
        "            \"stationary_ratio\": sum(1 for s in self.decomposition_stats.values()\n",
        "                                  if s['residual_stationary']) / len(self.decomposition_stats)\n",
        "        }\n",
        "        self.run.log_artifact(artifact)\n",
        "\n",
        "def main():\n",
        "    import argparse\n",
        "\n",
        "    parser = argparse.ArgumentParser(description=\"Decompose time series data\")\n",
        "    parser.add_argument(\"--input-file\", default=\"data/flagged.parquet\", help=\"Input file\")\n",
        "    parser.add_argument(\"--window-size\", type=int, default=720, help=\"Window size for decomposition\")\n",
        "    parser.add_argument(\"--stride\", type=int, default=360, help=\"Stride\")\n",
        "    parser.add_argument(\"--num-examples\", type=int, default=3, help=\"Number of example plots\")\n",
        "\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    # Initialize W&B\n",
        "    run = wandb.init(project=\"predictive-maintenance\", job_type=\"decomposition\")\n",
        "\n",
        "    # Load data\n",
        "    df = pd.read_parquet(args.input_file)\n",
        "\n",
        "    # Perform decomposition\n",
        "    decomposer = TimeSeriesDecomposer(run)\n",
        "    decompositions = decomposer.decompose_windows(\n",
        "        df, args.window_size, args.stride, args.num_examples\n",
        "    )\n",
        "\n",
        "    # Save artifacts\n",
        "    decomposer.save_decomposition_artifacts(decompositions)\n",
        "\n",
        "    run.finish()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()'''"
      ],
      "metadata": {
        "id": "SSgSOW6gExlW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile src/features/decompose.py\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import wandb\n",
        "from statsmodels.tsa.seasonal import STL\n",
        "from statsmodels.tsa.stattools import adfuller\n",
        "import matplotlib.pyplot as plt\n",
        "from typing import Dict, Tuple, Optional\n",
        "from src.utils.sensor_utils import get_sensor_columns, log_sensor_column_info\n",
        "class TimeSeriesDecomposer:\n",
        "    \"\"\"Perform seasonal and trend decomposition with W&B logging.\"\"\"\n",
        "\n",
        "    def __init__(self, wandb_run: wandb.run = None):\n",
        "        self.run = wandb_run or wandb.init(project=\"predictive-maintenance\")\n",
        "        self.decomposition_stats = {}\n",
        "\n",
        "    def test_stationarity(self, series: pd.Series) -> Dict:\n",
        "        \"\"\"Perform Augmented Dickey-Fuller test for stationarity.\"\"\"\n",
        "        # drop missing values\n",
        "        series_clean = series.dropna()\n",
        "\n",
        "        # if the series is constant or has zero variance, skip ADF\n",
        "        if series_clean.nunique() <= 1 or np.var(series_clean) == 0:\n",
        "            return {\n",
        "                'adf_statistic': np.nan,\n",
        "                'p_value': 1.0,\n",
        "                'critical_values': {},\n",
        "                'is_stationary': False\n",
        "            }\n",
        "\n",
        "        # run the ADF test\n",
        "        result = adfuller(series_clean)\n",
        "\n",
        "        return {\n",
        "            'adf_statistic': float(result[0]),\n",
        "            'p_value':        float(result[1]),\n",
        "            'critical_values': result[4],\n",
        "            'is_stationary':  bool(result[1] < 0.05)\n",
        "        }\n",
        "\n",
        "    def apply_stl(self, window: pd.DataFrame, period: int = 24,\n",
        "                  seasonal: int = 7, trend: Optional[int] = None,\n",
        "                  robust: bool = True) -> Dict[str, pd.DataFrame]:\n",
        "        \"\"\"Apply STL decomposition to window.\"\"\"\n",
        "        sensor_cols = get_sensor_columns(window)\n",
        "        decomposition_results = {}\n",
        "\n",
        "        for col in sensor_cols:\n",
        "            series = window[col].dropna()\n",
        "\n",
        "            if len(series) < 2 * period:\n",
        "                self.run.log({f\"skipped_{col}_insufficient_data\": True})\n",
        "                continue\n",
        "\n",
        "            # STL decomposition\n",
        "            stl = STL(series, period=period, seasonal=seasonal,\n",
        "                     trend=trend, robust=robust)\n",
        "            result = stl.fit()\n",
        "\n",
        "            # Store components\n",
        "            decomposition_results[col] = pd.DataFrame({\n",
        "                'original': series,\n",
        "                'trend': result.trend,\n",
        "                'seasonal': result.seasonal,\n",
        "                'residual': result.resid\n",
        "            })\n",
        "\n",
        "            # Calculate component strengths\n",
        "            var_total = np.var(series)\n",
        "            var_resid = np.var(result.resid)\n",
        "            var_seasonal = np.var(result.seasonal)\n",
        "\n",
        "            eps = 1e-8  # small constant to prevent division by zero\n",
        "\n",
        "            # Log warnings if variances are zero\n",
        "            if var_total == 0:\n",
        "                self.run.log({f\"{col}_zero_variance_total\": True})\n",
        "            if (var_resid + var_seasonal) == 0:\n",
        "                self.run.log({f\"{col}_zero_variance_resid_plus_seasonal\": True})\n",
        "\n",
        "            seasonal_strength = max(0, 1 - var_resid / (var_resid + var_seasonal + eps))\n",
        "            trend_strength = max(0, 1 - var_resid / (var_total + eps))\n",
        "\n",
        "            # Test stationarity of residuals\n",
        "            residual_stationarity = self.test_stationarity(result.resid)\n",
        "\n",
        "            # Store statistics\n",
        "            self.decomposition_stats[col] = {\n",
        "                'seasonal_strength': float(seasonal_strength),\n",
        "                'trend_strength': float(trend_strength),\n",
        "                'residual_variance': float(var_resid),\n",
        "                'residual_stationary': residual_stationarity['is_stationary'],\n",
        "                'residual_adf_pvalue': residual_stationarity['p_value']\n",
        "            }\n",
        "\n",
        "            # Log metrics\n",
        "            self.run.log({\n",
        "                f\"{col}_seasonal_strength\": seasonal_strength,\n",
        "                f\"{col}_trend_strength\": trend_strength,\n",
        "                f\"{col}_residual_stationary\": residual_stationarity['is_stationary']\n",
        "            })\n",
        "\n",
        "        return decomposition_results\n",
        "\n",
        "    def create_decomposition_plot(self, decomposition: pd.DataFrame,\n",
        "                                 sensor_name: str) -> plt.Figure:\n",
        "        \"\"\"Create decomposition visualization.\"\"\"\n",
        "        fig, axes = plt.subplots(4, 1, figsize=(12, 10))\n",
        "\n",
        "        # Original\n",
        "        axes[0].plot(decomposition.index, decomposition['original'], 'b-', alpha=0.7)\n",
        "        axes[0].set_ylabel('Original')\n",
        "        axes[0].set_title(f'STL Decomposition: {sensor_name}')\n",
        "        axes[0].grid(True, alpha=0.3)\n",
        "\n",
        "        # Trend\n",
        "        axes[1].plot(decomposition.index, decomposition['trend'], 'r-', alpha=0.7)\n",
        "        axes[1].set_ylabel('Trend')\n",
        "        axes[1].grid(True, alpha=0.3)\n",
        "\n",
        "        # Seasonal\n",
        "        axes[2].plot(decomposition.index, decomposition['seasonal'], 'g-', alpha=0.7)\n",
        "        axes[2].set_ylabel('Seasonal')\n",
        "        axes[2].grid(True, alpha=0.3)\n",
        "\n",
        "        # Residual\n",
        "        axes[3].plot(decomposition.index, decomposition['residual'], 'm-', alpha=0.7)\n",
        "        axes[3].set_ylabel('Residual')\n",
        "        axes[3].set_xlabel('Time')\n",
        "        axes[3].grid(True, alpha=0.3)\n",
        "\n",
        "        # Add strength annotations\n",
        "        if sensor_name in self.decomposition_stats:\n",
        "            stats = self.decomposition_stats[sensor_name]\n",
        "            fig.text(0.02, 0.98,\n",
        "                    f\"Seasonal Strength: {stats['seasonal_strength']:.3f}\\n\"\n",
        "                    f\"Trend Strength: {stats['trend_strength']:.3f}\\n\"\n",
        "                    f\"Residual Stationary: {stats['residual_stationary']}\",\n",
        "                    transform=fig.transFigure, fontsize=10,\n",
        "                    verticalalignment='top',\n",
        "                    bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
        "\n",
        "        plt.tight_layout()\n",
        "        return fig\n",
        "\n",
        "    def decompose_windows(self, df: pd.DataFrame, window_size: int = 720,\n",
        "                         stride: int = 360, num_examples: int = 3):\n",
        "        \"\"\"Decompose multiple windows and log examples.\"\"\"\n",
        "        from src.windowing.segment import WindowSegmenter\n",
        "\n",
        "        segmenter = WindowSegmenter(self.run)\n",
        "        windows = list(segmenter.generate_windows(df, window_size, stride))\n",
        "\n",
        "        # Select example windows\n",
        "        example_indices = np.linspace(0, len(windows)-1, num_examples, dtype=int)\n",
        "\n",
        "        all_decompositions = []\n",
        "\n",
        "        for i, window_idx in enumerate(example_indices):\n",
        "            window = windows[window_idx]\n",
        "\n",
        "            # Apply STL decomposition\n",
        "            decomposition_results = self.apply_stl(window)\n",
        "            all_decompositions.append(decomposition_results)\n",
        "\n",
        "            # Create and log plots for first sensor\n",
        "            if decomposition_results:\n",
        "                first_sensor = list(decomposition_results.keys())[0]\n",
        "                fig = self.create_decomposition_plot(\n",
        "                    decomposition_results[first_sensor],\n",
        "                    first_sensor\n",
        "                )\n",
        "                self.run.log({f\"stl_plot_window_{i}\": wandb.Image(fig)})\n",
        "                plt.close()\n",
        "\n",
        "        # Log decomposition statistics table\n",
        "        stats_df = pd.DataFrame.from_dict(self.decomposition_stats, orient='index')\n",
        "        stats_df.index.name = 'sensor'\n",
        "        stats_df.reset_index(inplace=True)\n",
        "\n",
        "        table = wandb.Table(dataframe=stats_df)\n",
        "        self.run.log({\"decomposition_statistics\": table})\n",
        "\n",
        "        # Create summary visualization\n",
        "        self.create_summary_visualization()\n",
        "\n",
        "        return all_decompositions\n",
        "\n",
        "    def create_summary_visualization(self):\n",
        "        \"\"\"Create summary visualization of decomposition results.\"\"\"\n",
        "        if not self.decomposition_stats:\n",
        "            return\n",
        "\n",
        "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))\n",
        "\n",
        "        # Seasonal vs Trend strength scatter\n",
        "        sensors = list(self.decomposition_stats.keys())\n",
        "        seasonal_strengths = [self.decomposition_stats[s]['seasonal_strength'] for s in sensors]\n",
        "        trend_strengths = [self.decomposition_stats[s]['trend_strength'] for s in sensors]\n",
        "\n",
        "        scatter = ax1.scatter(seasonal_strengths, trend_strengths,\n",
        "                            c=range(len(sensors)), cmap='viridis', s=100, alpha=0.7)\n",
        "\n",
        "        for i, sensor in enumerate(sensors):\n",
        "            ax1.annotate(sensor, (seasonal_strengths[i], trend_strengths[i]),\n",
        "                        xytext=(5, 5), textcoords='offset points', fontsize=8)\n",
        "\n",
        "        ax1.set_xlabel('Seasonal Strength')\n",
        "        ax1.set_ylabel('Trend Strength')\n",
        "        ax1.set_title('Component Strength Analysis')\n",
        "        ax1.grid(True, alpha=0.3)\n",
        "        ax1.set_xlim(-0.1, 1.1)\n",
        "        ax1.set_ylim(-0.1, 1.1)\n",
        "\n",
        "        # Stationarity results\n",
        "        stationary_counts = sum(1 for s in self.decomposition_stats.values()\n",
        "                              if s['residual_stationary'])\n",
        "        non_stationary_counts = len(self.decomposition_stats) - stationary_counts\n",
        "\n",
        "        ax2.pie([stationary_counts, non_stationary_counts],\n",
        "               labels=['Stationary', 'Non-stationary'],\n",
        "               autopct='%1.1f%%', startangle=90,\n",
        "               colors=['lightgreen', 'lightcoral'])\n",
        "        ax2.set_title('Residual Stationarity Results')\n",
        "\n",
        "        plt.tight_layout()\n",
        "        self.run.log({\"decomposition_summary\": wandb.Image(fig)})\n",
        "        plt.close()\n",
        "\n",
        "    def save_decomposition_artifacts(self, decompositions: list,\n",
        "                                  output_dir: str = \"features\"):\n",
        "        \"\"\"Save decomposition results as artifacts.\"\"\"\n",
        "        import os\n",
        "        import json\n",
        "\n",
        "        os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "        # Save statistics\n",
        "        stats_path = f\"{output_dir}/decomposition_stats.json\"\n",
        "        with open(stats_path, 'w') as f:\n",
        "            json.dump(self.decomposition_stats, f, indent=2)\n",
        "\n",
        "        # Log as artifact\n",
        "        artifact = wandb.Artifact(\"decomposition-stats\", type=\"dataset\")\n",
        "        artifact.add_file(stats_path)\n",
        "        artifact.metadata = {\n",
        "            \"num_sensors\": len(self.decomposition_stats),\n",
        "            \"avg_seasonal_strength\": float(np.mean([s['seasonal_strength']\n",
        "                                                  for s in self.decomposition_stats.values()])),\n",
        "            \"stationary_ratio\": sum(1 for s in self.decomposition_stats.values()\n",
        "                                  if s['residual_stationary']) / len(self.decomposition_stats)\n",
        "        }\n",
        "        self.run.log_artifact(artifact)\n",
        "\n",
        "def main():\n",
        "    import argparse\n",
        "\n",
        "    parser = argparse.ArgumentParser(description=\"Decompose time series data\")\n",
        "    parser.add_argument(\"--input-file\", default=\"data/flagged.parquet\", help=\"Input file\")\n",
        "    parser.add_argument(\"--window-size\", type=int, default=720, help=\"Window size for decomposition\")\n",
        "    parser.add_argument(\"--stride\", type=int, default=360, help=\"Stride\")\n",
        "    parser.add_argument(\"--num-examples\", type=int, default=3, help=\"Number of example plots\")\n",
        "\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    # Initialize W&B\n",
        "    run = wandb.init(project=\"predictive-maintenance\", job_type=\"decomposition\")\n",
        "\n",
        "    # Load data\n",
        "    df = pd.read_parquet(args.input_file)\n",
        "\n",
        "    # Perform decomposition\n",
        "    decomposer = TimeSeriesDecomposer(run)\n",
        "    decompositions = decomposer.decompose_windows(\n",
        "        df, args.window_size, args.stride, args.num_examples\n",
        "    )\n",
        "\n",
        "    # Save artifacts\n",
        "    decomposer.save_decomposition_artifacts(decompositions)\n",
        "\n",
        "    run.finish()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "Uq-uSGEDk1_E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile src/features/extract.py\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import wandb\n",
        "from scipy import stats\n",
        "from typing import Dict, List, Optional\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Import the new sensor column utility\n",
        "from src.utils.sensor_utils import get_sensor_columns, get_feature_columns, log_sensor_column_info\n",
        "\n",
        "\n",
        "class FeatureExtractor:\n",
        "    \"\"\"\n",
        "    Extract statistical and temporal features from sensor data windows.\n",
        "    Now uses DYNAMIC sensor column identification instead of hardcoded 'sensor_' prefix.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, wandb_run: wandb.run = None):\n",
        "        self.run = wandb_run or wandb.init(project=\"predictive-maintenance\")\n",
        "        self.feature_stats = {}\n",
        "\n",
        "    def extract_statistical_features(self, window: pd.DataFrame) -> Dict[str, float]:\n",
        "        \"\"\"Extract statistical features from a single window.\"\"\"\n",
        "        features = {}\n",
        "\n",
        "        # Use dynamic sensor column identification\n",
        "        sensor_cols = get_sensor_columns(window)\n",
        "\n",
        "        for col in sensor_cols:\n",
        "            data = window[col].dropna()\n",
        "\n",
        "            if len(data) == 0:\n",
        "                # Handle empty series\n",
        "                prefix = col\n",
        "                features.update({\n",
        "                    f'{prefix}_mean': 0.0,\n",
        "                    f'{prefix}_std': 0.0,\n",
        "                    f'{prefix}_min': 0.0,\n",
        "                    f'{prefix}_max': 0.0,\n",
        "                    f'{prefix}_median': 0.0,\n",
        "                    f'{prefix}_q25': 0.0,\n",
        "                    f'{prefix}_q75': 0.0,\n",
        "                    f'{prefix}_iqr': 0.0,\n",
        "                    f'{prefix}_skew': 0.0,\n",
        "                    f'{prefix}_kurt': 0.0,\n",
        "                    f'{prefix}_valid_ratio': 0.0\n",
        "                })\n",
        "                continue\n",
        "\n",
        "            prefix = col\n",
        "\n",
        "            # Basic statistics\n",
        "            features[f'{prefix}_mean'] = float(data.mean())\n",
        "            features[f'{prefix}_std'] = float(data.std())\n",
        "            features[f'{prefix}_min'] = float(data.min())\n",
        "            features[f'{prefix}_max'] = float(data.max())\n",
        "            features[f'{prefix}_median'] = float(data.median())\n",
        "\n",
        "            # Quantiles\n",
        "            features[f'{prefix}_q25'] = float(data.quantile(0.25))\n",
        "            features[f'{prefix}_q75'] = float(data.quantile(0.75))\n",
        "            features[f'{prefix}_iqr'] = features[f'{prefix}_q75'] - features[f'{prefix}_q25']\n",
        "\n",
        "            # Distribution shape\n",
        "            features[f'{prefix}_skew'] = float(data.skew())\n",
        "            features[f'{prefix}_kurt'] = float(data.kurtosis())\n",
        "\n",
        "            # Data quality\n",
        "            features[f'{prefix}_valid_ratio'] = float(len(data) / len(window))\n",
        "\n",
        "            # Range and variation\n",
        "            features[f'{prefix}_range'] = features[f'{prefix}_max'] - features[f'{prefix}_min']\n",
        "            features[f'{prefix}_cv'] = features[f'{prefix}_std'] / features[f'{prefix}_mean'] if features[f'{prefix}_mean'] != 0 else 0.0\n",
        "\n",
        "            # Peak detection (simple)\n",
        "            if len(data) > 5:\n",
        "                features[f'{prefix}_peak_count'] = float(len(data[(data > data.quantile(0.95))]))\n",
        "                features[f'{prefix}_valley_count'] = float(len(data[(data < data.quantile(0.05))]))\n",
        "            else:\n",
        "                features[f'{prefix}_peak_count'] = 0.0\n",
        "                features[f'{prefix}_valley_count'] = 0.0\n",
        "\n",
        "        return features\n",
        "\n",
        "    def extract_temporal_features(self, window: pd.DataFrame) -> Dict[str, float]:\n",
        "        \"\"\"Extract temporal and trend features.\"\"\"\n",
        "        features = {}\n",
        "\n",
        "        # Use dynamic sensor column identification\n",
        "        sensor_cols = get_sensor_columns(window)\n",
        "\n",
        "        for col in sensor_cols:\n",
        "            data = window[col].dropna()\n",
        "\n",
        "            if len(data) < 2:\n",
        "                prefix = col\n",
        "                features.update({\n",
        "                    f'{prefix}_trend_slope': 0.0,\n",
        "                    f'{prefix}_trend_r2': 0.0,\n",
        "                    f'{prefix}_autocorr_lag1': 0.0,\n",
        "                    f'{prefix}_zero_crossings': 0.0,\n",
        "                    f'{prefix}_mean_abs_change': 0.0\n",
        "                })\n",
        "                continue\n",
        "\n",
        "            prefix = col\n",
        "\n",
        "            # Trend analysis\n",
        "            if len(data) >= 2:\n",
        "                x = np.arange(len(data))\n",
        "                try:\n",
        "                    slope, intercept, r_value, p_value, std_err = stats.linregress(x, data.values)\n",
        "                    features[f'{prefix}_trend_slope'] = float(slope)\n",
        "                    features[f'{prefix}_trend_r2'] = float(r_value ** 2)\n",
        "                except:\n",
        "                    features[f'{prefix}_trend_slope'] = 0.0\n",
        "                    features[f'{prefix}_trend_r2'] = 0.0\n",
        "\n",
        "            # Autocorrelation (lag-1)\n",
        "            if len(data) > 2:\n",
        "                try:\n",
        "                    autocorr = data.autocorr(lag=1)\n",
        "                    features[f'{prefix}_autocorr_lag1'] = float(autocorr) if not np.isnan(autocorr) else 0.0\n",
        "                except:\n",
        "                    features[f'{prefix}_autocorr_lag1'] = 0.0\n",
        "            else:\n",
        "                features[f'{prefix}_autocorr_lag1'] = 0.0\n",
        "\n",
        "            # Zero crossings (relative to mean)\n",
        "            mean_centered = data - data.mean()\n",
        "            zero_crossings = ((mean_centered[:-1] * mean_centered[1:]) < 0).sum()\n",
        "            features[f'{prefix}_zero_crossings'] = float(zero_crossings)\n",
        "\n",
        "            # Mean absolute change\n",
        "            if len(data) > 1:\n",
        "                abs_changes = np.abs(data.diff()).dropna()\n",
        "                features[f'{prefix}_mean_abs_change'] = float(abs_changes.mean()) if len(abs_changes) > 0 else 0.0\n",
        "            else:\n",
        "                features[f'{prefix}_mean_abs_change'] = 0.0\n",
        "\n",
        "        return features\n",
        "\n",
        "    def extract_frequency_features(self, window: pd.DataFrame) -> Dict[str, float]:\n",
        "        \"\"\"Extract frequency domain features using FFT.\"\"\"\n",
        "        features = {}\n",
        "\n",
        "        # Use dynamic sensor column identification\n",
        "        sensor_cols = get_sensor_columns(window)\n",
        "\n",
        "        for col in sensor_cols:\n",
        "            data = window[col].dropna()\n",
        "\n",
        "            if len(data) < 8:  # Need minimum points for FFT\n",
        "                prefix = col\n",
        "                features.update({\n",
        "                    f'{prefix}_dominant_freq': 0.0,\n",
        "                    f'{prefix}_spectral_energy': 0.0,\n",
        "                    f'{prefix}_spectral_centroid': 0.0,\n",
        "                    f'{prefix}_spectral_rolloff': 0.0\n",
        "                })\n",
        "                continue\n",
        "\n",
        "            prefix = col\n",
        "\n",
        "            try:\n",
        "                # Compute FFT\n",
        "                fft_vals = np.fft.fft(data.values)\n",
        "                fft_freq = np.fft.fftfreq(len(data))\n",
        "\n",
        "                # Power spectrum\n",
        "                power_spectrum = np.abs(fft_vals) ** 2\n",
        "\n",
        "                # Dominant frequency\n",
        "                dominant_freq_idx = np.argmax(power_spectrum[1:len(power_spectrum)//2]) + 1\n",
        "                features[f'{prefix}_dominant_freq'] = float(np.abs(fft_freq[dominant_freq_idx]))\n",
        "\n",
        "                # Spectral energy\n",
        "                features[f'{prefix}_spectral_energy'] = float(np.sum(power_spectrum))\n",
        "\n",
        "                # Spectral centroid\n",
        "                freq_positive = fft_freq[:len(fft_freq)//2]\n",
        "                power_positive = power_spectrum[:len(power_spectrum)//2]\n",
        "                if np.sum(power_positive) > 0:\n",
        "                    spectral_centroid = np.sum(freq_positive * power_positive) / np.sum(power_positive)\n",
        "                    features[f'{prefix}_spectral_centroid'] = float(np.abs(spectral_centroid))\n",
        "                else:\n",
        "                    features[f'{prefix}_spectral_centroid'] = 0.0\n",
        "\n",
        "                # Spectral rolloff (frequency below which 85% of energy is contained)\n",
        "                cumsum_power = np.cumsum(power_positive)\n",
        "                total_power = cumsum_power[-1]\n",
        "                rolloff_idx = np.where(cumsum_power >= 0.85 * total_power)[0]\n",
        "                if len(rolloff_idx) > 0:\n",
        "                    features[f'{prefix}_spectral_rolloff'] = float(np.abs(freq_positive[rolloff_idx[0]]))\n",
        "                else:\n",
        "                    features[f'{prefix}_spectral_rolloff'] = 0.0\n",
        "\n",
        "            except Exception as e:\n",
        "                # Fallback values if FFT fails\n",
        "                prefix = col\n",
        "                features.update({\n",
        "                    f'{prefix}_dominant_freq': 0.0,\n",
        "                    f'{prefix}_spectral_energy': 0.0,\n",
        "                    f'{prefix}_spectral_centroid': 0.0,\n",
        "                    f'{prefix}_spectral_rolloff': 0.0\n",
        "                })\n",
        "\n",
        "        return features\n",
        "\n",
        "    def extract_features(self, window: pd.DataFrame) -> pd.Series:\n",
        "        \"\"\"Extract all features from a single window.\"\"\"\n",
        "        features = {}\n",
        "\n",
        "        # Use dynamic sensor column identification\n",
        "        sensor_cols = get_sensor_columns(window)\n",
        "\n",
        "        if not sensor_cols:\n",
        "            # If no sensor columns found, return empty features\n",
        "            return pd.Series(features)\n",
        "\n",
        "        # Extract different types of features\n",
        "        statistical_features = self.extract_statistical_features(window)\n",
        "        temporal_features = self.extract_temporal_features(window)\n",
        "        frequency_features = self.extract_frequency_features(window)\n",
        "\n",
        "        # Combine all features\n",
        "        features.update(statistical_features)\n",
        "        features.update(temporal_features)\n",
        "        features.update(frequency_features)\n",
        "\n",
        "        # Add engineering features if we have at least one valid feature\n",
        "        for feat_name, feat_val in features.items():\n",
        "            if np.isnan(feat_val) or np.isinf(feat_val):\n",
        "                features[feat_name] = 0.0\n",
        "            else:\n",
        "                features[feat_name] = feat_val\n",
        "\n",
        "        # Window-level metadata features\n",
        "        features['window_start'] = window.index[0].timestamp()\n",
        "        features['window_hour'] = window.index[0].hour\n",
        "        features['window_dayofweek'] = window.index[0].dayofweek\n",
        "\n",
        "        # Cross-sensor features (if multiple sensors)\n",
        "        if len(sensor_cols) > 1:\n",
        "            # Correlation matrix\n",
        "            sensor_data = window[sensor_cols].dropna()\n",
        "            if len(sensor_data) > 10:\n",
        "                try:\n",
        "                    corr_matrix = sensor_data.corr()\n",
        "\n",
        "                    # Extract upper triangle of correlation matrix\n",
        "                    for i, col1 in enumerate(sensor_cols):\n",
        "                        for j, col2 in enumerate(sensor_cols[i+1:], i+1):\n",
        "                            corr_val = corr_matrix.loc[col1, col2]\n",
        "                            if not np.isnan(corr_val):\n",
        "                                features[f'corr_{col1}_{col2}'] = float(corr_val)\n",
        "                            else:\n",
        "                                features[f'corr_{col1}_{col2}'] = 0.0\n",
        "                except:\n",
        "                    # Skip correlation features if calculation fails\n",
        "                    pass\n",
        "\n",
        "        return pd.Series(features)\n",
        "\n",
        "    def extract_window_features(self, df: pd.DataFrame, window_size: int = 60,\n",
        "                              stride: int = 30) -> pd.DataFrame:\n",
        "        \"\"\"Extract features from all windows in the dataframe.\"\"\"\n",
        "        from src.windowing.segment import WindowSegmenter\n",
        "\n",
        "        segmenter = WindowSegmenter(self.run)\n",
        "        windows = list(segmenter.generate_windows(df, window_size, stride))\n",
        "\n",
        "        self.run.log({\n",
        "            \"total_windows_for_extraction\": len(windows),\n",
        "            \"feature_extraction_method\": \"dynamic_sensor_identification\"\n",
        "        })\n",
        "\n",
        "        # Extract features for each window\n",
        "        feature_list = []\n",
        "        for i, window in enumerate(windows):\n",
        "            window_features = self.extract_features(window)\n",
        "            window_features['window_id'] = i\n",
        "            feature_list.append(window_features)\n",
        "\n",
        "            # Log progress\n",
        "            if (i + 1) % 100 == 0:\n",
        "                self.run.log({\"features_extracted\": i + 1})\n",
        "\n",
        "        # Create feature matrix\n",
        "        feature_matrix = pd.DataFrame(feature_list)\n",
        "\n",
        "        # Calculate and log feature statistics\n",
        "        self.calculate_feature_statistics(feature_matrix)\n",
        "\n",
        "        return feature_matrix\n",
        "\n",
        "    def calculate_feature_statistics(self, feature_matrix: pd.DataFrame):\n",
        "        \"\"\"Calculate and log statistics about extracted features.\"\"\"\n",
        "        # Basic statistics\n",
        "        num_features = len(feature_matrix.columns) - 2  # Exclude window_id and window_start\n",
        "        num_windows = len(feature_matrix)\n",
        "\n",
        "        # Use dynamic sensor identification for logging\n",
        "        sensor_cols = get_sensor_columns(feature_matrix)\n",
        "\n",
        "        self.run.log({\n",
        "            \"feature_extraction_total_features\": num_features,\n",
        "            \"feature_extraction_total_windows\": num_windows,\n",
        "            \"feature_extraction_sensor_columns\": len(sensor_cols),\n",
        "            \"feature_matrix_shape\": str(feature_matrix.shape)\n",
        "        })\n",
        "\n",
        "        # Feature completeness\n",
        "        completeness = feature_matrix.notna().mean()\n",
        "        self.run.log({\n",
        "            \"feature_completeness_mean\": float(completeness.mean()),\n",
        "            \"feature_completeness_min\": float(completeness.min()),\n",
        "            \"features_with_missing\": int((completeness < 1.0).sum())\n",
        "        })\n",
        "\n",
        "        # Identify potential feature types\n",
        "        statistical_features = len([col for col in feature_matrix.columns if any(stat in col for stat in ['_mean', '_std', '_min', '_max'])])\n",
        "        temporal_features = len([col for col in feature_matrix.columns if any(temp in col for temp in ['_trend', '_autocorr', '_change'])])\n",
        "        frequency_features = len([col for col in feature_matrix.columns if any(freq in col for freq in ['_freq', '_spectral'])])\n",
        "        correlation_features = len([col for col in feature_matrix.columns if 'corr_' in col])\n",
        "\n",
        "        self.run.log({\n",
        "            \"statistical_features_count\": statistical_features,\n",
        "            \"temporal_features_count\": temporal_features,\n",
        "            \"frequency_features_count\": frequency_features,\n",
        "            \"correlation_features_count\": correlation_features\n",
        "        })\n",
        "\n",
        "        # Store statistics\n",
        "        self.feature_stats = {\n",
        "            'total_features': num_features,\n",
        "            'total_windows': num_windows,\n",
        "            'completeness_stats': completeness.describe().to_dict(),\n",
        "            'feature_types': {\n",
        "                'statistical': statistical_features,\n",
        "                'temporal': temporal_features,\n",
        "                'frequency': frequency_features,\n",
        "                'correlation': correlation_features\n",
        "            }\n",
        "        }\n",
        "\n",
        "    def save_feature_matrix(self, feature_matrix: pd.DataFrame, output_path: str):\n",
        "        \"\"\"\n",
        "        Save the feature matrix to a CSV file and log it as a W&B artifact.\n",
        "        Keeps logic minimal to match existing pipeline calls.\n",
        "        \"\"\"\n",
        "        import os\n",
        "        os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
        "\n",
        "        # Save locally\n",
        "        feature_matrix.to_csv(output_path, index=False)\n",
        "\n",
        "        # Log as W&B artifact if a run is active\n",
        "        try:\n",
        "            import wandb\n",
        "            artifact = wandb.Artifact(name=os.path.basename(output_path), type=\"dataset\")\n",
        "            artifact.add_file(output_path)\n",
        "            self.run.log_artifact(artifact)\n",
        "        except Exception as e:\n",
        "            print(f\"[Warning] Could not log feature matrix to W&B: {e}\")\n",
        "\n",
        "def main():\n",
        "    import argparse\n",
        "\n",
        "    parser = argparse.ArgumentParser(description=\"Extract features from windowed sensor data\")\n",
        "    parser.add_argument(\"--input-file\", default=\"data/imputed.parquet\", help=\"Input file\")\n",
        "    parser.add_argument(\"--output-file\", default=\"data/features.parquet\", help=\"Output file\")\n",
        "    parser.add_argument(\"--window-size\", type=int, default=60, help=\"Window size in minutes\")\n",
        "    parser.add_argument(\"--stride\", type=int, default=30, help=\"Stride between windows in minutes\")\n",
        "\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    # Initialize W&B\n",
        "    run = wandb.init(project=\"predictive-maintenance\", job_type=\"feature-extraction\")\n",
        "\n",
        "    # Load data\n",
        "    print(f\"Loading data from {args.input_file}...\")\n",
        "    df = pd.read_parquet(args.input_file)\n",
        "    print(f\"Loaded data shape: {df.shape}\")\n",
        "\n",
        "    # Log sensor column information using dynamic identification\n",
        "    from src.utils.sensor_utils import get_sensor_columns, log_sensor_column_info\n",
        "    sensor_cols = get_sensor_columns(df)\n",
        "    sensor_info = log_sensor_column_info(df, sensor_cols, run)\n",
        "\n",
        "    print(f\"Identified {len(sensor_cols)} sensor columns using dynamic identification\")\n",
        "    print(f\"Sensor columns: {sensor_cols[:10]}{'...' if len(sensor_cols) > 10 else ''}\")\n",
        "\n",
        "    # Extract features\n",
        "    print(f\"Extracting features with window_size={args.window_size}, stride={args.stride}...\")\n",
        "    extractor = FeatureExtractor(run)\n",
        "    feature_matrix = extractor.extract_window_features(\n",
        "        df,\n",
        "        window_size=args.window_size,\n",
        "        stride=args.stride\n",
        "    )\n",
        "\n",
        "    print(f\"Extracted feature matrix shape: {feature_matrix.shape}\")\n",
        "    print(f\"Total features per window: {len(feature_matrix.columns) - 2}\")  # Exclude metadata columns\n",
        "\n",
        "    # Save results\n",
        "    print(f\"Saving features to {args.output_file}...\")\n",
        "    feature_matrix.to_parquet(args.output_file)\n",
        "\n",
        "    # Log as artifact\n",
        "    artifact = wandb.Artifact(\"extracted-features\", type=\"features\")\n",
        "    artifact.add_file(args.output_file)\n",
        "    artifact.metadata = {\n",
        "        \"window_size\": args.window_size,\n",
        "        \"stride\": args.stride,\n",
        "        \"total_features\": len(feature_matrix.columns) - 2,\n",
        "        \"total_windows\": len(feature_matrix),\n",
        "        \"sensor_columns_used\": len(sensor_cols),\n",
        "        \"sensor_identification_method\": \"dynamic_numeric_columns\",\n",
        "        \"feature_types\": extractor.feature_stats.get('feature_types', {}),\n",
        "        \"completeness_mean\": extractor.feature_stats.get('completeness_stats', {}).get('mean', 0)\n",
        "    }\n",
        "    run.log_artifact(artifact)\n",
        "\n",
        "    run.finish()\n",
        "    print(\"Feature extraction completed!\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "lHRz8hVpE886"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile src/scaling/scale.py\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import wandb\n",
        "from sklearn.preprocessing import RobustScaler, StandardScaler, MinMaxScaler\n",
        "import joblib\n",
        "import matplotlib.pyplot as plt\n",
        "from typing import Tuple, Optional\n",
        "\n",
        "# Import the new sensor column utility\n",
        "from src.utils.sensor_utils import get_feature_columns, log_sensor_column_info\n",
        "\n",
        "\n",
        "class FeatureScaler:\n",
        "    \"\"\"\n",
        "    Scale features with robust methods and W&B tracking.\n",
        "    Now uses DYNAMIC feature column identification instead of hardcoded patterns.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, wandb_run: wandb.run = None):\n",
        "        self.run = wandb_run or wandb.init(project=\"predictive-maintenance\")\n",
        "        self.scaler = None\n",
        "        self.scaling_stats = {}\n",
        "\n",
        "    def fit_scaler(self, baseline_features: pd.DataFrame,\n",
        "                   method: str = 'robust') -> RobustScaler:\n",
        "        \"\"\"Fit scaler on baseline features using dynamic column identification.\"\"\"\n",
        "        # Use dynamic feature column identification\n",
        "        feature_cols = get_feature_columns(baseline_features, exclude_metadata=True)\n",
        "\n",
        "        # Log feature identification info\n",
        "        log_sensor_column_info(baseline_features, feature_cols, self.run)\n",
        "\n",
        "        self.run.log({\n",
        "            \"scaling_method\": method,\n",
        "            \"scaling_feature_count\": len(feature_cols),\n",
        "            \"scaling_identification_method\": \"dynamic_feature_columns\"\n",
        "        })\n",
        "\n",
        "        # Select scaler\n",
        "        if method == 'robust':\n",
        "            self.scaler = RobustScaler()\n",
        "        elif method == 'standard':\n",
        "            self.scaler = StandardScaler()\n",
        "        elif method == 'minmax':\n",
        "            self.scaler = MinMaxScaler()\n",
        "        else:\n",
        "            raise ValueError(f\"Unknown scaling method: {method}\")\n",
        "\n",
        "        # Fit on baseline data\n",
        "        baseline_data = baseline_features[feature_cols]\n",
        "        self.scaler.fit(baseline_data)\n",
        "\n",
        "        # Calculate and log fit statistics\n",
        "        if method == 'robust':\n",
        "            centers = self.scaler.center_\n",
        "            scales = self.scaler.scale_\n",
        "\n",
        "            self.scaling_stats = {\n",
        "                'method': method,\n",
        "                'num_features': len(feature_cols),\n",
        "                'median_center': float(np.median(centers)),\n",
        "                'median_scale': float(np.median(scales)),\n",
        "                'features_with_zero_scale': int(np.sum(scales == 0))\n",
        "            }\n",
        "        elif method == 'standard':\n",
        "            means = self.scaler.mean_\n",
        "            stds = self.scaler.scale_\n",
        "\n",
        "            self.scaling_stats = {\n",
        "                'method': method,\n",
        "                'num_features': len(feature_cols),\n",
        "                'mean_center': float(np.mean(means)),\n",
        "                'mean_scale': float(np.mean(stds)),\n",
        "                'features_with_zero_scale': int(np.sum(stds == 0))\n",
        "            }\n",
        "        elif method == 'minmax':\n",
        "            mins = self.scaler.data_min_\n",
        "            maxs = self.scaler.data_max_\n",
        "            ranges = maxs - mins\n",
        "\n",
        "            self.scaling_stats = {\n",
        "                'method': method,\n",
        "                'num_features': len(feature_cols),\n",
        "                'mean_min': float(np.mean(mins)),\n",
        "                'mean_max': float(np.mean(maxs)),\n",
        "                'mean_range': float(np.mean(ranges)),\n",
        "                'features_with_zero_range': int(np.sum(ranges == 0))\n",
        "            }\n",
        "\n",
        "        # Log scaler statistics\n",
        "        for key, value in self.scaling_stats.items():\n",
        "            self.run.log({f\"scaler_{key}\": value})\n",
        "\n",
        "        return self.scaler\n",
        "\n",
        "    def transform_features(self, features: pd.DataFrame) -> pd.DataFrame:\n",
        "        \"\"\"Transform features using fitted scaler with dynamic column identification.\"\"\"\n",
        "        if self.scaler is None:\n",
        "            raise ValueError(\"Scaler must be fitted before transforming\")\n",
        "\n",
        "        # Use dynamic feature column identification\n",
        "        feature_cols = get_feature_columns(features, exclude_metadata=True)\n",
        "\n",
        "        # Create copy for transformation\n",
        "        scaled_features = features.copy()\n",
        "\n",
        "        try:\n",
        "            # Transform only the feature columns\n",
        "            feature_data = features[feature_cols]\n",
        "            scaled_data = self.scaler.transform(feature_data)\n",
        "            scaled_features[feature_cols] = scaled_data\n",
        "\n",
        "            # Log transformation statistics\n",
        "            self.run.log({\n",
        "                \"transform_feature_count\": len(feature_cols),\n",
        "                \"transform_sample_count\": len(features),\n",
        "                \"transform_total_values\": len(features) * len(feature_cols)\n",
        "            })\n",
        "\n",
        "            # Check for any scaling issues\n",
        "            scaled_means = np.mean(scaled_data, axis=0)\n",
        "            scaled_stds = np.std(scaled_data, axis=0)\n",
        "\n",
        "            self.run.log({\n",
        "                \"post_scaling_mean_of_means\": float(np.mean(scaled_means)),\n",
        "                \"post_scaling_mean_of_stds\": float(np.mean(scaled_stds)),\n",
        "                \"post_scaling_inf_count\": int(np.isinf(scaled_data).sum()),\n",
        "                \"post_scaling_nan_count\": int(np.isnan(scaled_data).sum())\n",
        "            })\n",
        "\n",
        "        except Exception as e:\n",
        "            self.run.log({\n",
        "                \"scaling_transform_error\": str(e),\n",
        "                \"scaling_fallback\": \"identity_transform\"\n",
        "            })\n",
        "            # Return original features if scaling fails\n",
        "            return features\n",
        "\n",
        "        return scaled_features\n",
        "\n",
        "    def apply_scaler(self, features: pd.DataFrame) -> pd.DataFrame:\n",
        "        \"\"\"Apply the fitted scaler to the given features DataFrame.\"\"\"\n",
        "        if self.scaler is None:\n",
        "            raise ValueError(\"Scaler must be fitted before applying.\")\n",
        "\n",
        "        # Use dynamic feature column identification\n",
        "        feature_cols = get_feature_columns(features, exclude_metadata=True)\n",
        "\n",
        "        # Transform only the feature columns and keep structure intact\n",
        "        scaled_array = self.scaler.transform(features[feature_cols])\n",
        "        scaled_df = features.copy()\n",
        "        scaled_df[feature_cols] = scaled_array\n",
        "        return scaled_df\n",
        "\n",
        "\n",
        "    def fit_transform(self, baseline_features: pd.DataFrame,\n",
        "                     method: str = 'robust') -> Tuple[pd.DataFrame, RobustScaler]:\n",
        "        \"\"\"Fit scaler and transform baseline features in one step.\"\"\"\n",
        "        self.fit_scaler(baseline_features, method)\n",
        "        scaled_features = self.transform_features(baseline_features)\n",
        "        return scaled_features, self.scaler\n",
        "\n",
        "    def save_scaler(self, filepath: str = \"models/feature_scaler.joblib\"):\n",
        "        \"\"\"Save fitted scaler to disk.\"\"\"\n",
        "        if self.scaler is None:\n",
        "            raise ValueError(\"No scaler to save - fit a scaler first\")\n",
        "\n",
        "        joblib.dump(self.scaler, filepath)\n",
        "\n",
        "        # Log scaler artifact\n",
        "        artifact = wandb.Artifact(\"feature-scaler\", type=\"model\")\n",
        "        artifact.add_file(filepath)\n",
        "        artifact.metadata = self.scaling_stats\n",
        "        self.run.log_artifact(artifact)\n",
        "\n",
        "        self.run.log({\"scaler_saved\": filepath})\n",
        "\n",
        "    def load_scaler(self, filepath: str = \"models/feature_scaler.joblib\"):\n",
        "        \"\"\"Load scaler from disk.\"\"\"\n",
        "        self.scaler = joblib.load(filepath)\n",
        "        self.run.log({\"scaler_loaded\": filepath})\n",
        "\n",
        "    def create_scaling_visualization(self, original_features: pd.DataFrame,\n",
        "                                  scaled_features: pd.DataFrame):\n",
        "        \"\"\"Create visualization comparing original and scaled features.\"\"\"\n",
        "        # Use dynamic feature column identification\n",
        "        feature_cols = get_feature_columns(original_features, exclude_metadata=True)\n",
        "\n",
        "        if len(feature_cols) == 0:\n",
        "            return\n",
        "\n",
        "        # Select subset of features for visualization\n",
        "        viz_features = feature_cols[:12]  # First 12 features\n",
        "\n",
        "        fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
        "\n",
        "        # Original feature distributions\n",
        "        original_data = original_features[viz_features]\n",
        "        axes[0, 0].boxplot([original_data[col].dropna() for col in viz_features])\n",
        "        axes[0, 0].set_title('Original Feature Distributions')\n",
        "        axes[0, 0].set_xticklabels(viz_features, rotation=45)\n",
        "        axes[0, 0].set_ylabel('Values')\n",
        "\n",
        "        # Scaled feature distributions\n",
        "        scaled_data = scaled_features[viz_features]\n",
        "        axes[0, 1].boxplot([scaled_data[col].dropna() for col in viz_features])\n",
        "        axes[0, 1].set_title('Scaled Feature Distributions')\n",
        "        axes[0, 1].set_xticklabels(viz_features, rotation=45)\n",
        "        axes[0, 1].set_ylabel('Scaled Values')\n",
        "\n",
        "        # Feature means comparison\n",
        "        original_means = original_data.mean()\n",
        "        scaled_means = scaled_data.mean()\n",
        "        x_pos = np.arange(len(viz_features))\n",
        "        width = 0.35\n",
        "\n",
        "        axes[1, 0].bar(x_pos - width/2, original_means, width, label='Original', alpha=0.7)\n",
        "        axes[1, 0].bar(x_pos + width/2, scaled_means, width, label='Scaled', alpha=0.7)\n",
        "        axes[1, 0].set_title('Feature Means Comparison')\n",
        "        axes[1, 0].set_xticks(x_pos)\n",
        "        axes[1, 0].set_xticklabels(viz_features, rotation=45)\n",
        "        axes[1, 0].legend()\n",
        "\n",
        "        # Feature standard deviations comparison\n",
        "        original_stds = original_data.std()\n",
        "        scaled_stds = scaled_data.std()\n",
        "\n",
        "        axes[1, 1].bar(x_pos - width/2, original_stds, width, label='Original', alpha=0.7)\n",
        "        axes[1, 1].bar(x_pos + width/2, scaled_stds, width, label='Scaled', alpha=0.7)\n",
        "        axes[1, 1].set_title('Feature Standard Deviations Comparison')\n",
        "        axes[1, 1].set_xticks(x_pos)\n",
        "        axes[1, 1].set_xticklabels(viz_features, rotation=45)\n",
        "        axes[1, 1].legend()\n",
        "\n",
        "        plt.tight_layout()\n",
        "        self.run.log({\"scaling_visualization\": wandb.Image(fig)})\n",
        "        plt.close()\n",
        "\n",
        "    def get_feature_importance_by_scale(self, features: pd.DataFrame) -> pd.DataFrame:\n",
        "        \"\"\"Analyze which features are most affected by scaling.\"\"\"\n",
        "        if self.scaler is None:\n",
        "            raise ValueError(\"Scaler must be fitted first\")\n",
        "\n",
        "        # Use dynamic feature column identification\n",
        "        feature_cols = get_feature_columns(features, exclude_metadata=True)\n",
        "\n",
        "        original_data = features[feature_cols]\n",
        "        scaled_data = self.scaler.transform(original_data)\n",
        "\n",
        "        # Calculate scaling impact metrics\n",
        "        results = []\n",
        "        for i, col in enumerate(feature_cols):\n",
        "            orig_std = original_data[col].std()\n",
        "            scaled_std = scaled_data[:, i].std()\n",
        "\n",
        "            orig_range = original_data[col].max() - original_data[col].min()\n",
        "            scaled_range = scaled_data[:, i].max() - scaled_data[:, i].min()\n",
        "\n",
        "            results.append({\n",
        "                'feature': col,\n",
        "                'original_std': orig_std,\n",
        "                'scaled_std': scaled_std,\n",
        "                'std_change_ratio': scaled_std / orig_std if orig_std != 0 else 0,\n",
        "                'original_range': orig_range,\n",
        "                'scaled_range': scaled_range,\n",
        "                'range_change_ratio': scaled_range / orig_range if orig_range != 0 else 0\n",
        "            })\n",
        "\n",
        "        importance_df = pd.DataFrame(results)\n",
        "\n",
        "        # Log as table\n",
        "        table = wandb.Table(dataframe=importance_df.head(20))\n",
        "        self.run.log({\"feature_scaling_impact\": table})\n",
        "\n",
        "        return importance_df\n",
        "\n",
        "    def validate_scaling(self, baseline_scaled: pd.DataFrame, features_scaled: pd.DataFrame):\n",
        "        \"\"\"\n",
        "        Validate that scaling applied to baseline and new features is consistent.\n",
        "        Compares means and standard deviations for the same features.\n",
        "        Logs results to W&B and warns on large discrepancies.\n",
        "        \"\"\"\n",
        "        # Identify feature columns\n",
        "        feature_cols = get_feature_columns(baseline_scaled, exclude_metadata=True)\n",
        "        if len(feature_cols) == 0:\n",
        "            raise ValueError(\"No feature columns found for scaling validation.\")\n",
        "\n",
        "        # Calculate means and stds\n",
        "        baseline_means = baseline_scaled[feature_cols].mean()\n",
        "        features_means = features_scaled[feature_cols].mean()\n",
        "        baseline_stds = baseline_scaled[feature_cols].std()\n",
        "        features_stds = features_scaled[feature_cols].std()\n",
        "\n",
        "        # Per-feature absolute differences\n",
        "        mean_diffs = (baseline_means - features_means).abs()\n",
        "        std_diffs = (baseline_stds - features_stds).abs()\n",
        "\n",
        "        # Aggregate metrics\n",
        "        mean_diff_avg = float(mean_diffs.mean())\n",
        "        std_diff_avg = float(std_diffs.mean())\n",
        "\n",
        "        # Log summary metrics\n",
        "        self.run.log({\n",
        "            \"scaling_validation_mean_diff_avg\": mean_diff_avg,\n",
        "            \"scaling_validation_std_diff_avg\": std_diff_avg\n",
        "        })\n",
        "\n",
        "        # Log detailed table to W&B (only top deviations)\n",
        "        diff_df = pd.DataFrame({\n",
        "            \"feature\": feature_cols,\n",
        "            \"mean_diff\": mean_diffs.values,\n",
        "            \"std_diff\": std_diffs.values\n",
        "        }).sort_values(\"mean_diff\", ascending=False)\n",
        "\n",
        "        self.run.log({\"scaling_validation_details\": wandb.Table(dataframe=diff_df.head(20))})\n",
        "\n",
        "        # Warn if too large deviation\n",
        "        if mean_diff_avg > 0.5 or std_diff_avg > 0.5:\n",
        "            print(f\"⚠️ Warning: Large scaling mismatch detected \"\n",
        "                  f\"(mean_diff_avg={mean_diff_avg:.4f}, std_diff_avg={std_diff_avg:.4f})\")\n",
        "\n",
        "\n",
        "\n",
        "def main():\n",
        "    import argparse\n",
        "\n",
        "    parser = argparse.ArgumentParser(description=\"Scale features for machine learning\")\n",
        "    parser.add_argument(\"--input-file\", default=\"data/features.parquet\", help=\"Input features file\")\n",
        "    parser.add_argument(\"--output-file\", default=\"data/scaled_features.parquet\", help=\"Output scaled features file\")\n",
        "    parser.add_argument(\"--method\", default=\"robust\", choices=[\"robust\", \"standard\", \"minmax\"], help=\"Scaling method\")\n",
        "    parser.add_argument(\"--save-scaler\", default=\"models/feature_scaler.joblib\", help=\"Path to save scaler\")\n",
        "\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    # Initialize W&B\n",
        "    run = wandb.init(project=\"predictive-maintenance\", job_type=\"feature-scaling\")\n",
        "\n",
        "    # Load features\n",
        "    print(f\"Loading features from {args.input_file}...\")\n",
        "    features = pd.read_parquet(args.input_file)\n",
        "    print(f\"Loaded features shape: {features.shape}\")\n",
        "\n",
        "    # Log feature information using dynamic identification\n",
        "    from src.utils.sensor_utils import get_feature_columns, log_sensor_column_info\n",
        "    feature_cols = get_feature_columns(features, exclude_metadata=True)\n",
        "    feature_info = log_sensor_column_info(features, feature_cols, run)\n",
        "\n",
        "    print(f\"Identified {len(feature_cols)} feature columns for scaling\")\n",
        "    print(f\"Feature columns: {feature_cols[:10]}{'...' if len(feature_cols) > 10 else ''}\")\n",
        "\n",
        "    # Initialize scaler\n",
        "    scaler = FeatureScaler(run)\n",
        "\n",
        "    # Fit and transform features\n",
        "    print(f\"Scaling features using {args.method} method...\")\n",
        "    scaled_features, fitted_scaler = scaler.fit_transform(features, method=args.method)\n",
        "\n",
        "    print(f\"Scaled features shape: {scaled_features.shape}\")\n",
        "\n",
        "    # Create visualization\n",
        "    print(\"Creating scaling visualization...\")\n",
        "    scaler.create_scaling_visualization(features, scaled_features)\n",
        "\n",
        "    # Analyze scaling impact\n",
        "    print(\"Analyzing scaling impact...\")\n",
        "    scaling_impact = scaler.get_feature_importance_by_scale(features)\n",
        "    print(f\"Features most affected by scaling (top 5):\")\n",
        "    top_affected = scaling_impact.nlargest(5, 'std_change_ratio')[['feature', 'std_change_ratio']]\n",
        "    for _, row in top_affected.iterrows():\n",
        "        print(f\"  {row['feature']}: {row['std_change_ratio']:.3f}x std change\")\n",
        "\n",
        "    # Save scaled features\n",
        "    print(f\"Saving scaled features to {args.output_file}...\")\n",
        "    scaled_features.to_parquet(args.output_file)\n",
        "\n",
        "    # Save scaler\n",
        "    print(f\"Saving scaler to {args.save_scaler}...\")\n",
        "    scaler.save_scaler(args.save_scaler)\n",
        "\n",
        "    # Log final artifact\n",
        "    artifact = wandb.Artifact(\"scaled-features\", type=\"features\")\n",
        "    artifact.add_file(args.output_file)\n",
        "    artifact.metadata = {\n",
        "        \"scaling_method\": args.method,\n",
        "        \"original_feature_count\": len(feature_cols),\n",
        "        \"total_samples\": len(features),\n",
        "        \"feature_identification_method\": \"dynamic_feature_columns\",\n",
        "        \"scaler_stats\": scaler.scaling_stats\n",
        "    }\n",
        "    run.log_artifact(artifact)\n",
        "\n",
        "    run.finish()\n",
        "    print(\"Feature scaling completed!\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "oXOfqxcdFTBa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile src/features/select_reduce.py\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import wandb\n",
        "from sklearn.feature_selection import VarianceThreshold, SelectKBest, mutual_info_regression\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.random_projection import GaussianRandomProjection\n",
        "import joblib\n",
        "import matplotlib.pyplot as plt\n",
        "from typing import Tuple, List\n",
        "import os\n",
        "\n",
        "# Import the new sensor column utility\n",
        "from src.utils.sensor_utils import get_feature_columns, log_sensor_column_info\n",
        "\n",
        "\n",
        "class FeatureSelector:\n",
        "    \"\"\"\n",
        "    Feature selection and dimensionality reduction with W&B tracking.\n",
        "    Now uses DYNAMIC feature column identification instead of hardcoded patterns.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, wandb_run: wandb.run = None):\n",
        "        self.run = wandb_run or wandb.init(project=\"predictive-maintenance\")\n",
        "        self.selection_stats = {}\n",
        "        self.models = {}\n",
        "\n",
        "    def variance_filter(self, features: pd.DataFrame, threshold: float = 0.01) -> Tuple[pd.DataFrame, List[str]]:\n",
        "        \"\"\"Filter features based on variance threshold using dynamic column identification.\"\"\"\n",
        "        # Use dynamic feature column identification\n",
        "        feature_cols = get_feature_columns(features, exclude_metadata=True)\n",
        "\n",
        "        # Log feature identification info\n",
        "        log_sensor_column_info(features, feature_cols, self.run)\n",
        "\n",
        "        self.run.log({\n",
        "            \"variance_filter_threshold\": threshold,\n",
        "            \"variance_filter_input_features\": len(feature_cols),\n",
        "            \"feature_identification_method\": \"dynamic_feature_columns\"\n",
        "        })\n",
        "\n",
        "        # Apply variance threshold\n",
        "        selector = VarianceThreshold(threshold=threshold)\n",
        "        selected_data = selector.fit_transform(features[feature_cols])\n",
        "\n",
        "        # Get selected feature names\n",
        "        selected_features = [feature_cols[i] for i in range(len(feature_cols)) if selector.variances_[i] > threshold]\n",
        "        removed_features = [feature_cols[i] for i in range(len(feature_cols)) if selector.variances_[i] <= threshold]\n",
        "\n",
        "        # Create filtered dataframe\n",
        "        filtered_df = pd.DataFrame(selected_data, columns=selected_features, index=features.index)\n",
        "\n",
        "        # Add back non-feature columns (metadata)\n",
        "        non_feature_cols = [col for col in features.columns if col not in feature_cols]\n",
        "        for col in non_feature_cols:\n",
        "            filtered_df[col] = features[col]\n",
        "\n",
        "        # Log statistics\n",
        "        self.run.log({\n",
        "            \"variance_filter_selected_features\": len(selected_features),\n",
        "            \"variance_filter_removed_features\": len(removed_features),\n",
        "            \"variance_filter_retention_rate\": len(selected_features) / len(feature_cols) if feature_cols else 0,\n",
        "            \"variance_filter_mean_variance\": float(np.mean(selector.variances_)),\n",
        "            \"variance_filter_min_variance\": float(np.min(selector.variances_)),\n",
        "            \"variance_filter_max_variance\": float(np.max(selector.variances_))\n",
        "        })\n",
        "\n",
        "        # Store selection statistics\n",
        "        self.selection_stats['variance_filter'] = {\n",
        "            'threshold': threshold,\n",
        "            'input_features': len(feature_cols),\n",
        "            'selected_features': len(selected_features),\n",
        "            'removed_features': len(removed_features),\n",
        "            'retention_rate': len(selected_features) / len(feature_cols) if feature_cols else 0\n",
        "        }\n",
        "\n",
        "        # Store the selector model for later use\n",
        "        self.models['variance_selector'] = selector\n",
        "\n",
        "        return filtered_df, removed_features\n",
        "\n",
        "    def correlation_filter(self, features: pd.DataFrame, threshold: float = 0.95) -> Tuple[pd.DataFrame, List[str]]:\n",
        "        \"\"\"Remove highly correlated features using dynamic column identification.\"\"\"\n",
        "        # Use dynamic feature column identification\n",
        "        feature_cols = get_feature_columns(features, exclude_metadata=True)\n",
        "\n",
        "        if len(feature_cols) < 2:\n",
        "            return features, []\n",
        "\n",
        "        # Calculate correlation matrix\n",
        "        feature_data = features[feature_cols]\n",
        "        corr_matrix = feature_data.corr().abs()\n",
        "\n",
        "        # Find highly correlated pairs\n",
        "        upper_triangle = corr_matrix.where(\n",
        "            np.triu(np.ones(corr_matrix.shape), k=1).astype(bool)\n",
        "        )\n",
        "\n",
        "        # Identify features to remove\n",
        "        to_remove = [\n",
        "            column for column in upper_triangle.columns\n",
        "            if any(upper_triangle[column] > threshold)\n",
        "        ]\n",
        "\n",
        "        # Remove features\n",
        "        filtered_features = features.drop(columns=to_remove)\n",
        "\n",
        "        # Log statistics\n",
        "        self.run.log({\n",
        "            \"correlation_filter_threshold\": threshold,\n",
        "            \"correlation_filter_input_features\": len(feature_cols),\n",
        "            \"correlation_filter_removed_features\": len(to_remove),\n",
        "            \"correlation_filter_retained_features\": len(feature_cols) - len(to_remove),\n",
        "            \"correlation_filter_retention_rate\": (len(feature_cols) - len(to_remove)) / len(feature_cols)\n",
        "        })\n",
        "\n",
        "        # Store selection statistics\n",
        "        self.selection_stats['correlation_filter'] = {\n",
        "            'threshold': threshold,\n",
        "            'input_features': len(feature_cols),\n",
        "            'removed_features': len(to_remove),\n",
        "            'retention_rate': (len(feature_cols) - len(to_remove)) / len(feature_cols)\n",
        "        }\n",
        "\n",
        "        return filtered_features, to_remove\n",
        "\n",
        "    def univariate_selection(self, features: pd.DataFrame, target: pd.Series,\n",
        "                           k: int = 100, score_func=mutual_info_regression) -> Tuple[pd.DataFrame, List[str]]:\n",
        "        \"\"\"Select k best features using univariate statistical tests.\"\"\"\n",
        "        # Use dynamic feature column identification\n",
        "        feature_cols = get_feature_columns(features, exclude_metadata=True)\n",
        "\n",
        "        if len(feature_cols) == 0:\n",
        "            return features, []\n",
        "\n",
        "        # Ensure k doesn't exceed available features\n",
        "        k = min(k, len(feature_cols))\n",
        "\n",
        "        # Apply univariate selection\n",
        "        selector = SelectKBest(score_func=score_func, k=k)\n",
        "\n",
        "        # Handle potential issues with mutual information\n",
        "        try:\n",
        "            feature_data = features[feature_cols].fillna(0)  # Fill NaN values\n",
        "            selected_data = selector.fit_transform(feature_data, target)\n",
        "\n",
        "            # Get selected feature names\n",
        "            selected_mask = selector.get_support()\n",
        "            selected_features = [feature_cols[i] for i in range(len(feature_cols)) if selected_mask[i]]\n",
        "            removed_features = [feature_cols[i] for i in range(len(feature_cols)) if not selected_mask[i]]\n",
        "\n",
        "            # Create filtered dataframe\n",
        "            filtered_df = pd.DataFrame(selected_data, columns=selected_features, index=features.index)\n",
        "\n",
        "            # Add back non-feature columns\n",
        "            non_feature_cols = [col for col in features.columns if col not in feature_cols]\n",
        "            for col in non_feature_cols:\n",
        "                filtered_df[col] = features[col]\n",
        "\n",
        "            # Log statistics\n",
        "            scores = selector.scores_\n",
        "            self.run.log({\n",
        "                \"univariate_selection_k\": k,\n",
        "                \"univariate_selection_input_features\": len(feature_cols),\n",
        "                \"univariate_selection_selected_features\": len(selected_features),\n",
        "                \"univariate_selection_mean_score\": float(np.mean(scores)),\n",
        "                \"univariate_selection_max_score\": float(np.max(scores)),\n",
        "                \"univariate_selection_min_score\": float(np.min(scores))\n",
        "            })\n",
        "\n",
        "            # Store selection statistics\n",
        "            self.selection_stats['univariate_selection'] = {\n",
        "                'k': k,\n",
        "                'input_features': len(feature_cols),\n",
        "                'selected_features': len(selected_features),\n",
        "                'mean_score': float(np.mean(scores)),\n",
        "                'score_function': str(score_func.__name__)\n",
        "            }\n",
        "\n",
        "            # Store selector for future use\n",
        "            self.models['univariate_selector'] = selector\n",
        "\n",
        "        except Exception as e:\n",
        "            self.run.log({\"univariate_selection_error\": str(e)})\n",
        "            return features, []\n",
        "\n",
        "        return filtered_df, removed_features\n",
        "\n",
        "    def pca_reduction(self, features: pd.DataFrame, n_components: float = 0.95) -> Tuple[pd.DataFrame, int]:\n",
        "        \"\"\"Apply PCA dimensionality reduction.\"\"\"\n",
        "        # Use dynamic feature column identification\n",
        "        feature_cols = get_feature_columns(features, exclude_metadata=True)\n",
        "\n",
        "        if len(feature_cols) == 0:\n",
        "            return features, 0\n",
        "\n",
        "        # Apply PCA\n",
        "        if isinstance(n_components, float):\n",
        "            # Variance retention mode\n",
        "            pca = PCA(n_components=n_components, random_state=42)\n",
        "        else:\n",
        "            # Fixed number of components\n",
        "            n_components = min(n_components, len(feature_cols))\n",
        "            pca = PCA(n_components=n_components, random_state=42)\n",
        "\n",
        "        try:\n",
        "            feature_data = features[feature_cols].fillna(0)\n",
        "            pca_data = pca.fit_transform(feature_data)\n",
        "\n",
        "            # Create PCA feature names\n",
        "            pca_columns = [f'PC_{i+1}' for i in range(pca_data.shape[1])]\n",
        "\n",
        "            # Create PCA dataframe\n",
        "            pca_df = pd.DataFrame(pca_data, columns=pca_columns, index=features.index)\n",
        "\n",
        "            # Add back non-feature columns\n",
        "            non_feature_cols = [col for col in features.columns if col not in feature_cols]\n",
        "            for col in non_feature_cols:\n",
        "                pca_df[col] = features[col]\n",
        "\n",
        "            # Log statistics\n",
        "            explained_variance_ratio = pca.explained_variance_ratio_\n",
        "            cumulative_variance = np.cumsum(explained_variance_ratio)\n",
        "\n",
        "            self.run.log({\n",
        "                \"pca_input_features\": len(feature_cols),\n",
        "                \"pca_output_components\": pca_data.shape[1],\n",
        "                \"pca_explained_variance_total\": float(cumulative_variance[-1]),\n",
        "                \"pca_explained_variance_first_component\": float(explained_variance_ratio[0]),\n",
        "                \"pca_reduction_ratio\": pca_data.shape[1] / len(feature_cols)\n",
        "            })\n",
        "\n",
        "            # Store PCA model and statistics\n",
        "            self.models['pca'] = pca\n",
        "            self.selection_stats['pca'] = {\n",
        "                'input_features': len(feature_cols),\n",
        "                'output_components': pca_data.shape[1],\n",
        "                'explained_variance_total': float(cumulative_variance[-1]),\n",
        "                'reduction_ratio': pca_data.shape[1] / len(feature_cols)\n",
        "            }\n",
        "\n",
        "            # Log explained variance plot\n",
        "            self.create_pca_visualization(explained_variance_ratio, cumulative_variance)\n",
        "\n",
        "        except Exception as e:\n",
        "            self.run.log({\"pca_error\": str(e)})\n",
        "            return features, 0\n",
        "\n",
        "        return pca_df, len(feature_cols) - pca_data.shape[1]\n",
        "\n",
        "    def random_projection(self, features: pd.DataFrame, n_components: int = 100) -> Tuple[pd.DataFrame, int]:\n",
        "        \"\"\"Apply random projection for dimensionality reduction.\"\"\"\n",
        "        # Use dynamic feature column identification\n",
        "        feature_cols = get_feature_columns(features, exclude_metadata=True)\n",
        "\n",
        "        if len(feature_cols) == 0:\n",
        "            return features, 0\n",
        "\n",
        "        # Ensure n_components doesn't exceed available features\n",
        "        n_components = min(n_components, len(feature_cols))\n",
        "\n",
        "        try:\n",
        "            # Apply random projection\n",
        "            rp = GaussianRandomProjection(n_components=n_components, random_state=42)\n",
        "            feature_data = features[feature_cols].fillna(0)\n",
        "            rp_data = rp.fit_transform(feature_data)\n",
        "\n",
        "            # Create random projection feature names\n",
        "            rp_columns = [f'RP_{i+1}' for i in range(rp_data.shape[1])]\n",
        "\n",
        "            # Create random projection dataframe\n",
        "            rp_df = pd.DataFrame(rp_data, columns=rp_columns, index=features.index)\n",
        "\n",
        "            # Add back non-feature columns\n",
        "            non_feature_cols = [col for col in features.columns if col not in feature_cols]\n",
        "            for col in non_feature_cols:\n",
        "                rp_df[col] = features[col]\n",
        "\n",
        "            # Log statistics\n",
        "            self.run.log({\n",
        "                \"random_projection_input_features\": len(feature_cols),\n",
        "                \"random_projection_output_components\": rp_data.shape[1],\n",
        "                \"random_projection_reduction_ratio\": rp_data.shape[1] / len(feature_cols)\n",
        "            })\n",
        "\n",
        "            # Store model and statistics\n",
        "            self.models['random_projection'] = rp\n",
        "            self.selection_stats['random_projection'] = {\n",
        "                'input_features': len(feature_cols),\n",
        "                'output_components': rp_data.shape[1],\n",
        "                'reduction_ratio': rp_data.shape[1] / len(feature_cols)\n",
        "            }\n",
        "\n",
        "        except Exception as e:\n",
        "            self.run.log({\"random_projection_error\": str(e)})\n",
        "            return features, 0\n",
        "\n",
        "        return rp_df, len(feature_cols) - rp_data.shape[1]\n",
        "\n",
        "    def create_pca_visualization(self, explained_variance_ratio: np.ndarray,\n",
        "                               cumulative_variance: np.ndarray):\n",
        "        \"\"\"Create PCA visualization.\"\"\"\n",
        "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
        "\n",
        "        # Individual explained variance\n",
        "        components = range(1, len(explained_variance_ratio) + 1)\n",
        "        ax1.bar(components[:20], explained_variance_ratio[:20])  # Show first 20 components\n",
        "        ax1.set_xlabel('Principal Component')\n",
        "        ax1.set_ylabel('Explained Variance Ratio')\n",
        "        ax1.set_title('Explained Variance by Component')\n",
        "\n",
        "        # Cumulative explained variance\n",
        "        ax2.plot(components, cumulative_variance, 'bo-')\n",
        "        ax2.axhline(y=0.95, color='r', linestyle='--', label='95% Variance')\n",
        "        ax2.axhline(y=0.90, color='orange', linestyle='--', label='90% Variance')\n",
        "        ax2.set_xlabel('Number of Components')\n",
        "        ax2.set_ylabel('Cumulative Explained Variance')\n",
        "        ax2.set_title('Cumulative Explained Variance')\n",
        "        ax2.legend()\n",
        "        ax2.grid(True)\n",
        "\n",
        "        plt.tight_layout()\n",
        "        self.run.log({\"pca_visualization\": wandb.Image(fig)})\n",
        "        plt.close()\n",
        "\n",
        "    def save_models(self, filepath_prefix: str = \"models/feature_selection\"):\n",
        "        \"\"\"Save all fitted models.\"\"\"\n",
        "        # Create models directory if it doesn't exist\n",
        "        os.makedirs(os.path.dirname(filepath_prefix) if os.path.dirname(filepath_prefix) else \"models\", exist_ok=True)\n",
        "\n",
        "        for model_name, model in self.models.items():\n",
        "            # Special handling for different model types\n",
        "            if model_name == 'variance_selector':\n",
        "                filepath = \"models/variance_selector.pkl\"\n",
        "            elif model_name == 'pca':\n",
        "                filepath = \"models/pca.pkl\"\n",
        "            else:\n",
        "                filepath = f\"{filepath_prefix}_{model_name}.joblib\"\n",
        "\n",
        "            joblib.dump(model, filepath)\n",
        "\n",
        "            # Log model artifact\n",
        "            artifact = wandb.Artifact(f\"feature-selection-{model_name}\", type=\"model\")\n",
        "            artifact.add_file(filepath)\n",
        "            artifact.metadata = self.selection_stats.get(model_name, {})\n",
        "            self.run.log_artifact(artifact)\n",
        "\n",
        "        self.run.log({\"models_saved\": len(self.models)})\n",
        "\n",
        "    def combine_selection_methods(\n",
        "        self,\n",
        "        features,\n",
        "        variance_threshold=0.0,\n",
        "        mi_top_k=None,\n",
        "        pca_variance=None\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Combines multiple feature selection methods:\n",
        "        - Variance Threshold\n",
        "        - Mutual Information\n",
        "        - PCA dimensionality reduction\n",
        "\n",
        "        FIXED VERSION: Correctly calls existing methods and handles return values\n",
        "        \"\"\"\n",
        "        selected_features = features\n",
        "\n",
        "        # Variance threshold\n",
        "        if variance_threshold > 0.0:\n",
        "            self.run.log({\"feature_selection_step\": \"variance_threshold\"})\n",
        "            selected_features, removed = self.variance_filter(\n",
        "                selected_features,\n",
        "                threshold=variance_threshold\n",
        "            )\n",
        "            self.run.log({\n",
        "                \"variance_removed_features\": len(removed),\n",
        "                \"variance_remaining_features\": len(selected_features.columns)\n",
        "            })\n",
        "\n",
        "        # Mutual Information\n",
        "        if mi_top_k is not None and mi_top_k > 0:\n",
        "            self.run.log({\"feature_selection_step\": \"mutual_information\"})\n",
        "\n",
        "            # Generate target for mutual information\n",
        "            # TODO: Replace with actual anomaly scores or degradation indicators\n",
        "            # Options:\n",
        "            # 1. Use RUL (Remaining Useful Life) if available\n",
        "            # 2. Use health index computed from domain knowledge\n",
        "            # 3. Use anomaly scores from unsupervised methods\n",
        "            # 4. Use time-based degradation assumption (current placeholder)\n",
        "\n",
        "            # For now, using time-based progression as proxy for degradation\n",
        "            # This assumes degradation increases with time (reasonable for many systems)\n",
        "            target = np.arange(len(selected_features))\n",
        "\n",
        "            # Normalize target to [0, 1] range for better MI computation\n",
        "            target = (target - target.min()) / (target.max() - target.min() + 1e-10)\n",
        "\n",
        "            self.run.log({\n",
        "                \"mi_target_type\": \"time_progression_placeholder\",\n",
        "                \"mi_target_warning\": \"Using synthetic target - replace with actual anomaly/degradation scores\"\n",
        "            })\n",
        "\n",
        "            selected_features, removed = self.univariate_selection(\n",
        "                selected_features,\n",
        "                target=target,\n",
        "                k=mi_top_k\n",
        "            )\n",
        "            self.run.log({\n",
        "                \"mi_removed_features\": len(removed),\n",
        "                \"mi_selected_features\": mi_top_k,\n",
        "                \"mi_remaining_features\": len(selected_features.columns)\n",
        "            })\n",
        "\n",
        "        # PCA\n",
        "        if pca_variance is not None and 0 < pca_variance <= 1.0:\n",
        "            self.run.log({\"feature_selection_step\": \"pca_reduction\"})\n",
        "            selected_features, n_reduced = self.pca_reduction(\n",
        "                selected_features,\n",
        "                n_components=pca_variance\n",
        "            )\n",
        "            self.run.log({\n",
        "                \"pca_components_reduced\": n_reduced,\n",
        "                \"pca_variance_retained\": pca_variance,\n",
        "                \"pca_final_components\": len([col for col in selected_features.columns if col.startswith('PC_')])\n",
        "            })\n",
        "\n",
        "        # Log final statistics\n",
        "        feature_cols = get_feature_columns(selected_features, exclude_metadata=True)\n",
        "        self.run.log({\n",
        "            \"final_feature_count\": len(feature_cols),\n",
        "            \"total_features\": len(selected_features.columns),\n",
        "            \"metadata_columns\": len(selected_features.columns) - len(feature_cols)\n",
        "        })\n",
        "\n",
        "        # Final combined set\n",
        "        combined_features = selected_features\n",
        "        return combined_features\n",
        "\n",
        "\n",
        "def main():\n",
        "    import argparse\n",
        "\n",
        "    parser = argparse.ArgumentParser(description=\"Feature selection and dimensionality reduction\")\n",
        "    parser.add_argument(\"--input-file\", default=\"data/scaled_features.parquet\", help=\"Input features file\")\n",
        "    parser.add_argument(\"--output-file\", default=\"data/selected_features.parquet\", help=\"Output selected features file\")\n",
        "    parser.add_argument(\"--method\", default=\"variance\",\n",
        "                       choices=[\"variance\", \"correlation\", \"univariate\", \"pca\", \"random_projection\", \"all\"],\n",
        "                       help=\"Feature selection method\")\n",
        "    parser.add_argument(\"--variance-threshold\", type=float, default=0.01, help=\"Variance threshold\")\n",
        "    parser.add_argument(\"--correlation-threshold\", type=float, default=0.95, help=\"Correlation threshold\")\n",
        "    parser.add_argument(\"--k-best\", type=int, default=100, help=\"Number of best features for univariate selection\")\n",
        "    parser.add_argument(\"--pca-components\", type=float, default=0.95, help=\"PCA components (float for variance ratio)\")\n",
        "    parser.add_argument(\"--rp-components\", type=int, default=100, help=\"Random projection components\")\n",
        "\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    # Initialize W&B\n",
        "    run = wandb.init(project=\"predictive-maintenance\", job_type=\"feature-selection\")\n",
        "\n",
        "    # Load features\n",
        "    print(f\"Loading features from {args.input_file}...\")\n",
        "    features = pd.read_parquet(args.input_file)\n",
        "    print(f\"Loaded features shape: {features.shape}\")\n",
        "\n",
        "    # Log feature information using dynamic identification\n",
        "    from src.utils.sensor_utils import get_feature_columns, log_sensor_column_info\n",
        "    feature_cols = get_feature_columns(features, exclude_metadata=True)\n",
        "    feature_info = log_sensor_column_info(features, feature_cols, run)\n",
        "\n",
        "    print(f\"Identified {len(feature_cols)} feature columns for selection\")\n",
        "    print(f\"Feature columns: {feature_cols[:10]}{'...' if len(feature_cols) > 10 else ''}\")\n",
        "\n",
        "    # Initialize selector\n",
        "    selector = FeatureSelector(run)\n",
        "\n",
        "    # Apply feature selection based on method\n",
        "    selected_features = features.copy()\n",
        "    total_removed = 0\n",
        "\n",
        "    if args.method in [\"variance\", \"all\"]:\n",
        "        print(f\"Applying variance filter (threshold={args.variance_threshold})...\")\n",
        "        selected_features, removed = selector.variance_filter(selected_features, args.variance_threshold)\n",
        "        total_removed += len(removed)\n",
        "        print(f\"Removed {len(removed)} low-variance features\")\n",
        "\n",
        "    if args.method in [\"correlation\", \"all\"]:\n",
        "        print(f\"Applying correlation filter (threshold={args.correlation_threshold})...\")\n",
        "        selected_features, removed = selector.correlation_filter(selected_features, args.correlation_threshold)\n",
        "        total_removed += len(removed)\n",
        "        print(f\"Removed {len(removed)} highly correlated features\")\n",
        "\n",
        "    if args.method in [\"univariate\", \"all\"]:\n",
        "        print(f\"Applying univariate selection (k={args.k_best})...\")\n",
        "        # For demonstration, create a dummy target (in practice, this should be provided)\n",
        "        dummy_target = np.random.rand(len(features))\n",
        "        selected_features, removed = selector.univariate_selection(selected_features, dummy_target, args.k_best)\n",
        "        total_removed += len(removed)\n",
        "        print(f\"Selected {args.k_best} best features using univariate selection\")\n",
        "\n",
        "    if args.method in [\"pca\", \"all\"]:\n",
        "        print(f\"Applying PCA (components={args.pca_components})...\")\n",
        "        selected_features, reduced = selector.pca_reduction(selected_features, args.pca_components)\n",
        "        print(f\"Reduced {reduced} features using PCA\")\n",
        "\n",
        "    if args.method == \"random_projection\":\n",
        "        print(f\"Applying random projection (components={args.rp_components})...\")\n",
        "        selected_features, reduced = selector.random_projection(selected_features, args.rp_components)\n",
        "        print(f\"Reduced {reduced} features using random projection\")\n",
        "\n",
        "    print(f\"Final feature shape: {selected_features.shape}\")\n",
        "    print(f\"Total features removed/reduced: {total_removed}\")\n",
        "\n",
        "    # Save selected features\n",
        "    print(f\"Saving selected features to {args.output_file}...\")\n",
        "    selected_features.to_parquet(args.output_file)\n",
        "\n",
        "    # Save models\n",
        "    print(\"Saving feature selection models...\")\n",
        "    selector.save_models()\n",
        "\n",
        "    # Log final artifact\n",
        "    artifact = wandb.Artifact(\"selected-features\", type=\"features\")\n",
        "    artifact.add_file(args.output_file)\n",
        "    artifact.metadata = {\n",
        "        \"selection_method\": args.method,\n",
        "        \"original_feature_count\": len(feature_cols),\n",
        "        \"final_feature_count\": len(get_feature_columns(selected_features, exclude_metadata=True)),\n",
        "        \"total_removed\": total_removed,\n",
        "        \"feature_identification_method\": \"dynamic_feature_columns\",\n",
        "        \"selection_stats\": selector.selection_stats\n",
        "    }\n",
        "    run.log_artifact(artifact)\n",
        "\n",
        "    run.finish()\n",
        "    print(\"Feature selection completed!\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "Ferenna1FgQr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile src/model_input/assemble.py\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import wandb\n",
        "from typing import Tuple, List, Dict, Optional\n",
        "import h5py\n",
        "import os\n",
        "import sys # Import sys\n",
        "from src.utils.sensor_utils import get_feature_columns, log_sensor_column_info\n",
        "\n",
        "class SequenceAssembler:\n",
        "    \"\"\"Assemble sequences for model input with W&B tracking and pipeline mode support.\"\"\"\n",
        "\n",
        "    def __init__(self, wandb_run: wandb.run = None, pipeline_mode: str = \"train\"):\n",
        "        self.run = wandb_run or wandb.init(project=\"predictive-maintenance\")\n",
        "        self.pipeline_mode = pipeline_mode\n",
        "        self.assembly_stats = {}\n",
        "\n",
        "    def create_sequences_lstm(self, features: pd.DataFrame,\n",
        "                            sequence_length: int = 10,\n",
        "                            overlap: float = 0.5) -> Tuple[np.ndarray, np.ndarray]:\n",
        "        \"\"\"Create sequences for LSTM models.\"\"\"\n",
        "        # Sort by window_id to ensure temporal order\n",
        "        features = features.sort_values('window_id')\n",
        "\n",
        "        # Get feature columns\n",
        "        feature_cols = get_feature_columns(df, exclude_metadata=True)\n",
        "\n",
        "        data = features[feature_cols].values\n",
        "        n_features = len(feature_cols)\n",
        "        n_samples = len(data)\n",
        "\n",
        "        # Calculate stride based on overlap\n",
        "        stride = max(1, int(sequence_length * (1 - overlap)))\n",
        "\n",
        "        # Calculate number of sequences\n",
        "        num_sequences = (n_samples - sequence_length) // stride + 1\n",
        "\n",
        "        # Create sequences\n",
        "        X_lstm = np.zeros((num_sequences, sequence_length, n_features))\n",
        "        timestamps = np.zeros(num_sequences)\n",
        "\n",
        "        for i in range(num_sequences):\n",
        "            start_idx = i * stride\n",
        "            end_idx = start_idx + sequence_length\n",
        "            X_lstm[i] = data[start_idx:end_idx]\n",
        "            timestamps[i] = features.iloc[start_idx]['window_id']\n",
        "\n",
        "        # Log statistics\n",
        "        self.assembly_stats['lstm'] = {\n",
        "            'sequence_length': sequence_length,\n",
        "            'overlap': overlap,\n",
        "            'stride': stride,\n",
        "            'num_sequences': num_sequences,\n",
        "            'shape': X_lstm.shape\n",
        "        }\n",
        "\n",
        "        self.run.log({\n",
        "            \"lstm_num_sequences\": num_sequences,\n",
        "            \"lstm_sequence_shape\": str(X_lstm.shape),\n",
        "            \"lstm_memory_mb\": X_lstm.nbytes / (1024 * 1024)\n",
        "        })\n",
        "\n",
        "        return X_lstm, timestamps\n",
        "\n",
        "    def create_tabular_format(self, features: pd.DataFrame) -> np.ndarray:\n",
        "        \"\"\"Create tabular format for traditional ML models.\"\"\"\n",
        "        feature_cols = [col for col in features.columns\n",
        "                       if col not in ['window_id', 'window_start']]\n",
        "\n",
        "        X_tabular = features[feature_cols].values\n",
        "\n",
        "        # Log statistics\n",
        "        self.assembly_stats['tabular'] = {\n",
        "            'num_samples': len(X_tabular),\n",
        "            'num_features': X_tabular.shape[1],\n",
        "            'shape': X_tabular.shape\n",
        "        }\n",
        "\n",
        "        self.run.log({\n",
        "            \"tabular_num_samples\": len(X_tabular),\n",
        "            \"tabular_num_features\": X_tabular.shape[1]\n",
        "        })\n",
        "\n",
        "        return X_tabular\n",
        "\n",
        "    def create_sequences_for_mode(self, features: pd.DataFrame,\n",
        "                                sequence_length: int = 10,\n",
        "                                overlap: float = 0.5,\n",
        "                                train_ratio: float = 0.8) -> Dict[str, np.ndarray]:\n",
        "        \"\"\"Create sequences based on pipeline mode.\"\"\"\n",
        "        X_lstm, timestamps = self.create_sequences_lstm(features, sequence_length, overlap)\n",
        "\n",
        "        if self.pipeline_mode == \"train\":\n",
        "            # For training mode: create only train/val split (no test)\n",
        "            n_samples = len(X_lstm)\n",
        "            train_end = int(n_samples * train_ratio)\n",
        "\n",
        "            splits = {\n",
        "                'X_train': X_lstm[:train_end],\n",
        "                'X_val': X_lstm[train_end:],\n",
        "                'timestamps_train': timestamps[:train_end],\n",
        "                'timestamps_val': timestamps[train_end:]\n",
        "            }\n",
        "\n",
        "            # Log split statistics\n",
        "            self.run.log({\n",
        "                \"X_train_samples\": len(splits['X_train']),\n",
        "                \"X_train_shape\": str(splits['X_train'].shape),\n",
        "                \"X_val_samples\": len(splits['X_val']),\n",
        "                \"X_val_shape\": str(splits['X_val'].shape)\n",
        "            })\n",
        "\n",
        "        elif self.pipeline_mode == \"test\":\n",
        "            # For test mode: create full dataset for anomaly detection\n",
        "            splits = {\n",
        "                'X_test': X_lstm,\n",
        "                'timestamps_test': timestamps\n",
        "            }\n",
        "\n",
        "            self.run.log({\n",
        "                \"X_test_samples\": len(splits['X_test']),\n",
        "                \"X_test_shape\": str(splits['X_test'].shape)\n",
        "            })\n",
        "\n",
        "        else:\n",
        "            raise ValueError(f\"Invalid pipeline_mode: {self.pipeline_mode}. Must be 'train' or 'test'\")\n",
        "\n",
        "        return splits\n",
        "\n",
        "    def create_train_val_test_split(self, X: np.ndarray, timestamps: np.ndarray = None,\n",
        "                                  train_ratio: float = 0.7,\n",
        "                                  val_ratio: float = 0.15) -> Dict[str, np.ndarray]:\n",
        "        \"\"\"Create train/validation/test splits (legacy method for compatibility).\"\"\"\n",
        "        n_samples = len(X)\n",
        "\n",
        "        # Calculate split indices\n",
        "        train_end = int(n_samples * train_ratio)\n",
        "        val_end = int(n_samples * (train_ratio + val_ratio))\n",
        "\n",
        "        # Split data\n",
        "        splits = {\n",
        "            'X_train': X[:train_end],\n",
        "            'X_val': X[train_end:val_end],\n",
        "            'X_test': X[val_end:]\n",
        "        }\n",
        "\n",
        "        if timestamps is not None:\n",
        "            splits['timestamps_train'] = timestamps[:train_end]\n",
        "            splits['timestamps_val'] = timestamps[train_end:val_end]\n",
        "            splits['timestamps_test'] = timestamps[val_end:]\n",
        "\n",
        "        # Log split statistics\n",
        "        for split_name, split_data in splits.items():\n",
        "            if split_name.startswith('X_'):\n",
        "                self.run.log({\n",
        "                    f\"{split_name}_samples\": len(split_data),\n",
        "                    f\"{split_name}_shape\": str(split_data.shape)\n",
        "                })\n",
        "\n",
        "        return splits\n",
        "\n",
        "    def save_sequences(self, sequences: Dict[str, np.ndarray],\n",
        "                      output_dir: str = \"data\"):\n",
        "        \"\"\"Save sequences in multiple formats with W&B artifacts.\"\"\"\n",
        "        os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "        # Save as numpy arrays\n",
        "        for name, array in sequences.items():\n",
        "            if isinstance(array, np.ndarray):\n",
        "                np_path = f\"{output_dir}/{name}.npy\"\n",
        "                np.save(np_path, array)\n",
        "\n",
        "                # Log basic arrays as artifacts\n",
        "                if name.startswith('X_'):\n",
        "                    artifact = wandb.Artifact(\n",
        "                        f\"model-input-{name}-{self.pipeline_mode}\",\n",
        "                        type=\"dataset\"\n",
        "                    )\n",
        "                    artifact.add_file(np_path)\n",
        "                    artifact.metadata = {\n",
        "                        \"shape\": array.shape,\n",
        "                        \"dtype\": str(array.dtype),\n",
        "                        \"size_mb\": array.nbytes / (1024 * 1024),\n",
        "                        \"pipeline_mode\": self.pipeline_mode\n",
        "                    }\n",
        "                    self.run.log_artifact(artifact)\n",
        "\n",
        "        # Save as HDF5 for efficient loading\n",
        "        h5_path = f\"{output_dir}/model_inputs.h5\"\n",
        "        with h5py.File(h5_path, 'w') as h5f:\n",
        "            for name, array in sequences.items():\n",
        "                if isinstance(array, np.ndarray):\n",
        "                    h5f.create_dataset(name, data=array, compression='gzip')\n",
        "\n",
        "            # Store pipeline mode as attribute\n",
        "            h5f.attrs['pipeline_mode'] = self.pipeline_mode\n",
        "\n",
        "        # Log HDF5 as artifact\n",
        "        artifact = wandb.Artifact(f\"model-inputs-h5-{self.pipeline_mode}\", type=\"dataset\")\n",
        "        artifact.add_file(h5_path)\n",
        "        artifact.metadata = {\"pipeline_mode\": self.pipeline_mode}\n",
        "        self.run.log_artifact(artifact)\n",
        "\n",
        "        # Create and log metadata\n",
        "        self.create_assembly_report(sequences, output_dir)\n",
        "\n",
        "    def create_assembly_report(self, sequences: Dict[str, np.ndarray],\n",
        "                             output_dir: str):\n",
        "        \"\"\"Create comprehensive assembly report.\"\"\"\n",
        "        import matplotlib.pyplot as plt\n",
        "\n",
        "        # Summary statistics\n",
        "        summary = []\n",
        "        for name, array in sequences.items():\n",
        "            if isinstance(array, np.ndarray) and name.startswith('X_'):\n",
        "                summary.append({\n",
        "                    'dataset': name,\n",
        "                    'shape': str(array.shape),\n",
        "                    'size_mb': f\"{array.nbytes / (1024 * 1024):.2f}\",\n",
        "                    'dtype': str(array.dtype),\n",
        "                    'min': f\"{np.min(array):.4f}\",\n",
        "                    'max': f\"{np.max(array):.4f}\",\n",
        "                    'mean': f\"{np.mean(array):.4f}\",\n",
        "                    'std': f\"{np.std(array):.4f}\",\n",
        "                    'pipeline_mode': self.pipeline_mode\n",
        "                })\n",
        "\n",
        "        if summary:\n",
        "            summary_df = pd.DataFrame(summary)\n",
        "            table = wandb.Table(dataframe=summary_df)\n",
        "            self.run.log({f\"assembly_summary_{self.pipeline_mode}\": table})\n",
        "\n",
        "        # Create visualization\n",
        "        if summary:\n",
        "            fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
        "            fig.suptitle(f'Sequence Assembly Report - {self.pipeline_mode.upper()} Mode', fontsize=16)\n",
        "\n",
        "            # Dataset sizes\n",
        "            ax1 = axes[0, 0]\n",
        "            datasets = [s['dataset'] for s in summary]\n",
        "            sizes = [float(s['size_mb']) for s in summary]\n",
        "            ax1.bar(datasets, sizes, color='skyblue')\n",
        "            ax1.set_xlabel('Dataset')\n",
        "            ax1.set_ylabel('Size (MB)')\n",
        "            ax1.set_title('Dataset Sizes')\n",
        "            ax1.tick_params(axis='x', rotation=45)\n",
        "\n",
        "            # Sample distribution (first dataset)\n",
        "            ax2 = axes[0, 1]\n",
        "            first_dataset_key = next((k for k in sequences.keys() if k.startswith('X_')), None)\n",
        "            if first_dataset_key and first_dataset_key in sequences:\n",
        "                sample_data = sequences[first_dataset_key].flatten()[:10000]  # Sample first 10k points\n",
        "                ax2.hist(sample_data, bins=50, alpha=0.7, color='green')\n",
        "                ax2.set_xlabel('Value')\n",
        "                ax2.set_ylabel('Frequency')\n",
        "                ax2.set_title(f'Sample Value Distribution ({first_dataset_key})')\n",
        "                ax2.grid(True, alpha=0.3)\n",
        "\n",
        "            # Mode-specific information\n",
        "            ax3 = axes[1, 0]\n",
        "            mode_info = [\n",
        "                f\"Pipeline Mode: {self.pipeline_mode.upper()}\",\n",
        "                f\"Total Datasets: {len([k for k in sequences.keys() if k.startswith('X_')])}\",\n",
        "                f\"Assembly Stats: {len(self.assembly_stats)} formats\"\n",
        "            ]\n",
        "\n",
        "            for i, info in enumerate(mode_info):\n",
        "                ax3.text(0.1, 0.8 - i*0.2, info, fontsize=12, va='center', transform=ax3.transAxes)\n",
        "\n",
        "            ax3.set_xlim(0, 1)\n",
        "            ax3.set_ylim(0, 1)\n",
        "            ax3.set_xticks([])\n",
        "            ax3.set_yticks([])\n",
        "            ax3.set_title('Pipeline Information')\n",
        "            for spine in ax3.spines.values():\n",
        "                spine.set_visible(False)\n",
        "\n",
        "            # Memory usage\n",
        "            ax4 = axes[1, 1]\n",
        "            if len(summary) > 0:\n",
        "                memory_usage = {s['dataset']: float(s['size_mb']) for s in summary}\n",
        "                colors = ['lightblue', 'lightgreen', 'lightcoral', 'lightyellow'][:len(memory_usage)]\n",
        "                wedges, texts, autotexts = ax4.pie(\n",
        "                    memory_usage.values(),\n",
        "                    labels=memory_usage.keys(),\n",
        "                    autopct='%1.1f%%',\n",
        "                    startangle=90,\n",
        "                    colors=colors\n",
        "                )\n",
        "                ax4.set_title('Memory Usage Distribution')\n",
        "\n",
        "            plt.tight_layout()\n",
        "            self.run.log({f\"assembly_visualization_{self.pipeline_mode}\": wandb.Image(fig)})\n",
        "            plt.close()\n",
        "\n",
        "        # Save assembly statistics\n",
        "        import json\n",
        "        stats_path = f\"{output_dir}/assembly_stats_{self.pipeline_mode}.json\"\n",
        "        with open(stats_path, 'w') as f:\n",
        "            # Convert numpy types to native Python types for JSON serialization\n",
        "            stats_serializable = {\n",
        "                \"pipeline_mode\": self.pipeline_mode,\n",
        "                \"assembly_stats\": {}\n",
        "            }\n",
        "            for key, value in self.assembly_stats.items():\n",
        "                if isinstance(value, dict):\n",
        "                    stats_serializable[\"assembly_stats\"][key] = {\n",
        "                        k: v.tolist() if isinstance(v, np.ndarray) else v\n",
        "                        for k, v in value.items()\n",
        "                    }\n",
        "                else:\n",
        "                    stats_serializable[\"assembly_stats\"][key] = value\n",
        "            json.dump(stats_serializable, f, indent=2)\n",
        "\n",
        "def main():\n",
        "    import argparse\n",
        "\n",
        "    parser = argparse.ArgumentParser(description=\"Assemble sequences for model input\")\n",
        "    parser.add_argument(\"--input-file\", default=\"features/reduced_features.csv\", help=\"Input file\")\n",
        "    parser.add_argument(\"--output-dir\", default=\"data\", help=\"Output directory\")\n",
        "    parser.add_argument(\"--sequence-length\", type=int, default=10, help=\"Sequence length for LSTM\")\n",
        "    parser.add_argument(\"--overlap\", type=float, default=0.5, help=\"Overlap for sequences\")\n",
        "    parser.add_argument(\"--train-ratio\", type=float, default=0.8, help=\"Training set ratio (for train mode)\")\n",
        "    parser.add_argument(\"--pipeline-mode\", choices=[\"train\", \"test\"], default=\"train\",\n",
        "                       help=\"Pipeline mode: 'train' or 'test'\")\n",
        "\n",
        "    # Parse known arguments, ignoring the rest\n",
        "    args, unknown = parser.parse_known_args()\n",
        "\n",
        "    # Initialize W&B\n",
        "    run = wandb.init(project=\"predictive-maintenance\", job_type=\"sequence-assembly\")\n",
        "\n",
        "    # Load features\n",
        "    features = pd.read_csv(args.input_file)\n",
        "\n",
        "    # Create sequences with mode-specific behavior\n",
        "    assembler = SequenceAssembler(run, pipeline_mode=args.pipeline_mode)\n",
        "\n",
        "    # Create sequences based on mode\n",
        "    sequences = assembler.create_sequences_for_mode(\n",
        "        features,\n",
        "        sequence_length=args.sequence_length,\n",
        "        overlap=args.overlap,\n",
        "        train_ratio=args.train_ratio\n",
        "    )\n",
        "\n",
        "    # Also create tabular format for compatibility\n",
        "    X_tabular = assembler.create_tabular_format(features)\n",
        "    if args.pipeline_mode == \"train\":\n",
        "        # For train mode, split tabular data too\n",
        "        n_samples = len(X_tabular)\n",
        "        train_end = int(n_samples * args.train_ratio)\n",
        "        sequences['X_tabular_train'] = X_tabular[:train_end]\n",
        "        sequences['X_tabular_val'] = X_tabular[train_end:]\n",
        "    else:\n",
        "        # For test mode, keep all tabular data\n",
        "        sequences['X_tabular_test'] = X_tabular\n",
        "\n",
        "    # Save sequences\n",
        "    assembler.save_sequences(sequences, args.output_dir)\n",
        "\n",
        "    run.finish()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "CRCo97mjFy2b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile src/monitoring/drift.py\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import wandb\n",
        "from scipy.stats import wasserstein_distance, ks_2samp\n",
        "from typing import Dict, Tuple, List\n",
        "import json\n",
        "import yaml\n",
        "\n",
        "class DriftMonitor:\n",
        "    \"\"\"Monitor data drift and trigger retraining with W&B integration.\"\"\"\n",
        "\n",
        "    def __init__(self, wandb_run: wandb.run = None):\n",
        "        self.run = wandb_run or wandb.init(project=\"predictive-maintenance\")\n",
        "        self.drift_metrics = {}\n",
        "\n",
        "    def compute_psi(self, baseline: np.ndarray, current: np.ndarray,\n",
        "                   bins: int = 10) -> float:\n",
        "        \"\"\"Compute Population Stability Index (PSI).\"\"\"\n",
        "        # Create bins based on baseline\n",
        "        min_val = min(baseline.min(), current.min())\n",
        "        max_val = max(baseline.max(), current.max())\n",
        "        bins_edges = np.linspace(min_val, max_val, bins + 1)\n",
        "\n",
        "        # Calculate frequencies\n",
        "        baseline_freq, _ = np.histogram(baseline, bins=bins_edges)\n",
        "        current_freq, _ = np.histogram(current, bins=bins_edges)\n",
        "\n",
        "        # Normalize to probabilities\n",
        "        baseline_prob = (baseline_freq + 1) / (len(baseline) + bins)  # Add 1 to avoid log(0)\n",
        "        current_prob = (current_freq + 1) / (len(current) + bins)\n",
        "\n",
        "        # Calculate PSI\n",
        "        psi = np.sum((current_prob - baseline_prob) * np.log(current_prob / baseline_prob))\n",
        "\n",
        "        return float(psi)\n",
        "\n",
        "    def compute_kl_divergence(self, baseline: np.ndarray, current: np.ndarray,\n",
        "                            bins: int = 50) -> float:\n",
        "        \"\"\"Compute Kullback-Leibler divergence.\"\"\"\n",
        "        # Create common bins\n",
        "        min_val = min(baseline.min(), current.min())\n",
        "        max_val = max(baseline.max(), current.max())\n",
        "        bins_edges = np.linspace(min_val, max_val, bins + 1)\n",
        "\n",
        "        # Calculate probabilities\n",
        "        p, _ = np.histogram(baseline, bins=bins_edges, density=True)\n",
        "        q, _ = np.histogram(current, bins=bins_edges, density=True)\n",
        "\n",
        "        # Normalize\n",
        "        p = p + 1e-10\n",
        "        q = q + 1e-10\n",
        "        p = p / p.sum()\n",
        "        q = q / q.sum()\n",
        "\n",
        "        # KL divergence\n",
        "        kl = np.sum(p * np.log(p / q))\n",
        "\n",
        "        return float(kl)\n",
        "\n",
        "    def compute_wasserstein(self, baseline: np.ndarray, current: np.ndarray) -> float:\n",
        "        \"\"\"Compute Wasserstein distance.\"\"\"\n",
        "        return float(wasserstein_distance(baseline, current))\n",
        "\n",
        "    def compute_ks_statistic(self, baseline: np.ndarray, current: np.ndarray) -> Tuple[float, float]:\n",
        "        \"\"\"Compute Kolmogorov-Smirnov test statistic.\"\"\"\n",
        "        ks_stat, p_value = ks_2samp(baseline, current)\n",
        "        return float(ks_stat), float(p_value)\n",
        "\n",
        "    def monitor_feature_drift(self, baseline_features: pd.DataFrame,\n",
        "                            current_features: pd.DataFrame) -> Dict[str, Dict[str, float]]:\n",
        "        \"\"\"Monitor drift for all features.\"\"\"\n",
        "        # Get common feature columns\n",
        "        feature_cols = [col for col in baseline_features.columns\n",
        "                       if col in current_features.columns and col not in ['window_id', 'window_start']]\n",
        "\n",
        "        drift_results = {}\n",
        "\n",
        "        for col in feature_cols:\n",
        "            baseline_vals = baseline_features[col].dropna().values\n",
        "            current_vals = current_features[col].dropna().values\n",
        "\n",
        "            if len(baseline_vals) < 10 or len(current_vals) < 10:\n",
        "                continue\n",
        "\n",
        "            # Compute drift metrics\n",
        "            psi = self.compute_psi(baseline_vals, current_vals)\n",
        "            kl = self.compute_kl_divergence(baseline_vals, current_vals)\n",
        "            wasserstein = self.compute_wasserstein(baseline_vals, current_vals)\n",
        "            ks_stat, ks_pvalue = self.compute_ks_statistic(baseline_vals, current_vals)\n",
        "\n",
        "            drift_results[col] = {\n",
        "                'psi': psi,\n",
        "                'kl_divergence': kl,\n",
        "                'wasserstein': wasserstein,\n",
        "                'ks_statistic': ks_stat,\n",
        "                'ks_pvalue': ks_pvalue\n",
        "            }\n",
        "\n",
        "            # Log individual metrics\n",
        "            self.run.log({\n",
        "                f\"drift_psi_{col}\": psi,\n",
        "                f\"drift_kl_{col}\": kl,\n",
        "                f\"drift_wasserstein_{col}\": wasserstein,\n",
        "                f\"drift_ks_{col}\": ks_stat\n",
        "            })\n",
        "\n",
        "        self.drift_metrics = drift_results\n",
        "        return drift_results\n",
        "\n",
        "    def evaluate_drift_severity(self, drift_results: Dict[str, Dict[str, float]],\n",
        "                              thresholds: Dict[str, float] = None) -> Dict[str, str]:\n",
        "        \"\"\"Evaluate drift severity based on thresholds.\"\"\"\n",
        "        if thresholds is None:\n",
        "            thresholds = {\n",
        "                'psi': {'low': 0.1, 'medium': 0.2, 'high': 0.3},\n",
        "                'kl_divergence': {'low': 0.5, 'medium': 1.0, 'high': 2.0},\n",
        "                'wasserstein': {'low': 0.1, 'medium': 0.3, 'high': 0.5},\n",
        "                'ks_statistic': {'low': 0.1, 'medium': 0.2, 'high': 0.3}\n",
        "            }\n",
        "\n",
        "        severity_results = {}\n",
        "\n",
        "        for feature, metrics in drift_results.items():\n",
        "            severities = []\n",
        "\n",
        "            for metric_name, metric_value in metrics.items():\n",
        "                if metric_name == 'ks_pvalue':\n",
        "                    continue\n",
        "\n",
        "                if metric_name in thresholds:\n",
        "                    thresh = thresholds[metric_name]\n",
        "                    if metric_value < thresh['low']:\n",
        "                        severity = 'none'\n",
        "                    elif metric_value < thresh['medium']:\n",
        "                        severity = 'low'\n",
        "                    elif metric_value < thresh['high']:\n",
        "                        severity = 'medium'\n",
        "                    else:\n",
        "                        severity = 'high'\n",
        "\n",
        "                    severities.append(severity)\n",
        "\n",
        "            # Overall severity is the maximum\n",
        "            severity_order = ['none', 'low', 'medium', 'high']\n",
        "            max_severity = max(severities, key=lambda x: severity_order.index(x))\n",
        "            severity_results[feature] = max_severity\n",
        "\n",
        "        return severity_results\n",
        "\n",
        "    def create_drift_dashboard(self, drift_results: Dict[str, Dict[str, float]],\n",
        "                             severity_results: Dict[str, str]):\n",
        "        \"\"\"Create comprehensive drift visualization dashboard.\"\"\"\n",
        "        import matplotlib.pyplot as plt\n",
        "        import seaborn as sns\n",
        "\n",
        "        # Prepare data for visualization\n",
        "        features = list(drift_results.keys())[:20]  # Top 20 features\n",
        "        psi_values = [drift_results[f]['psi'] for f in features]\n",
        "        kl_values = [drift_results[f]['kl_divergence'] for f in features]\n",
        "\n",
        "        fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
        "\n",
        "        # PSI values\n",
        "        ax1 = axes[0, 0]\n",
        "        colors = ['green' if v < 0.1 else 'yellow' if v < 0.2 else 'red' for v in psi_values]\n",
        "        bars = ax1.barh(range(len(features)), psi_values, color=colors)\n",
        "        ax1.set_yticks(range(len(features)))\n",
        "        ax1.set_yticklabels(features, fontsize=8)\n",
        "        ax1.set_xlabel('PSI Value')\n",
        "        ax1.set_title('Population Stability Index by Feature')\n",
        "        ax1.axvline(x=0.1, color='green', linestyle='--', alpha=0.5, label='Low drift')\n",
        "        ax1.axvline(x=0.2, color='orange', linestyle='--', alpha=0.5, label='Medium drift')\n",
        "        ax1.axvline(x=0.3, color='red', linestyle='--', alpha=0.5, label='High drift')\n",
        "        ax1.legend()\n",
        "\n",
        "        # Severity distribution\n",
        "        ax2 = axes[0, 1]\n",
        "        severity_counts = pd.Series(severity_results.values()).value_counts()\n",
        "        colors_severity = {'none': 'green', 'low': 'yellow', 'medium': 'orange', 'high': 'red'}\n",
        "        ax2.pie(severity_counts.values, labels=severity_counts.index, autopct='%1.1f%%',\n",
        "               colors=[colors_severity[s] for s in severity_counts.index],\n",
        "               startangle=90)\n",
        "        ax2.set_title('Overall Drift Severity Distribution')\n",
        "\n",
        "        # Heatmap of drift metrics\n",
        "        ax3 = axes[1, 0]\n",
        "        metrics_df = pd.DataFrame(drift_results).T.head(15)\n",
        "        metrics_df = metrics_df[['psi', 'kl_divergence', 'wasserstein', 'ks_statistic']]\n",
        "\n",
        "        # Normalize for better visualization\n",
        "        metrics_normalized = (metrics_df - metrics_df.min()) / (metrics_df.max() - metrics_df.min())\n",
        "\n",
        "        sns.heatmap(metrics_normalized, annot=True, fmt='.2f', cmap='YlOrRd',\n",
        "                   ax=ax3, cbar_kws={'label': 'Normalized Drift Score'})\n",
        "        ax3.set_title('Drift Metrics Heatmap (Top 15 Features)')\n",
        "        ax3.set_xlabel('Drift Metric')\n",
        "        ax3.set_ylabel('Feature')\n",
        "\n",
        "        # Time series of average drift (if historical data available)\n",
        "        ax4 = axes[1, 1]\n",
        "        avg_psi = np.mean(psi_values)\n",
        "        avg_kl = np.mean(kl_values)\n",
        "\n",
        "        # Simulate historical data for demo\n",
        "        time_points = pd.date_range(end=pd.Timestamp.now(), periods=10, freq='D')\n",
        "        historical_psi = np.random.normal(avg_psi, 0.05, 10)\n",
        "        historical_psi[-1] = avg_psi\n",
        "\n",
        "        ax4.plot(time_points, historical_psi, 'o-', label='PSI', linewidth=2)\n",
        "        ax4.axhline(y=0.1, color='green', linestyle='--', alpha=0.5)\n",
        "        ax4.axhline(y=0.2, color='orange', linestyle='--', alpha=0.5)\n",
        "        ax4.axhline(y=0.3, color='red', linestyle='--', alpha=0.5)\n",
        "        ax4.set_xlabel('Date')\n",
        "        ax4.set_ylabel('Average PSI')\n",
        "        ax4.set_title('Drift Trend Over Time')\n",
        "        ax4.legend()\n",
        "        ax4.grid(True, alpha=0.3)\n",
        "\n",
        "        plt.tight_layout()\n",
        "        self.run.log({\"drift_dashboard\": wandb.Image(fig)})\n",
        "        plt.close()\n",
        "\n",
        "    def generate_retraining_sweep(self, drift_severity: Dict[str, str],\n",
        "                                output_path: str = \"configs/sweep.yaml\"):\n",
        "        \"\"\"Generate W&B sweep configuration for retraining.\"\"\"\n",
        "        # Count high severity drifts\n",
        "        high_drift_count = sum(1 for s in drift_severity.values() if s == 'high')\n",
        "        medium_drift_count = sum(1 for s in drift_severity.values() if s == 'medium')\n",
        "\n",
        "        # Determine if retraining is needed\n",
        "        retrain_needed = high_drift_count > 5 or medium_drift_count > 10\n",
        "\n",
        "        if retrain_needed:\n",
        "            # Create sweep configuration\n",
        "            sweep_config = {\n",
        "                'method': 'bayes',\n",
        "                'name': f'drift-triggered-retraining-{pd.Timestamp.now().strftime(\"%Y%m%d\")}',\n",
        "                'metric': {\n",
        "                    'name': 'validation_loss',\n",
        "                    'goal': 'minimize'\n",
        "                },\n",
        "                'parameters': {\n",
        "                    'learning_rate': {\n",
        "                        'min': 0.0001,\n",
        "                        'max': 0.01,\n",
        "                        'distribution': 'log_uniform'\n",
        "                    },\n",
        "                    'batch_size': {\n",
        "                        'values': [32, 64, 128, 256]\n",
        "                    },\n",
        "                    'hidden_size': {\n",
        "                        'values': [64, 128, 256]\n",
        "                    },\n",
        "                    'dropout': {\n",
        "                        'min': 0.1,\n",
        "                        'max': 0.5,\n",
        "                        'distribution': 'uniform'\n",
        "                    },\n",
        "                    'retrain_preprocessing': {\n",
        "                        'value': True\n",
        "                    }\n",
        "                },\n",
        "                'early_terminate': {\n",
        "                    'type': 'hyperband',\n",
        "                    'min_iter': 5,\n",
        "                    'eta': 3\n",
        "                }\n",
        "            }\n",
        "\n",
        "            # Save sweep configuration\n",
        "            with open(output_path, 'w') as f:\n",
        "                yaml.dump(sweep_config, f)\n",
        "\n",
        "            # Log sweep config as artifact\n",
        "            artifact = wandb.Artifact(\"retraining-sweep-config\", type=\"config\")\n",
        "            artifact.add_file(output_path)\n",
        "            artifact.metadata = {\n",
        "                \"triggered_by\": \"drift_detection\",\n",
        "                \"high_drift_features\": high_drift_count,\n",
        "                \"medium_drift_features\": medium_drift_count\n",
        "            }\n",
        "            self.run.log_artifact(artifact)\n",
        "\n",
        "            # Log alert\n",
        "            wandb.alert(\n",
        "                title=\"High Data Drift Detected\",\n",
        "                text=f\"Detected {high_drift_count} features with high drift. Retraining recommended.\",\n",
        "                level=wandb.AlertLevel.WARN\n",
        "            )\n",
        "\n",
        "            self.run.log({\n",
        "                \"retraining_triggered\": True,\n",
        "                \"high_drift_features\": high_drift_count,\n",
        "                \"medium_drift_features\": medium_drift_count\n",
        "            })\n",
        "\n",
        "        return retrain_needed\n",
        "\n",
        "    def save_drift_report(self, drift_results: Dict[str, Dict[str, float]],\n",
        "                         severity_results: Dict[str, str],\n",
        "                         output_dir: str = \"monitoring\"):\n",
        "        \"\"\"Save comprehensive drift report.\"\"\"\n",
        "        import os\n",
        "        os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "        # Create report dataframe\n",
        "        report_data = []\n",
        "        for feature, metrics in drift_results.items():\n",
        "            row = {'feature': feature}\n",
        "            row.update(metrics)\n",
        "            row['severity'] = severity_results.get(feature, 'unknown')\n",
        "            report_data.append(row)\n",
        "\n",
        "        report_df = pd.DataFrame(report_data)\n",
        "        report_df = report_df.sort_values('psi', ascending=False)\n",
        "\n",
        "        # Save report\n",
        "        report_path = f\"{output_dir}/drift_report.csv\"\n",
        "        report_df.to_csv(report_path, index=False)\n",
        "\n",
        "        # Log as table\n",
        "        table = wandb.Table(dataframe=report_df.head(50))\n",
        "        self.run.log({\"drift_report\": table})\n",
        "\n",
        "        # Save summary statistics\n",
        "        summary = {\n",
        "            \"timestamp\": pd.Timestamp.now().isoformat(),\n",
        "            \"total_features\": len(drift_results),\n",
        "            \"avg_psi\": float(report_df['psi'].mean()),\n",
        "            \"max_psi\": float(report_df['psi'].max()),\n",
        "            \"features_with_high_drift\": int((report_df['severity'] == 'high').sum()),\n",
        "            \"features_with_medium_drift\": int((report_df['severity'] == 'medium').sum()),\n",
        "            \"features_with_low_drift\": int((report_df['severity'] == 'low').sum())\n",
        "        }\n",
        "\n",
        "        summary_path = f\"{output_dir}/drift_summary.json\"\n",
        "        with open(summary_path, 'w') as f:\n",
        "            json.dump(summary, f, indent=2)\n",
        "\n",
        "def main():\n",
        "    import argparse\n",
        "\n",
        "    parser = argparse.ArgumentParser(description=\"Monitor data drift\")\n",
        "    parser.add_argument(\"--baseline-file\", default=\"features/reduced_features.csv\",\n",
        "                       help=\"Baseline features file\")\n",
        "    parser.add_argument(\"--current-file\", required=True, help=\"Current features file\")\n",
        "    parser.add_argument(\"--psi-threshold\", type=float, default=0.2, help=\"PSI threshold for alerts\")\n",
        "    parser.add_argument(\"--output-dir\", default=\"monitoring\", help=\"Output directory\")\n",
        "\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    # Initialize W&B\n",
        "    run = wandb.init(project=\"predictive-maintenance\", job_type=\"drift-monitoring\")\n",
        "\n",
        "    # Load data\n",
        "    baseline_features = pd.read_csv(args.baseline_file)\n",
        "    current_features = pd.read_csv(args.current_file)\n",
        "\n",
        "    # Monitor drift\n",
        "    monitor = DriftMonitor(run)\n",
        "    drift_results = monitor.monitor_feature_drift(baseline_features, current_features)\n",
        "\n",
        "    # Evaluate severity\n",
        "    severity_results = monitor.evaluate_drift_severity(drift_results)\n",
        "\n",
        "    # Create visualizations\n",
        "    monitor.create_drift_dashboard(drift_results, severity_results)\n",
        "\n",
        "    # Generate retraining sweep if needed\n",
        "    retrain_needed = monitor.generate_retraining_sweep(severity_results)\n",
        "\n",
        "    # Save report\n",
        "    monitor.save_drift_report(drift_results, severity_results, args.output_dir)\n",
        "\n",
        "    run.finish()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "AGOczL5kGA7y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile src/model/lstm_anomaly_detector.py\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import wandb\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from typing import Dict, Tuple, Optional, Union\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import h5py\n",
        "import os\n",
        "from sklearn.metrics import precision_recall_curve, roc_auc_score\n",
        "\n",
        "class LSTMAutoencoder(nn.Module):\n",
        "    \"\"\"LSTM Autoencoder for anomaly detection.\"\"\"\n",
        "\n",
        "    def __init__(self, input_dim: int, hidden_dim: int = 64,\n",
        "                 num_layers: int = 2, dropout: float = 0.2):\n",
        "        super(LSTMAutoencoder, self).__init__()\n",
        "\n",
        "        self.input_dim = input_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        # Encoder\n",
        "        self.encoder = nn.LSTM(\n",
        "            input_dim, hidden_dim, num_layers,\n",
        "            batch_first=True, dropout=dropout\n",
        "        )\n",
        "\n",
        "        # Decoder\n",
        "        self.decoder = nn.LSTM(\n",
        "            hidden_dim, hidden_dim, num_layers,\n",
        "            batch_first=True, dropout=dropout\n",
        "        )\n",
        "\n",
        "        # Output layer\n",
        "        self.output_layer = nn.Linear(hidden_dim, input_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Encoder\n",
        "        encoded, (h_n, c_n) = self.encoder(x)\n",
        "\n",
        "        # Use the last hidden state for decoding\n",
        "        batch_size, seq_len, _ = x.shape\n",
        "        decoder_input = encoded[:, -1, :].unsqueeze(1).repeat(1, seq_len, 1)\n",
        "\n",
        "        # Decoder\n",
        "        decoded, _ = self.decoder(decoder_input, (h_n, c_n))\n",
        "\n",
        "        # Output\n",
        "        output = self.output_layer(decoded)\n",
        "\n",
        "        return output\n",
        "\n",
        "class AnomalyDetector:\n",
        "    \"\"\"LSTM-based anomaly detector with pipeline mode support.\"\"\"\n",
        "\n",
        "    def __init__(self, wandb_run: wandb.run = None, pipeline_mode: str = \"train\"):\n",
        "        self.run = wandb_run or wandb.init(project=\"predictive-maintenance\")\n",
        "        self.pipeline_mode = pipeline_mode\n",
        "        self.model = None\n",
        "        self.threshold = None\n",
        "        self.training_history = {}\n",
        "        self.anomaly_stats = {}\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "        # Log pipeline mode\n",
        "        self.run.log({\"pipeline_mode\": pipeline_mode})\n",
        "\n",
        "    def load_sequences(self, data_path: str) -> Dict[str, np.ndarray]:\n",
        "        \"\"\"Load sequences from HDF5 file.\"\"\"\n",
        "        sequences = {}\n",
        "\n",
        "        with h5py.File(data_path, 'r') as h5f:\n",
        "            for key in h5f.keys():\n",
        "                sequences[key] = h5f[key][:]\n",
        "\n",
        "            # Check if pipeline mode is stored in file\n",
        "            if 'pipeline_mode' in h5f.attrs:\n",
        "                file_mode = h5f.attrs['pipeline_mode']\n",
        "                if isinstance(file_mode, bytes):\n",
        "                    file_mode = file_mode.decode('utf-8')\n",
        "\n",
        "                self.run.log({\"data_file_pipeline_mode\": file_mode})\n",
        "\n",
        "        return sequences\n",
        "\n",
        "    def create_model(self, input_dim: int, config: Dict):\n",
        "        \"\"\"Create LSTM autoencoder model.\"\"\"\n",
        "        self.model = LSTMAutoencoder(\n",
        "            input_dim=input_dim,\n",
        "            hidden_dim=config.get('hidden_dim', 64),\n",
        "            num_layers=config.get('num_layers', 2),\n",
        "            dropout=config.get('dropout', 0.2)\n",
        "        ).to(self.device)\n",
        "\n",
        "        # Log model configuration\n",
        "        self.run.log({\n",
        "            \"model_input_dim\": input_dim,\n",
        "            \"model_hidden_dim\": config.get('hidden_dim', 64),\n",
        "            \"model_num_layers\": config.get('num_layers', 2),\n",
        "            \"model_dropout\": config.get('dropout', 0.2),\n",
        "            \"model_total_params\": sum(p.numel() for p in self.model.parameters())\n",
        "        })\n",
        "\n",
        "    def train_model(self, X_train: np.ndarray, X_val: np.ndarray,\n",
        "                   epochs: int = 100, batch_size: int = 32):\n",
        "        \"\"\"Train the LSTM autoencoder (only in train mode).\"\"\"\n",
        "        if self.pipeline_mode != \"train\":\n",
        "            raise ValueError(\"Model training is only available in 'train' pipeline mode\")\n",
        "\n",
        "        if self.model is None:\n",
        "            raise ValueError(\"Model not created. Call create_model() first.\")\n",
        "\n",
        "        # Convert to tensors\n",
        "        X_train_tensor = torch.FloatTensor(X_train).to(self.device)\n",
        "        X_val_tensor = torch.FloatTensor(X_val).to(self.device)\n",
        "\n",
        "        # Create data loaders\n",
        "        train_dataset = TensorDataset(X_train_tensor, X_train_tensor)\n",
        "        val_dataset = TensorDataset(X_val_tensor, X_val_tensor)\n",
        "\n",
        "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "        val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "        # Optimizer and loss\n",
        "        optimizer = optim.Adam(self.model.parameters(), lr=0.001)\n",
        "        criterion = nn.MSELoss()\n",
        "\n",
        "        # Training history\n",
        "        train_losses = []\n",
        "        val_losses = []\n",
        "\n",
        "        self.model.train()\n",
        "        for epoch in range(epochs):\n",
        "            # Training\n",
        "            train_loss = 0.0\n",
        "            for batch_x, batch_y in train_loader:\n",
        "                optimizer.zero_grad()\n",
        "                outputs = self.model(batch_x)\n",
        "                loss = criterion(outputs, batch_y)\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "                train_loss += loss.item()\n",
        "\n",
        "            # Validation\n",
        "            val_loss = 0.0\n",
        "            self.model.eval()\n",
        "            with torch.no_grad():\n",
        "                for batch_x, batch_y in val_loader:\n",
        "                    outputs = self.model(batch_x)\n",
        "                    loss = criterion(outputs, batch_y)\n",
        "                    val_loss += loss.item()\n",
        "            self.model.train()\n",
        "\n",
        "            avg_train_loss = train_loss / len(train_loader)\n",
        "            avg_val_loss = val_loss / len(val_loader)\n",
        "\n",
        "            train_losses.append(avg_train_loss)\n",
        "            val_losses.append(avg_val_loss)\n",
        "\n",
        "            # Log every 10 epochs\n",
        "            if (epoch + 1) % 10 == 0:\n",
        "                self.run.log({\n",
        "                    \"epoch\": epoch + 1,\n",
        "                    \"train_loss\": avg_train_loss,\n",
        "                    \"val_loss\": avg_val_loss\n",
        "                })\n",
        "\n",
        "            if (epoch + 1) % 50 == 0:\n",
        "                print(f\"Epoch {epoch+1}/{epochs}, Train Loss: {avg_train_loss:.6f}, Val Loss: {avg_val_loss:.6f}\")\n",
        "\n",
        "        # Store training history\n",
        "        self.training_history = {\n",
        "            'train_losses': train_losses,\n",
        "            'val_losses': val_losses,\n",
        "            'epochs': epochs\n",
        "        }\n",
        "\n",
        "        # Log final metrics\n",
        "        self.run.log({\n",
        "            \"final_train_loss\": train_losses[-1],\n",
        "            \"final_val_loss\": val_losses[-1],\n",
        "            \"training_epochs\": epochs\n",
        "        })\n",
        "\n",
        "    def load_pretrained_model(self, model_path: str):\n",
        "        \"\"\"Load a pre-trained model (for test mode).\"\"\"\n",
        "        if self.pipeline_mode != \"test\":\n",
        "            raise ValueError(\"Loading pre-trained model is only available in 'test' pipeline mode\")\n",
        "\n",
        "        if not os.path.exists(model_path):\n",
        "            raise FileNotFoundError(f\"Model file not found: {model_path}\")\n",
        "\n",
        "        checkpoint = torch.load(model_path, map_location=self.device)\n",
        "\n",
        "        # Create model with saved configuration\n",
        "        config = checkpoint.get('config', {})\n",
        "        input_dim = checkpoint['input_dim']\n",
        "\n",
        "        self.create_model(input_dim, config)\n",
        "        self.model.load_state_dict(checkpoint['model_state_dict'])\n",
        "        self.threshold = checkpoint.get('threshold', None)\n",
        "        self.training_history = checkpoint.get('training_history', {})\n",
        "\n",
        "        self.model.eval()\n",
        "\n",
        "        self.run.log({\n",
        "            \"loaded_model_path\": model_path,\n",
        "            \"loaded_threshold\": self.threshold,\n",
        "            \"model_loaded_successfully\": True\n",
        "        })\n",
        "\n",
        "    def set_threshold(self, X_val: np.ndarray, percentile: float = 95):\n",
        "        \"\"\"Set anomaly detection threshold based on validation data.\"\"\"\n",
        "        if self.model is None:\n",
        "            raise ValueError(\"Model not available. Train or load a model first.\")\n",
        "\n",
        "        self.model.eval()\n",
        "        X_val_tensor = torch.FloatTensor(X_val).to(self.device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            reconstructions = self.model(X_val_tensor)\n",
        "            reconstruction_errors = torch.mean((X_val_tensor - reconstructions) ** 2, dim=(1, 2))\n",
        "            reconstruction_errors = reconstruction_errors.cpu().numpy()\n",
        "\n",
        "        self.threshold = np.percentile(reconstruction_errors, percentile)\n",
        "\n",
        "        self.run.log({\n",
        "            \"threshold_percentile\": percentile,\n",
        "            \"threshold_value\": self.threshold,\n",
        "            \"val_reconstruction_error_mean\": np.mean(reconstruction_errors),\n",
        "            \"val_reconstruction_error_std\": np.std(reconstruction_errors)\n",
        "        })\n",
        "\n",
        "    def detect_anomalies(self, X_test: np.ndarray,\n",
        "                        timestamps: Optional[np.ndarray] = None) -> Dict:\n",
        "        \"\"\"Detect anomalies in test data.\"\"\"\n",
        "        if self.model is None:\n",
        "            raise ValueError(\"Model not available. Train or load a model first.\")\n",
        "        if self.threshold is None:\n",
        "            raise ValueError(\"Threshold not set. Call set_threshold() first.\")\n",
        "\n",
        "        self.model.eval()\n",
        "        X_test_tensor = torch.FloatTensor(X_test).to(self.device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            reconstructions = self.model(X_test_tensor)\n",
        "            reconstruction_errors = torch.mean((X_test_tensor - reconstructions) ** 2, dim=(1, 2))\n",
        "            reconstruction_errors = reconstruction_errors.cpu().numpy()\n",
        "\n",
        "        # Detect anomalies\n",
        "        anomalies = reconstruction_errors > self.threshold\n",
        "\n",
        "        # Statistics\n",
        "        num_anomalies = np.sum(anomalies)\n",
        "        anomaly_rate = num_anomalies / len(X_test)\n",
        "\n",
        "        self.anomaly_stats = {\n",
        "            'total_samples': len(X_test),\n",
        "            'anomalies_detected': int(num_anomalies),\n",
        "            'anomaly_rate': float(anomaly_rate),\n",
        "            'mean_reconstruction_error': float(np.mean(reconstruction_errors)),\n",
        "            'std_reconstruction_error': float(np.std(reconstruction_errors)),\n",
        "            'max_reconstruction_error': float(np.max(reconstruction_errors)),\n",
        "            'min_reconstruction_error': float(np.min(reconstruction_errors))\n",
        "        }\n",
        "\n",
        "        # Log results\n",
        "        self.run.log(self.anomaly_stats)\n",
        "\n",
        "        return {\n",
        "            'anomalies': anomalies,\n",
        "            'reconstruction_errors': reconstruction_errors,\n",
        "            'timestamps': timestamps,\n",
        "            'statistics': self.anomaly_stats,\n",
        "            'threshold': self.threshold\n",
        "        }\n",
        "\n",
        "    def create_training_visualizations(self):\n",
        "        \"\"\"Create training visualizations (only for train mode).\"\"\"\n",
        "        if self.pipeline_mode != \"train\" or not self.training_history:\n",
        "            return\n",
        "\n",
        "        fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
        "\n",
        "        # Training history\n",
        "        epochs = range(1, len(self.training_history['train_losses']) + 1)\n",
        "        axes[0].plot(epochs, self.training_history['train_losses'], 'b-', label='Training Loss')\n",
        "        axes[0].plot(epochs, self.training_history['val_losses'], 'r-', label='Validation Loss')\n",
        "        axes[0].set_xlabel('Epoch')\n",
        "        axes[0].set_ylabel('Loss (MSE)')\n",
        "        axes[0].set_title('Training History')\n",
        "        axes[0].legend()\n",
        "        axes[0].grid(True, alpha=0.3)\n",
        "\n",
        "        # Loss distribution\n",
        "        train_losses = self.training_history['train_losses']\n",
        "        val_losses = self.training_history['val_losses']\n",
        "\n",
        "        axes[1].hist(train_losses, bins=50, alpha=0.7, label='Training Loss', color='blue')\n",
        "        axes[1].hist(val_losses, bins=50, alpha=0.7, label='Validation Loss', color='red')\n",
        "        axes[1].set_xlabel('Loss Value')\n",
        "        axes[1].set_ylabel('Frequency')\n",
        "        axes[1].set_title('Loss Distribution')\n",
        "        axes[1].legend()\n",
        "        axes[1].grid(True, alpha=0.3)\n",
        "\n",
        "        plt.tight_layout()\n",
        "        self.run.log({\"training_visualization\": wandb.Image(fig)})\n",
        "        plt.close()\n",
        "\n",
        "    def create_anomaly_visualizations(self, results: Dict, X_test: np.ndarray):\n",
        "        \"\"\"Create anomaly detection visualizations.\"\"\"\n",
        "        fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
        "\n",
        "        # Reconstruction errors over time\n",
        "        ax1 = axes[0, 0]\n",
        "        errors = results['reconstruction_errors']\n",
        "        ax1.plot(errors, 'b-', alpha=0.7, linewidth=1)\n",
        "        ax1.axhline(y=self.threshold, color='r', linestyle='--', label=f'Threshold: {self.threshold:.4f}')\n",
        "        ax1.fill_between(range(len(errors)), errors, self.threshold,\n",
        "                        where=(errors > self.threshold), alpha=0.3, color='red', label='Anomalies')\n",
        "        ax1.set_xlabel('Sample Index')\n",
        "        ax1.set_ylabel('Reconstruction Error')\n",
        "        ax1.set_title('Reconstruction Errors Over Time')\n",
        "        ax1.legend()\n",
        "        ax1.grid(True, alpha=0.3)\n",
        "\n",
        "        # Reconstruction error distribution\n",
        "        ax2 = axes[0, 1]\n",
        "        ax2.hist(errors, bins=50, alpha=0.7, color='skyblue', edgecolor='black')\n",
        "        ax2.axvline(x=self.threshold, color='r', linestyle='--', label=f'Threshold: {self.threshold:.4f}')\n",
        "        ax2.set_xlabel('Reconstruction Error')\n",
        "        ax2.set_ylabel('Frequency')\n",
        "        ax2.set_title('Reconstruction Error Distribution')\n",
        "        ax2.legend()\n",
        "        ax2.grid(True, alpha=0.3)\n",
        "\n",
        "        # Anomaly detection summary\n",
        "        ax3 = axes[1, 0]\n",
        "        anomalies = results['anomalies']\n",
        "        normal_count = np.sum(~anomalies)\n",
        "        anomaly_count = np.sum(anomalies)\n",
        "\n",
        "        labels = ['Normal', 'Anomaly']\n",
        "        counts = [normal_count, anomaly_count]\n",
        "        colors = ['lightgreen', 'lightcoral']\n",
        "\n",
        "        wedges, texts, autotexts = ax3.pie(counts, labels=labels, colors=colors, autopct='%1.1f%%',\n",
        "                                          startangle=90)\n",
        "        ax3.set_title('Anomaly Detection Summary')\n",
        "\n",
        "        # Sample reconstruction comparison (first few samples)\n",
        "        ax4 = axes[1, 1]\n",
        "        n_samples_to_show = min(5, len(X_test))\n",
        "\n",
        "        if hasattr(self, 'model') and self.model is not None:\n",
        "            self.model.eval()\n",
        "            with torch.no_grad():\n",
        "                X_sample = torch.FloatTensor(X_test[:n_samples_to_show]).to(self.device)\n",
        "                reconstructions = self.model(X_sample).cpu().numpy()\n",
        "\n",
        "            # Plot first feature of first few samples\n",
        "            original_values = X_test[:n_samples_to_show, :, 0].flatten()\n",
        "            reconstructed_values = reconstructions[:n_samples_to_show, :, 0].flatten()\n",
        "\n",
        "            indices = range(len(original_values))\n",
        "            ax4.plot(indices, original_values, 'b-', label='Original', alpha=0.7)\n",
        "            ax4.plot(indices, reconstructed_values, 'r--', label='Reconstructed', alpha=0.7)\n",
        "            ax4.set_xlabel('Time Step')\n",
        "            ax4.set_ylabel('Feature Value')\n",
        "            ax4.set_title('Sample Reconstruction (First Feature)')\n",
        "            ax4.legend()\n",
        "            ax4.grid(True, alpha=0.3)\n",
        "\n",
        "        plt.tight_layout()\n",
        "        self.run.log({f\"anomaly_detection_visualization_{self.pipeline_mode}\": wandb.Image(fig)})\n",
        "        plt.close()\n",
        "\n",
        "    def create_predictions_table(self, results: Dict, num_samples: int = 100):\n",
        "        \"\"\"Create a table of predictions for W&B.\"\"\"\n",
        "        anomalies = results['anomalies']\n",
        "        errors = results['reconstruction_errors']\n",
        "        timestamps = results.get('timestamps', np.arange(len(anomalies)))\n",
        "\n",
        "        # Sample data for the table\n",
        "        n_samples = min(num_samples, len(anomalies))\n",
        "        indices = np.linspace(0, len(anomalies)-1, n_samples, dtype=int)\n",
        "\n",
        "        table_data = []\n",
        "        for i in indices:\n",
        "            table_data.append({\n",
        "                'sample_id': int(i),\n",
        "                'timestamp': float(timestamps[i]) if timestamps is not None else i,\n",
        "                'reconstruction_error': float(errors[i]),\n",
        "                'is_anomaly': bool(anomalies[i]),\n",
        "                'anomaly_score': float(errors[i] / self.threshold)\n",
        "            })\n",
        "\n",
        "        table = wandb.Table(dataframe=pd.DataFrame(table_data))\n",
        "        self.run.log({f\"predictions_table_{self.pipeline_mode}\": table})\n",
        "\n",
        "    def create_model_performance_summary(self, X_train: np.ndarray,\n",
        "                                       X_val: np.ndarray, X_test: np.ndarray):\n",
        "        \"\"\"Create a comprehensive model performance summary.\"\"\"\n",
        "        summary_data = {\n",
        "            'pipeline_mode': self.pipeline_mode,\n",
        "            'model_architecture': {\n",
        "                'input_dim': X_train.shape[2] if self.pipeline_mode == \"train\" else X_test.shape[2],\n",
        "                'hidden_dim': self.model.hidden_dim if self.model else 'N/A',\n",
        "                'num_layers': self.model.num_layers if self.model else 'N/A'\n",
        "            },\n",
        "            'data_shapes': {\n",
        "                'train': str(X_train.shape) if self.pipeline_mode == \"train\" else 'N/A',\n",
        "                'val': str(X_val.shape) if self.pipeline_mode == \"train\" else 'N/A',\n",
        "                'test': str(X_test.shape)\n",
        "            },\n",
        "            'training_metrics': self.training_history if self.pipeline_mode == \"train\" else 'N/A',\n",
        "            'anomaly_detection': self.anomaly_stats\n",
        "        }\n",
        "\n",
        "        # Convert to table\n",
        "        flattened_data = []\n",
        "        for category, data in summary_data.items():\n",
        "            if isinstance(data, dict):\n",
        "                for key, value in data.items():\n",
        "                    flattened_data.append({\n",
        "                        'category': category,\n",
        "                        'metric': key,\n",
        "                        'value': str(value)\n",
        "                    })\n",
        "            else:\n",
        "                flattened_data.append({\n",
        "                    'category': category,\n",
        "                    'metric': 'value',\n",
        "                    'value': str(data)\n",
        "                })\n",
        "\n",
        "        table = wandb.Table(dataframe=pd.DataFrame(flattened_data))\n",
        "        self.run.log({f\"model_performance_summary_{self.pipeline_mode}\": table})\n",
        "\n",
        "    def save_model_and_results(self, results: Dict, output_dir: str = \"models\"):\n",
        "        \"\"\"Save model and results.\"\"\"\n",
        "        os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "        if self.pipeline_mode == \"train\" and self.model is not None:\n",
        "            # Save trained model\n",
        "            model_path = f\"{output_dir}/lstm_autoencoder_{self.pipeline_mode}.pth\"\n",
        "            torch.save({\n",
        "                'model_state_dict': self.model.state_dict(),\n",
        "                'threshold': self.threshold,\n",
        "                'training_history': self.training_history,\n",
        "                'config': {\n",
        "                    'hidden_dim': self.model.hidden_dim,\n",
        "                    'num_layers': self.model.num_layers,\n",
        "                    'input_dim': self.model.input_dim\n",
        "                },\n",
        "                'input_dim': self.model.input_dim,\n",
        "                'pipeline_mode': self.pipeline_mode\n",
        "            }, model_path)\n",
        "\n",
        "            # Create model artifact\n",
        "            model_artifact = wandb.Artifact(f\"lstm-anomaly-detector-{self.pipeline_mode}\", type=\"model\")\n",
        "            model_artifact.add_file(model_path)\n",
        "            model_artifact.metadata = {\n",
        "                'threshold': float(self.threshold) if self.threshold else None,\n",
        "                'pipeline_mode': self.pipeline_mode,\n",
        "                'training_epochs': self.training_history.get('epochs', 0)\n",
        "            }\n",
        "            self.run.log_artifact(model_artifact)\n",
        "\n",
        "        # Save results\n",
        "        results_path = f\"{output_dir}/anomaly_results_{self.pipeline_mode}.npz\"\n",
        "        np.savez(results_path,\n",
        "                anomalies=results['anomalies'],\n",
        "                reconstruction_errors=results['reconstruction_errors'],\n",
        "                timestamps=results['timestamps'] if results['timestamps'] is not None else np.array([]),\n",
        "                threshold=self.threshold,\n",
        "                pipeline_mode=self.pipeline_mode)\n",
        "\n",
        "        # Create results artifact\n",
        "        results_artifact = wandb.Artifact(f\"anomaly-detection-results-{self.pipeline_mode}\", type=\"dataset\")\n",
        "        results_artifact.add_file(results_path)\n",
        "        results_artifact.metadata = {\n",
        "            'pipeline_mode': self.pipeline_mode,\n",
        "            'anomalies_detected': int(results['statistics']['anomalies_detected']),\n",
        "            'anomaly_rate': float(results['statistics']['anomaly_rate'])\n",
        "        }\n",
        "        self.run.log_artifact(results_artifact)\n",
        "\n",
        "def main():\n",
        "    import argparse\n",
        "\n",
        "    parser = argparse.ArgumentParser(description=\"LSTM Anomaly Detection\")\n",
        "    parser.add_argument(\"--data-path\", default=\"data/model_inputs.h5\", help=\"Path to sequence data\")\n",
        "    parser.add_argument(\"--pipeline-mode\", choices=[\"train\", \"test\"], default=\"train\",\n",
        "                       help=\"Pipeline mode: 'train' or 'test'\")\n",
        "    parser.add_argument(\"--model-path\", default=\"models/lstm_autoencoder_train.pth\",\n",
        "                       help=\"Path to pre-trained model (for test mode)\")\n",
        "    parser.add_argument(\"--epochs\", type=int, default=100, help=\"Training epochs (train mode)\")\n",
        "    parser.add_argument(\"--batch-size\", type=int, default=32, help=\"Batch size\")\n",
        "    parser.add_argument(\"--hidden-dim\", type=int, default=64, help=\"LSTM hidden dimension\")\n",
        "    parser.add_argument(\"--threshold-percentile\", type=float, default=95, help=\"Anomaly threshold percentile\")\n",
        "\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    # Initialize W&B\n",
        "    run = wandb.init(project=\"predictive-maintenance\", job_type=\"anomaly-detection\")\n",
        "\n",
        "    # Create detector with pipeline mode\n",
        "    detector = AnomalyDetector(run, pipeline_mode=args.pipeline_mode)\n",
        "\n",
        "    # Load data\n",
        "    sequences = detector.load_sequences(args.data_path)\n",
        "\n",
        "    if args.pipeline_mode == \"train\":\n",
        "        # Training mode\n",
        "        if 'X_train' not in sequences or 'X_val' not in sequences:\n",
        "            raise ValueError(\"Training data (X_train, X_val) not found in data file\")\n",
        "\n",
        "        X_train = sequences['X_train']\n",
        "        X_val = sequences['X_val']\n",
        "\n",
        "        # Create and train model\n",
        "        input_dim = X_train.shape[2]\n",
        "        config = {\n",
        "            'hidden_dim': args.hidden_dim,\n",
        "            'num_layers': 2,\n",
        "            'dropout': 0.2\n",
        "        }\n",
        "\n",
        "        detector.create_model(input_dim, config)\n",
        "        detector.train_model(X_train, X_val, epochs=args.epochs, batch_size=args.batch_size)\n",
        "\n",
        "        # Set threshold\n",
        "        detector.set_threshold(X_val, percentile=args.threshold_percentile)\n",
        "\n",
        "        # Create training visualizations\n",
        "        detector.create_training_visualizations()\n",
        "\n",
        "        # Save model\n",
        "        detector.save_model_and_results({\n",
        "            'anomalies': np.array([]),\n",
        "            'reconstruction_errors': np.array([]),\n",
        "            'timestamps': None,\n",
        "            'statistics': {'anomalies_detected': 0, 'anomaly_rate': 0.0}\n",
        "        })\n",
        "\n",
        "    elif args.pipeline_mode == \"test\":\n",
        "        # Testing mode\n",
        "        if 'X_test' not in sequences:\n",
        "            raise ValueError(\"Test data (X_test) not found in data file\")\n",
        "\n",
        "        X_test = sequences['X_test']\n",
        "        timestamps_test = sequences.get('timestamps_test', None)\n",
        "\n",
        "        # Load pre-trained model\n",
        "        detector.load_pretrained_model(args.model_path)\n",
        "\n",
        "        # Detect anomalies\n",
        "        results = detector.detect_anomalies(X_test, timestamps_test)\n",
        "\n",
        "        # Create visualizations\n",
        "        detector.create_anomaly_visualizations(results, X_test)\n",
        "        detector.create_predictions_table(results)\n",
        "        detector.create_model_performance_summary(np.array([]), np.array([]), X_test)\n",
        "\n",
        "        # Save results\n",
        "        detector.save_model_and_results(results)\n",
        "\n",
        "        # Print summary\n",
        "        print(f\"\\n🎯 ANOMALY DETECTION COMPLETE ({args.pipeline_mode.upper()} MODE)\")\n",
        "        print(f\"   Total Anomalies Detected: {results['statistics']['anomalies_detected']}\")\n",
        "        print(f\"   Anomaly Rate: {results['statistics']['anomaly_rate']:.2%}\")\n",
        "\n",
        "    run.finish()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "Zgk4DF6DRH4K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile src/monitoring/post_detection.py\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import wandb\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from typing import Dict, List\n",
        "from datetime import datetime\n",
        "\n",
        "class PostDetectionMonitor:\n",
        "    \"\"\"Post-detection monitoring and analysis with W&B tracking.\"\"\"\n",
        "\n",
        "    def __init__(self, wandb_run: wandb.run = None):\n",
        "        self.run = wandb_run or wandb.init(project=\"predictive-maintenance\")\n",
        "        self.monitoring_stats = {}\n",
        "\n",
        "    def analyze_reconstruction_errors(self, results: Dict):\n",
        "        \"\"\"Analyze reconstruction error distribution and patterns.\"\"\"\n",
        "        errors = results['reconstruction_errors']\n",
        "        anomalies = results['anomalies']\n",
        "\n",
        "        # Calculate error statistics\n",
        "        error_stats = {\n",
        "            'mean_error': float(np.mean(errors)),\n",
        "            'std_error': float(np.std(errors)),\n",
        "            'median_error': float(np.median(errors)),\n",
        "            'max_error': float(np.max(errors)),\n",
        "            'min_error': float(np.min(errors)),\n",
        "            'error_range': float(np.max(errors) - np.min(errors)),\n",
        "            'error_iqr': float(np.percentile(errors, 75) - np.percentile(errors, 25))\n",
        "        }\n",
        "\n",
        "        # Separate statistics for normal vs anomaly\n",
        "        normal_errors = errors[~anomalies]\n",
        "        anomaly_errors = errors[anomalies]\n",
        "\n",
        "        if len(normal_errors) > 0:\n",
        "            error_stats['normal_mean_error'] = float(np.mean(normal_errors))\n",
        "            error_stats['normal_std_error'] = float(np.std(normal_errors))\n",
        "            error_stats['normal_max_error'] = float(np.max(normal_errors))\n",
        "\n",
        "        if len(anomaly_errors) > 0:\n",
        "            error_stats['anomaly_mean_error'] = float(np.mean(anomaly_errors))\n",
        "            error_stats['anomaly_std_error'] = float(np.std(anomaly_errors))\n",
        "            error_stats['anomaly_min_error'] = float(np.min(anomaly_errors))\n",
        "\n",
        "        # Log statistics\n",
        "        for key, value in error_stats.items():\n",
        "            self.run.log({f\"post_detection_{key}\": value})\n",
        "\n",
        "        self.monitoring_stats.update(error_stats)\n",
        "\n",
        "        # Create reconstruction error histogram\n",
        "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))\n",
        "\n",
        "        # Overall error distribution\n",
        "        ax1.hist(errors, bins=50, alpha=0.7, color='blue', edgecolor='black')\n",
        "        ax1.axvline(np.mean(errors), color='red', linestyle='--', linewidth=2, label=f'Mean: {np.mean(errors):.2f}')\n",
        "        ax1.axvline(np.median(errors), color='green', linestyle='--', linewidth=2, label=f'Median: {np.median(errors):.2f}')\n",
        "        ax1.set_xlabel('Reconstruction Error')\n",
        "        ax1.set_ylabel('Frequency')\n",
        "        ax1.set_title('Overall Reconstruction Error Distribution')\n",
        "        ax1.legend()\n",
        "        ax1.grid(True, alpha=0.3)\n",
        "\n",
        "        # Log-scale histogram for better visibility\n",
        "        ax2.hist(errors, bins=50, alpha=0.7, color='purple', edgecolor='black')\n",
        "        ax2.set_yscale('log')\n",
        "        ax2.set_xlabel('Reconstruction Error')\n",
        "        ax2.set_ylabel('Frequency (log scale)')\n",
        "        ax2.set_title('Reconstruction Error Distribution (Log Scale)')\n",
        "        ax2.grid(True, alpha=0.3)\n",
        "\n",
        "        plt.tight_layout()\n",
        "        self.run.log({\"reconstruction_error_histograms\": wandb.Image(fig)})\n",
        "        plt.close()\n",
        "\n",
        "        # Create error percentile plot\n",
        "        fig, ax = plt.subplots(figsize=(10, 6))\n",
        "        percentiles = np.arange(0, 101, 1)\n",
        "        error_percentiles = np.percentile(errors, percentiles)\n",
        "\n",
        "        ax.plot(percentiles, error_percentiles, 'b-', linewidth=2)\n",
        "        ax.fill_between(percentiles, 0, error_percentiles, alpha=0.3)\n",
        "        ax.set_xlabel('Percentile')\n",
        "        ax.set_ylabel('Reconstruction Error')\n",
        "        ax.set_title('Reconstruction Error Percentile Distribution')\n",
        "        ax.grid(True, alpha=0.3)\n",
        "\n",
        "        # Mark key percentiles\n",
        "        key_percentiles = [50, 75, 90, 95, 99]\n",
        "        for p in key_percentiles:\n",
        "            val = np.percentile(errors, p)\n",
        "            ax.axhline(val, color='red', linestyle=':', alpha=0.5)\n",
        "            ax.text(5, val, f'P{p}: {val:.2f}', fontsize=9)\n",
        "\n",
        "        self.run.log({\"error_percentile_distribution\": wandb.Image(fig)})\n",
        "        plt.close()\n",
        "\n",
        "    def track_anomaly_patterns(self, results: Dict):\n",
        "        \"\"\"Track and visualize anomaly patterns over time.\"\"\"\n",
        "        anomalies = results['anomalies']\n",
        "        errors = results['reconstruction_errors']\n",
        "        timestamps = results.get('timestamps', None)\n",
        "\n",
        "        # Create anomaly rate tracking\n",
        "        window_size = 20  # Rolling window for anomaly rate\n",
        "        anomaly_rate_rolling = pd.Series(anomalies.astype(int)).rolling(window_size).mean()\n",
        "\n",
        "        fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(14, 10))\n",
        "\n",
        "        # Anomaly rate over time\n",
        "        x_axis = timestamps if timestamps is not None else np.arange(len(anomalies))\n",
        "\n",
        "        ax1.plot(x_axis, anomaly_rate_rolling, 'r-', linewidth=2, label=f'Rolling Anomaly Rate (window={window_size})')\n",
        "        ax1.fill_between(x_axis, 0, anomaly_rate_rolling, alpha=0.3, color='red')\n",
        "        ax1.set_ylabel('Anomaly Rate')\n",
        "        ax1.set_title('Anomaly Rate Over Time')\n",
        "        ax1.legend()\n",
        "        ax1.grid(True, alpha=0.3)\n",
        "\n",
        "        # Reconstruction error over time with anomaly markers\n",
        "        ax2.scatter(x_axis[~anomalies], errors[~anomalies], c='blue', alpha=0.5, s=20, label='Normal')\n",
        "        ax2.scatter(x_axis[anomalies], errors[anomalies], c='red', alpha=0.8, s=40, marker='x', label='Anomaly')\n",
        "        ax2.set_xlabel('Time' if timestamps is not None else 'Sequence Index')\n",
        "        ax2.set_ylabel('Reconstruction Error')\n",
        "        ax2.set_title('Reconstruction Error Timeline with Anomaly Markers')\n",
        "        ax2.legend()\n",
        "        ax2.grid(True, alpha=0.3)\n",
        "\n",
        "        plt.tight_layout()\n",
        "        self.run.log({\"anomaly_patterns_timeline\": wandb.Image(fig)})\n",
        "        plt.close()\n",
        "\n",
        "        # Calculate anomaly clustering metrics\n",
        "        if np.any(anomalies):\n",
        "            anomaly_indices = np.where(anomalies)[0]\n",
        "            if len(anomaly_indices) > 1:\n",
        "                # Calculate distances between consecutive anomalies\n",
        "                anomaly_distances = np.diff(anomaly_indices)\n",
        "\n",
        "                clustering_metrics = {\n",
        "                    'avg_anomaly_distance': float(np.mean(anomaly_distances)),\n",
        "                    'min_anomaly_distance': int(np.min(anomaly_distances)),\n",
        "                    'max_anomaly_distance': int(np.max(anomaly_distances)),\n",
        "                    'anomaly_clustering_score': float(np.std(anomaly_distances))  # Lower = more clustered\n",
        "                }\n",
        "\n",
        "                self.monitoring_stats.update(clustering_metrics)\n",
        "\n",
        "                # Log clustering metrics\n",
        "                for key, value in clustering_metrics.items():\n",
        "                    self.run.log({f\"post_detection_{key}\": value})\n",
        "\n",
        "    def create_temporal_analysis(self, results: Dict):\n",
        "        \"\"\"Analyze temporal aspects of anomalies.\"\"\"\n",
        "        anomalies = results['anomalies']\n",
        "        errors = results['reconstruction_errors']\n",
        "\n",
        "        # Create hourly aggregation if possible\n",
        "        if 'timestamps' in results and results['timestamps'] is not None:\n",
        "            timestamps = results['timestamps']\n",
        "\n",
        "            # Convert timestamps to datetime if they're numeric\n",
        "            if isinstance(timestamps[0], (int, float)):\n",
        "                timestamps = pd.to_datetime(timestamps, unit='s')\n",
        "\n",
        "            # Create dataframe for temporal analysis\n",
        "            df = pd.DataFrame({\n",
        "                'timestamp': timestamps,\n",
        "                'error': errors,\n",
        "                'is_anomaly': anomalies\n",
        "            })\n",
        "\n",
        "            # Extract temporal features\n",
        "            df['hour'] = pd.to_datetime(df['timestamp']).dt.hour\n",
        "            df['day_of_week'] = pd.to_datetime(df['timestamp']).dt.dayofweek\n",
        "\n",
        "            # Aggregate by hour\n",
        "            hourly_stats = df.groupby('hour').agg({\n",
        "                'is_anomaly': ['sum', 'mean'],\n",
        "                'error': ['mean', 'std', 'max']\n",
        "            })\n",
        "\n",
        "            # Create temporal visualization\n",
        "            fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))\n",
        "\n",
        "            # Anomaly rate by hour\n",
        "            hours = hourly_stats.index\n",
        "            anomaly_rates = hourly_stats[('is_anomaly', 'mean')]\n",
        "\n",
        "            ax1.bar(hours, anomaly_rates, color='coral', alpha=0.7)\n",
        "            ax1.set_xlabel('Hour of Day')\n",
        "            ax1.set_ylabel('Anomaly Rate')\n",
        "            ax1.set_title('Anomaly Rate by Hour of Day')\n",
        "            ax1.grid(True, alpha=0.3)\n",
        "\n",
        "            # Mean error by hour\n",
        "            mean_errors = hourly_stats[('error', 'mean')]\n",
        "            std_errors = hourly_stats[('error', 'std')]\n",
        "\n",
        "            ax2.plot(hours, mean_errors, 'b-', linewidth=2, label='Mean Error')\n",
        "            ax2.fill_between(hours,\n",
        "                            mean_errors - std_errors,\n",
        "                            mean_errors + std_errors,\n",
        "                            alpha=0.3, label='±1 STD')\n",
        "            ax2.set_xlabel('Hour of Day')\n",
        "            ax2.set_ylabel('Mean Reconstruction Error')\n",
        "            ax2.set_title('Reconstruction Error by Hour of Day')\n",
        "            ax2.legend()\n",
        "            ax2.grid(True, alpha=0.3)\n",
        "\n",
        "            plt.tight_layout()\n",
        "            self.run.log({\"temporal_analysis\": wandb.Image(fig)})\n",
        "            plt.close()\n",
        "\n",
        "    def generate_summary_report(self, results: Dict):\n",
        "        \"\"\"Generate comprehensive post-detection summary report.\"\"\"\n",
        "        # Create summary statistics table\n",
        "        summary_data = {\n",
        "            'Metric': [],\n",
        "            'Value': []\n",
        "        }\n",
        "\n",
        "        # Basic statistics\n",
        "        summary_data['Metric'].extend([\n",
        "            'Total Sequences',\n",
        "            'Anomalies Detected',\n",
        "            'Anomaly Rate (%)',\n",
        "            'Mean Reconstruction Error',\n",
        "            'Max Reconstruction Error',\n",
        "            'Threshold Used'\n",
        "        ])\n",
        "\n",
        "        summary_data['Value'].extend([\n",
        "            results['statistics']['total_samples'],\n",
        "            results['statistics']['anomalies_detected'],\n",
        "            f\"{results['statistics']['anomaly_rate'] * 100:.2f}\",\n",
        "            f\"{np.mean(results['reconstruction_errors']):.4f}\",\n",
        "            f\"{np.max(results['reconstruction_errors']):.4f}\",\n",
        "            f\"{results.get('threshold', 'N/A')}\"\n",
        "        ])\n",
        "\n",
        "        # Add monitoring statistics\n",
        "        if self.monitoring_stats:\n",
        "            for key, value in self.monitoring_stats.items():\n",
        "                if key not in ['mean_error', 'max_error']:  # Avoid duplicates\n",
        "                    metric_name = key.replace('_', ' ').title()\n",
        "                    summary_data['Metric'].append(metric_name)\n",
        "                    summary_data['Value'].append(f\"{value:.4f}\" if isinstance(value, float) else str(value))\n",
        "\n",
        "        # Create summary table\n",
        "        summary_df = pd.DataFrame(summary_data)\n",
        "        summary_table = wandb.Table(dataframe=summary_df)\n",
        "        self.run.log({\"post_detection_summary\": summary_table})\n",
        "\n",
        "        # Create final summary visualization\n",
        "        fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(14, 10))\n",
        "\n",
        "        # Error distribution comparison\n",
        "        errors = results['reconstruction_errors']\n",
        "        anomalies = results['anomalies']\n",
        "\n",
        "        if np.any(~anomalies):\n",
        "            ax1.hist(errors[~anomalies], bins=30, alpha=0.7, label='Normal', color='blue', density=True)\n",
        "        if np.any(anomalies):\n",
        "            ax1.hist(errors[anomalies], bins=30, alpha=0.7, label='Anomaly', color='red', density=True)\n",
        "        ax1.set_xlabel('Reconstruction Error')\n",
        "        ax1.set_ylabel('Density')\n",
        "        ax1.set_title('Error Distribution: Normal vs Anomaly')\n",
        "        ax1.legend()\n",
        "        ax1.grid(True, alpha=0.3)\n",
        "\n",
        "        # Anomaly detection effectiveness\n",
        "        if np.any(anomalies) and np.any(~anomalies):\n",
        "            data = [errors[~anomalies], errors[anomalies]]\n",
        "            positions = [1, 2]\n",
        "\n",
        "            bp = ax2.boxplot(data, positions=positions, widths=0.6, patch_artist=True)\n",
        "            bp['boxes'][0].set_facecolor('blue')\n",
        "            bp['boxes'][1].set_facecolor('red')\n",
        "            ax2.set_xticklabels(['Normal', 'Anomaly'])\n",
        "            ax2.set_ylabel('Reconstruction Error')\n",
        "            ax2.set_title('Error Distribution Comparison')\n",
        "            ax2.grid(True, alpha=0.3)\n",
        "\n",
        "        # Cumulative anomaly detection\n",
        "        cumulative_anomalies = np.cumsum(anomalies)\n",
        "        x_axis = np.arange(len(anomalies))\n",
        "\n",
        "        ax3.plot(x_axis, cumulative_anomalies, 'r-', linewidth=2)\n",
        "        ax3.fill_between(x_axis, 0, cumulative_anomalies, alpha=0.3, color='red')\n",
        "        ax3.set_xlabel('Sequence Index')\n",
        "        ax3.set_ylabel('Cumulative Anomalies')\n",
        "        ax3.set_title('Cumulative Anomaly Detection')\n",
        "        ax3.grid(True, alpha=0.3)\n",
        "\n",
        "        # Detection confidence distribution\n",
        "        # Use distance from threshold as confidence measure\n",
        "        threshold = results.get('threshold', np.percentile(errors, 95))\n",
        "        confidence = np.abs(errors - threshold) / threshold\n",
        "\n",
        "        ax4.hist(confidence[anomalies] if np.any(anomalies) else [],\n",
        "                bins=30, alpha=0.7, color='green', edgecolor='black')\n",
        "        ax4.set_xlabel('Detection Confidence (Relative Distance from Threshold)')\n",
        "        ax4.set_ylabel('Frequency')\n",
        "        ax4.set_title('Anomaly Detection Confidence Distribution')\n",
        "        ax4.grid(True, alpha=0.3)\n",
        "\n",
        "        plt.tight_layout()\n",
        "        self.run.log({\"post_detection_summary_plots\": wandb.Image(fig)})\n",
        "        plt.close()\n",
        "\n",
        "        # Log final metrics\n",
        "        self.run.log({\n",
        "            \"post_detection_completed\": True,\n",
        "            \"monitoring_metrics_generated\": len(self.monitoring_stats)\n",
        "        })"
      ],
      "metadata": {
        "id": "5N9wBPBmx9Eh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''%%writefile src/pipeline.py\n",
        "#!/usr/bin/env python\n",
        "\"\"\"\n",
        "End-to-end pipeline orchestration with W&B Pipelines\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "import sys\n",
        "import wandb\n",
        "import argparse\n",
        "import logging\n",
        "from pathlib import Path\n",
        "import time\n",
        "from typing import Dict, Any\n",
        "\n",
        "# Setup logging\n",
        "logging.basicConfig(\n",
        "    level=logging.INFO,\n",
        "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n",
        ")\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "class PredictiveMaintenancePipeline:\n",
        "    \"\"\"Orchestrate the complete preprocessing pipeline with W&B.\"\"\"\n",
        "\n",
        "    def __init__(self, config: Dict[str, Any]):\n",
        "        self.config = config\n",
        "        self.artifacts = {}\n",
        "        self.metrics = {}\n",
        "\n",
        "    def run_step(self, step_name: str, job_type: str, run_function, **kwargs):\n",
        "        \"\"\"Run a single pipeline step with W&B tracking.\"\"\"\n",
        "        logger.info(f\"Starting step: {step_name}\")\n",
        "\n",
        "        # Initialize W&B run for this step\n",
        "        run = wandb.init(\n",
        "            project=self.config['project'],\n",
        "            job_type=job_type,\n",
        "            name=f\"{step_name}-{int(time.time())}\",\n",
        "            reinit=True\n",
        "        )\n",
        "\n",
        "        try:\n",
        "            # Execute step\n",
        "            result = run_function(run, **kwargs)\n",
        "\n",
        "            # Store artifacts and metrics\n",
        "            if isinstance(result, dict):\n",
        "                if 'artifact' in result:\n",
        "                    self.artifacts[step_name] = result['artifact']\n",
        "                if 'metrics' in result:\n",
        "                    self.metrics[step_name] = result['metrics']\n",
        "\n",
        "            logger.info(f\"Completed step: {step_name}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error in step {step_name}: {str(e)}\")\n",
        "            wandb.alert(\n",
        "                title=f\"Pipeline Step Failed: {step_name}\",\n",
        "                text=str(e),\n",
        "                level=wandb.AlertLevel.ERROR\n",
        "            )\n",
        "            raise\n",
        "\n",
        "        finally:\n",
        "            run.finish()\n",
        "\n",
        "    def step_1_ingestion(self, run):\n",
        "        \"\"\"Step 1: Data Ingestion & Time Alignment\"\"\"\n",
        "        from src.ingestion.loader import DataLoader\n",
        "\n",
        "        loader = DataLoader(run)\n",
        "        df = loader.load_csv_files(self.config['input_files'], 'timestamp')\n",
        "        df = loader.resample_to_frequency(df, self.config['resample_freq'])\n",
        "        loader.save_and_log_artifact(df, \"data/unified.parquet\")\n",
        "\n",
        "        return {'artifact': 'raw-data:latest'}\n",
        "\n",
        "    def step_2_quality_checks(self, run):\n",
        "        \"\"\"Step 2: Initial Quality Checks\"\"\"\n",
        "        from src.cleaning.quality_checks import QualityChecker\n",
        "\n",
        "        # Load previous artifact\n",
        "        artifact = run.use_artifact(self.artifacts.get('ingestion', 'raw-data:latest'))\n",
        "        artifact_dir = artifact.download()\n",
        "        df = pd.read_parquet(f\"{artifact_dir}/unified.parquet\")\n",
        "\n",
        "        checker = QualityChecker(run)\n",
        "        limits = checker.load_limits(\"configs/limits.json\")\n",
        "        df = checker.remove_duplicates(df)\n",
        "        df = checker.filter_by_limits(df, limits)\n",
        "        df, report = checker.profile_missingness(df)\n",
        "        checker.save_quality_artifacts(df, report, \"data\")\n",
        "        df.to_parquet(\"data/cleaned.parquet\")\n",
        "\n",
        "        return {'artifact': 'quality-metrics:latest'}\n",
        "\n",
        "    def step_3_time_features(self, run):\n",
        "        \"\"\"Step 3: Timestamp & Categorical Feature Engineering\"\"\"\n",
        "        from src.cleaning.time_features import TimeFeatureEngineer\n",
        "\n",
        "        df = pd.read_parquet(\"data/cleaned.parquet\")\n",
        "        engineer = TimeFeatureEngineer(run)\n",
        "        df = engineer.add_time_features(df)\n",
        "        df.to_parquet(\"data/time_features.parquet\")\n",
        "\n",
        "        artifact = wandb.Artifact(\"time-features\", type=\"dataset\")\n",
        "        artifact.add_file(\"data/time_features.parquet\")\n",
        "        run.log_artifact(artifact)\n",
        "\n",
        "        return {'artifact': 'time-features:latest'}\n",
        "\n",
        "    def step_4_imputation(self, run):\n",
        "        \"\"\"Step 4: Missing-Value Imputation\"\"\"\n",
        "        from src.cleaning.impute import MissingValueImputer\n",
        "\n",
        "        df = pd.read_parquet(\"data/time_features.parquet\")\n",
        "        imputer = MissingValueImputer(run)\n",
        "        df = imputer.impute_missing(df, short_limit=self.config['impute_short_limit'])\n",
        "        df.to_parquet(\"data/imputed.parquet\")\n",
        "\n",
        "        artifact = wandb.Artifact(\"imputed-data\", type=\"dataset\")\n",
        "        artifact.add_file(\"data/imputed.parquet\")\n",
        "        run.log_artifact(artifact)\n",
        "\n",
        "        return {'artifact': 'imputed-data:latest'}\n",
        "\n",
        "    def step_5_outlier_flagging(self, run):\n",
        "        \"\"\"Step 5: Minimal 'Safe' Outlier Flagging\"\"\"\n",
        "        from src.cleaning.outlier_flags import OutlierFlagger\n",
        "\n",
        "        df = pd.read_parquet(\"data/imputed.parquet\")\n",
        "        flagger = OutlierFlagger(run)\n",
        "        config = flagger.load_config(\"configs/outlier_config.json\")\n",
        "        df = flagger.flag_spikes(df, config)\n",
        "        df.to_parquet(\"data/flagged.parquet\")\n",
        "\n",
        "        artifact = wandb.Artifact(\"flagged-data\", type=\"dataset\")\n",
        "        artifact.add_file(\"data/flagged.parquet\")\n",
        "        run.log_artifact(artifact)\n",
        "\n",
        "        return {'artifact': 'flagged-data:latest'}\n",
        "\n",
        "    def step_6_windowing(self, run):\n",
        "        \"\"\"Step 6: Local Windowing\"\"\"\n",
        "        from src.windowing.segment import WindowSegmenter\n",
        "\n",
        "        df = pd.read_parquet(\"data/flagged.parquet\")\n",
        "        segmenter = WindowSegmenter(run)\n",
        "        segmenter.log_sample_windows(\n",
        "            df,\n",
        "            self.config['window_size'],\n",
        "            self.config['stride'],\n",
        "            self.config['num_sample_windows']\n",
        "        )\n",
        "        segmenter.save_windows_metadata()\n",
        "\n",
        "        return {'artifact': 'windows-metadata:latest'}\n",
        "\n",
        "    def step_7_baseline_cleaning(self, run):\n",
        "        \"\"\"Step 7: Window-Level Baseline Cleaning\"\"\"\n",
        "        from src.cleaning.window_clean import WindowCleaner\n",
        "\n",
        "        df = pd.read_parquet(\"data/flagged.parquet\")\n",
        "        cleaner = WindowCleaner(run)\n",
        "        cleaned_windows, stats = cleaner.clean_baseline_windows(\n",
        "            df,\n",
        "            self.config['window_size'],\n",
        "            self.config['stride'],\n",
        "            cleaning_method=self.config['baseline_cleaning_method']\n",
        "        )\n",
        "        cleaner.save_models_and_stats()\n",
        "\n",
        "        return {'artifact': 'baseline-statistics:latest'}\n",
        "\n",
        "    def step_8_decomposition(self, run):\n",
        "        \"\"\"Step 8: Seasonal & Trend Decomposition\"\"\"\n",
        "        from src.features.decompose import TimeSeriesDecomposer\n",
        "\n",
        "        df = pd.read_parquet(\"data/flagged.parquet\")\n",
        "        decomposer = TimeSeriesDecomposer(run)\n",
        "        decompositions = decomposer.decompose_windows(\n",
        "            df,\n",
        "            self.config['decomposition_window_size'],\n",
        "            self.config['decomposition_stride'],\n",
        "            self.config['num_decomposition_examples']\n",
        "        )\n",
        "        decomposer.save_decomposition_artifacts(decompositions)\n",
        "\n",
        "        return {'artifact': 'decomposition-stats:latest'}\n",
        "\n",
        "    def step_9_feature_extraction(self, run):\n",
        "        \"\"\"Step 9: Window-Level Feature Extraction\"\"\"\n",
        "        from src.features.extract import FeatureExtractor\n",
        "\n",
        "        df = pd.read_parquet(\"data/flagged.parquet\")\n",
        "        extractor = FeatureExtractor(run)\n",
        "        feature_matrix = extractor.extract_window_features(\n",
        "            df,\n",
        "            self.config['window_size'],\n",
        "            self.config['stride']\n",
        "        )\n",
        "        extractor.save_feature_matrix(feature_matrix, \"features/windows_features.csv\")\n",
        "\n",
        "        return {'artifact': 'feature-matrix:latest'}\n",
        "\n",
        "    def step_10_scaling(self, run):\n",
        "        \"\"\"Step 10: Scaling & Normalization\"\"\"\n",
        "        from src.scaling.scale import FeatureScaler\n",
        "\n",
        "        features = pd.read_csv(\"features/windows_features.csv\")\n",
        "        n_baseline = int(len(features) * self.config['baseline_ratio'])\n",
        "        baseline_features = features.iloc[:n_baseline]\n",
        "\n",
        "        scaler = FeatureScaler(run)\n",
        "        scaler.fit_scaler(baseline_features, method=self.config['scaling_method'])\n",
        "        scaled_features = scaler.apply_scaler(features)\n",
        "        scaled_baseline = scaled_features.iloc[:n_baseline]\n",
        "\n",
        "        scaler.validate_scaling(scaled_baseline, scaled_features)\n",
        "        scaled_features.to_csv(\"features/scaled_features.csv\", index=False)\n",
        "        scaler.save_scaler()\n",
        "\n",
        "        artifact = wandb.Artifact(\"scaled-features\", type=\"dataset\")\n",
        "        artifact.add_file(\"features/scaled_features.csv\")\n",
        "        run.log_artifact(artifact)\n",
        "\n",
        "        return {'artifact': 'scaled-features:latest'}\n",
        "\n",
        "    def step_11_feature_selection(self, run):\n",
        "        \"\"\"Step 11: Dimensionality Reduction & Feature Selection\"\"\"\n",
        "        from src.features.select_reduce import FeatureSelector\n",
        "\n",
        "        features = pd.read_csv(\"features/scaled_features.csv\")\n",
        "        selector = FeatureSelector(run)\n",
        "        reduced_features = selector.combine_selection_methods(\n",
        "            features,\n",
        "            variance_threshold=self.config['variance_threshold'],\n",
        "            mi_top_k=self.config['mi_top_k'],\n",
        "            pca_variance=self.config['pca_variance']\n",
        "        )\n",
        "\n",
        "        reduced_features.to_csv(\"features/reduced_features.csv\", index=False)\n",
        "        selector.save_models()\n",
        "\n",
        "        artifact = wandb.Artifact(\"reduced-features\", type=\"dataset\")\n",
        "        artifact.add_file(\"features/reduced_features.csv\")\n",
        "        artifact.metadata = selector.selection_stats\n",
        "        run.log_artifact(artifact)\n",
        "\n",
        "        return {'artifact': 'reduced-features:latest'}\n",
        "\n",
        "    def step_12_sequence_assembly(self, run):\n",
        "        \"\"\"Step 12: Sequence Assembly & Model Input\"\"\"\n",
        "        from src.model_input.assemble import SequenceAssembler\n",
        "\n",
        "        features = pd.read_csv(\"features/reduced_features.csv\")\n",
        "        assembler = SequenceAssembler(run)\n",
        "\n",
        "        # LSTM format\n",
        "        X_lstm, timestamps = assembler.create_sequences_lstm(\n",
        "            features,\n",
        "            self.config['sequence_length'],\n",
        "            self.config['sequence_overlap']\n",
        "        )\n",
        "\n",
        "        # Tabular format\n",
        "        X_tabular = assembler.create_tabular_format(features)\n",
        "\n",
        "        # Create splits\n",
        "        lstm_splits = assembler.create_train_val_test_split(\n",
        "            X_lstm, timestamps,\n",
        "            self.config['train_ratio'],\n",
        "            self.config['val_ratio']\n",
        "        )\n",
        "\n",
        "        tabular_splits = assembler.create_train_val_test_split(\n",
        "            X_tabular,\n",
        "            train_ratio=self.config['train_ratio'],\n",
        "            val_ratio=self.config['val_ratio']\n",
        "        )\n",
        "\n",
        "        # Combine and save\n",
        "        all_sequences = {}\n",
        "        all_sequences.update(lstm_splits)\n",
        "        all_sequences.update({f\"tabular_{k}\": v for k, v in tabular_splits.items()})\n",
        "\n",
        "        assembler.save_sequences(all_sequences, \"data\")\n",
        "\n",
        "        return {'artifact': 'model-inputs-h5:latest'}\n",
        "\n",
        "    def step_13_drift_monitoring(self, run):\n",
        "        \"\"\"Step 14: Drift Monitoring & Retraining Hooks\"\"\"\n",
        "        from src.monitoring.drift import DriftMonitor\n",
        "\n",
        "        # This step would typically run on new data\n",
        "        # For pipeline demo, we'll use a portion of existing data\n",
        "        features = pd.read_csv(\"features/reduced_features.csv\")\n",
        "        n_baseline = int(len(features) * 0.7)\n",
        "        baseline_features = features.iloc[:n_baseline]\n",
        "        current_features = features.iloc[n_baseline:]\n",
        "\n",
        "        monitor = DriftMonitor(run)\n",
        "        drift_results = monitor.monitor_feature_drift(baseline_features, current_features)\n",
        "        severity_results = monitor.evaluate_drift_severity(drift_results)\n",
        "\n",
        "        monitor.create_drift_dashboard(drift_results, severity_results)\n",
        "        retrain_needed = monitor.generate_retraining_sweep(severity_results)\n",
        "        monitor.save_drift_report(drift_results, severity_results, \"monitoring\")\n",
        "\n",
        "        return {\n",
        "            'metrics': {\n",
        "                'retrain_needed': retrain_needed,\n",
        "                'high_drift_features': sum(1 for s in severity_results.values() if s == 'high')\n",
        "            }\n",
        "        }\n",
        "\n",
        "    def step_14_lstm_anomaly_detection(self, run):\n",
        "        \"\"\"Step 14: LSTM Anomaly Detection\"\"\"\n",
        "        from src.model.lstm_anomaly_detector import AnomalyDetector\n",
        "\n",
        "        # Load sequences\n",
        "        detector = AnomalyDetector(run)\n",
        "        sequences = detector.load_sequences(\"data/model_inputs.h5\")\n",
        "\n",
        "        # Get train/val/test splits\n",
        "        X_train = sequences['X_train']\n",
        "        X_val = sequences['X_val']\n",
        "        X_test = sequences['X_test']\n",
        "        timestamps_test = sequences.get('timestamps_test', None)\n",
        "\n",
        "        # Create and train model\n",
        "        input_dim = X_train.shape[2]\n",
        "        config = {\n",
        "            'hidden_dim': self.config.get('lstm_hidden_dim', 64),\n",
        "            'num_layers': self.config.get('lstm_num_layers', 2),\n",
        "            'dropout': self.config.get('lstm_dropout', 0.2),\n",
        "            'learning_rate': self.config.get('lstm_learning_rate', 0.001)\n",
        "        }\n",
        "\n",
        "        detector.create_model(input_dim, config)\n",
        "        detector.train_model(\n",
        "            X_train, X_val,\n",
        "            epochs=self.config.get('lstm_epochs', 50),\n",
        "            batch_size=self.config.get('lstm_batch_size', 32)\n",
        "        )\n",
        "\n",
        "        # Set threshold and detect anomalies\n",
        "        detector.set_threshold(X_val, percentile=self.config.get('anomaly_threshold_percentile', 95))\n",
        "        results = detector.detect_anomalies(X_test, timestamps_test)\n",
        "\n",
        "        # Create all visualizations\n",
        "        detector.create_training_visualizations()\n",
        "        detector.create_anomaly_visualizations(results, X_test)\n",
        "        detector.create_predictions_table(results, num_samples=200)\n",
        "        detector.create_model_performance_summary(X_train, X_val, X_test)\n",
        "\n",
        "        # Save model and results\n",
        "        detector.save_model_and_results(results)\n",
        "\n",
        "        # Store results\n",
        "        anomalies_detected = results['statistics']['anomalies_detected']\n",
        "        anomaly_rate = results['statistics']['anomaly_rate']\n",
        "\n",
        "        logger.info(f\"🎯 ANOMALY DETECTION COMPLETE\")\n",
        "        logger.info(f\"   Total Anomalies Detected: {anomalies_detected}\")\n",
        "        logger.info(f\"   Anomaly Rate: {anomaly_rate:.2%}\")\n",
        "\n",
        "        return {\n",
        "            'artifact': 'lstm-anomaly-detector:latest',\n",
        "            'metrics': {\n",
        "                'anomalies_detected': anomalies_detected,\n",
        "                'anomaly_rate': anomaly_rate,\n",
        "                'total_samples': results['statistics']['total_samples']\n",
        "            }\n",
        "        }\n",
        "\n",
        "    def run_pipeline(self):\n",
        "        \"\"\"Execute the complete pipeline with W&B orchestration.\"\"\"\n",
        "        logger.info(\"Starting Predictive Maintenance Pipeline\")\n",
        "        start_time = time.time()\n",
        "\n",
        "        # Create pipeline run\n",
        "        pipeline_run = wandb.init(\n",
        "            project=self.config['project'],\n",
        "            job_type=\"pipeline-orchestration\",\n",
        "            name=f\"pipeline-{int(time.time())}\",\n",
        "            config=self.config\n",
        "        )\n",
        "\n",
        "        try:\n",
        "            # Execute steps in sequence\n",
        "            steps = [\n",
        "                (\"ingestion\", \"data-ingestion\", self.step_1_ingestion),\n",
        "                (\"quality_checks\", \"quality-checks\", self.step_2_quality_checks),\n",
        "                (\"time_features\", \"feature-engineering\", self.step_3_time_features),\n",
        "                (\"imputation\", \"imputation\", self.step_4_imputation),\n",
        "                (\"outlier_flagging\", \"outlier-detection\", self.step_5_outlier_flagging),\n",
        "                (\"windowing\", \"windowing\", self.step_6_windowing),\n",
        "                (\"baseline_cleaning\", \"baseline-cleaning\", self.step_7_baseline_cleaning),\n",
        "                (\"decomposition\", \"decomposition\", self.step_8_decomposition),\n",
        "                (\"feature_extraction\", \"feature-extraction\", self.step_9_feature_extraction),\n",
        "                (\"scaling\", \"feature-scaling\", self.step_10_scaling),\n",
        "                (\"feature_selection\", \"feature-selection\", self.step_11_feature_selection),\n",
        "                (\"sequence_assembly\", \"sequence-assembly\", self.step_12_sequence_assembly),\n",
        "                (\"drift_monitoring\", \"drift-monitoring\", self.step_13_drift_monitoring),  # Step 13\n",
        "                (\"lstm_anomaly_detection\", \"anomaly-detection\", self.step_14_lstm_anomaly_detection)  # Step 14\n",
        "            ]\n",
        "\n",
        "            for step_name, job_type, step_function in steps:\n",
        "                self.run_step(step_name, job_type, step_function)\n",
        "\n",
        "            # Log pipeline summary\n",
        "            pipeline_time = time.time() - start_time\n",
        "\n",
        "            # Get anomaly detection results\n",
        "            anomaly_metrics = self.metrics.get('lstm_anomaly_detection', {})\n",
        "\n",
        "            pipeline_run.log({\n",
        "                \"pipeline_duration_minutes\": pipeline_time / 60,\n",
        "                \"pipeline_status\": \"completed\",\n",
        "                \"total_steps\": len(steps),\n",
        "                \"final_anomalies_detected\": anomaly_metrics.get('anomalies_detected', 0),\n",
        "                \"final_anomaly_rate\": anomaly_metrics.get('anomaly_rate', 0)\n",
        "            })\n",
        "\n",
        "            # Create pipeline summary table\n",
        "            summary_data = []\n",
        "            for step_name, _, _ in steps:\n",
        "                summary_data.append({\n",
        "                    'step': step_name,\n",
        "                    'artifact': self.artifacts.get(step_name, 'N/A'),\n",
        "                    'status': 'completed'\n",
        "                })\n",
        "\n",
        "            summary_table = wandb.Table(dataframe=pd.DataFrame(summary_data))\n",
        "            pipeline_run.log({\"pipeline_summary\": summary_table})\n",
        "\n",
        "            logger.info(f\"Pipeline completed successfully in {pipeline_time:.2f} seconds\")\n",
        "            logger.info(f\"🎯 Final Results: {anomaly_metrics.get('anomalies_detected', 0)} anomalies detected ({anomaly_metrics.get('anomaly_rate', 0):.2%} of test data)\")\n",
        "\n",
        "            # Check if retraining is needed\n",
        "            if self.metrics.get('drift_monitoring', {}).get('retrain_needed', False):\n",
        "                logger.warning(\"Drift detected - retraining recommended!\")\n",
        "                wandb.alert(\n",
        "                    title=\"Pipeline Complete - Retraining Needed\",\n",
        "                    text=\"High drift detected. Please review the sweep configuration and initiate retraining.\",\n",
        "                    level=wandb.AlertLevel.WARN\n",
        "                )\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Pipeline failed: {str(e)}\")\n",
        "            pipeline_run.log({\"pipeline_status\": \"failed\", \"error\": str(e)})\n",
        "            raise\n",
        "\n",
        "        finally:\n",
        "            pipeline_run.finish()\n",
        "\n",
        "        return self.artifacts, self.metrics\n",
        "\n",
        "# Default configuration\n",
        "DEFAULT_CONFIG = {\n",
        "    'project': 'predictive-maintenance',\n",
        "    'input_files': ['data/sample_sensors.csv'],\n",
        "    'resample_freq': '1T',\n",
        "    'impute_short_limit': 5,\n",
        "    'window_size': 60,\n",
        "    'stride': 30,\n",
        "    'num_sample_windows': 3,\n",
        "    'baseline_cleaning_method': 'mad',\n",
        "    'decomposition_window_size': 720,\n",
        "    'decomposition_stride': 360,\n",
        "    'num_decomposition_examples': 3,\n",
        "    'baseline_ratio': 0.2,\n",
        "    'scaling_method': 'robust',\n",
        "    'variance_threshold': 0.01,\n",
        "    'mi_top_k': 100,\n",
        "    'pca_variance': 0.95,\n",
        "    'sequence_length': 10,\n",
        "    'sequence_overlap': 0.5,\n",
        "    'train_ratio': 0.7,\n",
        "    'val_ratio': 0.15,\n",
        "    # LSTM parameters\n",
        "    'lstm_hidden_dim': 64,\n",
        "    'lstm_num_layers': 2,\n",
        "    'lstm_dropout': 0.2,\n",
        "    'lstm_learning_rate': 0.001,\n",
        "    'lstm_epochs': 50,\n",
        "    'lstm_batch_size': 32,\n",
        "    'anomaly_threshold_percentile': 95\n",
        "}\n",
        "\n",
        "def main():\n",
        "    parser = argparse.ArgumentParser(description=\"Run predictive maintenance pipeline\")\n",
        "    parser.add_argument(\"--config\", type=str, help=\"Path to config file\")\n",
        "    parser.add_argument(\"--project\", type=str, default=\"predictive-maintenance\",\n",
        "                       help=\"W&B project name\")\n",
        "    parser.add_argument(\"--dry-run\", action=\"store_true\",\n",
        "                       help=\"Run without executing steps\")\n",
        "\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    # Load configuration\n",
        "    config = DEFAULT_CONFIG.copy()\n",
        "    if args.config:\n",
        "        import yaml\n",
        "        with open(args.config, 'r') as f:\n",
        "            config.update(yaml.safe_load(f))\n",
        "\n",
        "    config['project'] = args.project\n",
        "\n",
        "    # Run pipeline\n",
        "    pipeline = PredictiveMaintenancePipeline(config)\n",
        "    artifacts, metrics = pipeline.run_pipeline()\n",
        "\n",
        "    logger.info(\"Pipeline execution complete!\")\n",
        "    logger.info(f\"Artifacts: {list(artifacts.keys())}\")\n",
        "    logger.info(f\"Metrics: {metrics}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    import pandas as pd\n",
        "    main()'''"
      ],
      "metadata": {
        "id": "Bn1_vyeVGRM1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile run_pipeline.py\n",
        "#!/usr/bin/env python\n",
        "\"\"\"\n",
        "Complete pipeline execution script for Google Colab\n",
        "Orchestrates predictive maintenance pipeline with separate train/test processing\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "import sys\n",
        "import wandb\n",
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from datetime import datetime\n",
        "import logging\n",
        "import time\n",
        "\n",
        "# Setup logging\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# Setup paths for Colab\n",
        "os.chdir('/content/predictive-maintenance')\n",
        "sys.path.append('/content/predictive-maintenance')\n",
        "\n",
        "# Import all modules\n",
        "from src.wandb_init import setup_wandb_colab, init_wandb\n",
        "from src.ingestion.loader import DataLoader\n",
        "from src.cleaning.quality_checks import QualityChecker\n",
        "from src.cleaning.time_features import TimeFeatureEngineer\n",
        "from src.cleaning.impute import MissingValueImputer\n",
        "from src.cleaning.outlier_flags import OutlierFlagger\n",
        "from src.windowing.segment import WindowSegmenter\n",
        "from src.cleaning.window_clean import WindowCleaner\n",
        "from src.features.decompose import TimeSeriesDecomposer\n",
        "from src.features.extract import FeatureExtractor\n",
        "from src.scaling.scale import FeatureScaler\n",
        "from src.features.select_reduce import FeatureSelector\n",
        "from src.model_input.assemble import SequenceAssembler\n",
        "from src.monitoring.drift import DriftMonitor\n",
        "from src.model.lstm_anomaly_detector import AnomalyDetector\n",
        "from src.monitoring.post_detection import PostDetectionMonitor\n",
        "\n",
        "def run_preprocessing_pipeline(input_file: str, pipeline_mode: str = \"train\"):\n",
        "    \"\"\"\n",
        "    Run steps 1-12 (or 1-13 for test) of the preprocessing pipeline.\n",
        "\n",
        "    Args:\n",
        "        input_file: Path to input CSV file\n",
        "        pipeline_mode: \"train\" or \"test\" to determine pipeline steps\n",
        "\n",
        "    Returns:\n",
        "        Path to the final preprocessed data\n",
        "    \"\"\"\n",
        "    logger.info(f\"\\n{'='*60}\")\n",
        "    logger.info(f\"Running preprocessing pipeline in {pipeline_mode.upper()} mode\")\n",
        "    logger.info(f\"Input file: {input_file}\")\n",
        "    logger.info(f\"{'='*60}\\n\")\n",
        "\n",
        "    # Step 1: Data Ingestion\n",
        "    logger.info(\"\\n📥 Step 1: Data Ingestion & Time Alignment\")\n",
        "    run = init_wandb(run_name=f\"step1-data-ingestion-{pipeline_mode}\")\n",
        "    loader = DataLoader(run)\n",
        "    df = loader.load_csv_files([input_file], 'timestamp')\n",
        "    df = loader.resample_to_frequency(df, '1T')\n",
        "    loader.save_and_log_artifact(df, f\"data/unified_{pipeline_mode}.parquet\")\n",
        "    run.finish()\n",
        "\n",
        "    # Step 2: Quality Checks\n",
        "    logger.info(\"\\n✅ Step 2: Initial Quality Checks\")\n",
        "    run = init_wandb(run_name=f\"step2-quality-checks-{pipeline_mode}\")\n",
        "    checker = QualityChecker(run)\n",
        "    df = pd.read_parquet(f\"data/unified_{pipeline_mode}.parquet\")\n",
        "    #limits = checker.load_limits(\"configs/limits.json\")\n",
        "    df = checker.remove_duplicates(df)\n",
        "    #df = checker.filter_by_limits(df, limits)\n",
        "    df, report = checker.profile_missingness(df)\n",
        "    checker.save_quality_artifacts(df, report, f\"data/{pipeline_mode}\")\n",
        "    df.to_parquet(f\"data/cleaned_{pipeline_mode}.parquet\")\n",
        "    run.finish()\n",
        "\n",
        "    # Step 3: Time Features\n",
        "    logger.info(\"\\n⏰ Step 3: Timestamp & Categorical Feature Engineering\")\n",
        "    run = init_wandb(run_name=f\"step3-time-features-{pipeline_mode}\")\n",
        "    engineer = TimeFeatureEngineer(run)\n",
        "    df = pd.read_parquet(f\"data/cleaned_{pipeline_mode}.parquet\")\n",
        "    df = engineer.add_time_features(df)\n",
        "    df.to_parquet(f\"data/time_features_{pipeline_mode}.parquet\")\n",
        "    artifact = wandb.Artifact(f\"time-features-{pipeline_mode}\", type=\"dataset\")\n",
        "    artifact.add_file(f\"data/time_features_{pipeline_mode}.parquet\")\n",
        "    run.log_artifact(artifact)\n",
        "    run.finish()\n",
        "\n",
        "    # Step 4: Imputation\n",
        "    logger.info(\"\\n🔧 Step 4: Missing-Value Imputation\")\n",
        "    run = init_wandb(run_name=f\"step4-imputation-{pipeline_mode}\")\n",
        "    imputer = MissingValueImputer(run)\n",
        "    df = pd.read_parquet(f\"data/time_features_{pipeline_mode}.parquet\")\n",
        "    df = imputer.impute_missing(df, short_limit=5)\n",
        "    df.to_parquet(f\"data/imputed_{pipeline_mode}.parquet\")\n",
        "    artifact = wandb.Artifact(f\"imputed-data-{pipeline_mode}\", type=\"dataset\")\n",
        "    artifact.add_file(f\"data/imputed_{pipeline_mode}.parquet\")\n",
        "    run.log_artifact(artifact)\n",
        "    run.finish()\n",
        "\n",
        "    # Step 5: Outlier Flagging\n",
        "    logger.info(\"\\n🚨 Step 5: Minimal 'Safe' Outlier Flagging\")\n",
        "    run = init_wandb(run_name=f\"step5-outlier-flagging-{pipeline_mode}\")\n",
        "    flagger = OutlierFlagger(run)\n",
        "    df = pd.read_parquet(f\"data/imputed_{pipeline_mode}.parquet\")\n",
        "    config = flagger.load_config(\"configs/outlier_config.json\")\n",
        "    df = flagger.flag_spikes(df, config)\n",
        "    df.to_parquet(f\"data/flagged_{pipeline_mode}.parquet\")\n",
        "    artifact = wandb.Artifact(f\"flagged-data-{pipeline_mode}\", type=\"dataset\")\n",
        "    artifact.add_file(f\"data/flagged_{pipeline_mode}.parquet\")\n",
        "    run.log_artifact(artifact)\n",
        "    run.finish()\n",
        "\n",
        "    # Step 6: Local Windowing\n",
        "    logger.info(\"\\n🪟 Step 6: Local Windowing\")\n",
        "    run = init_wandb(run_name=f\"step6-windowing-{pipeline_mode}\")\n",
        "    segmenter = WindowSegmenter(run)\n",
        "    df = pd.read_parquet(f\"data/flagged_{pipeline_mode}.parquet\")\n",
        "    segmenter.log_sample_windows(df, window_size=60, stride=30, num_samples=3)\n",
        "    segmenter.save_windows_metadata(f\"data/windows_metadata_{pipeline_mode}.csv\")\n",
        "    run.finish()\n",
        "\n",
        "    # Step 7: Window-Level Baseline Cleaning\n",
        "    logger.info(\"\\n🧹 Step 7: Window-Level Baseline Cleaning\")\n",
        "    run = init_wandb(run_name=f\"step7-baseline-cleaning-{pipeline_mode}\")\n",
        "    cleaner = WindowCleaner(run)\n",
        "    df = pd.read_parquet(f\"data/flagged_{pipeline_mode}.parquet\")\n",
        "    cleaned_windows, stats = cleaner.clean_baseline_windows(\n",
        "        df, window_size=60, stride=30, cleaning_method='mad'\n",
        "    )\n",
        "    cleaner.save_models_and_stats(f\"models/{pipeline_mode}\")\n",
        "    run.finish()\n",
        "\n",
        "    # Step 8: Seasonal & Trend Decomposition\n",
        "    logger.info(\"\\n📈 Step 8: Seasonal & Trend Decomposition\")\n",
        "    run = init_wandb(run_name=f\"step8-decomposition-{pipeline_mode}\")\n",
        "    decomposer = TimeSeriesDecomposer(run)\n",
        "    df = pd.read_parquet(f\"data/flagged_{pipeline_mode}.parquet\")\n",
        "    decompositions = decomposer.decompose_windows(\n",
        "        df, window_size=720, stride=360, num_examples=3\n",
        "    )\n",
        "    decomposer.save_decomposition_artifacts(decompositions, f\"features/{pipeline_mode}\")\n",
        "    run.finish()\n",
        "\n",
        "    # Step 9: Window-Level Feature Extraction\n",
        "    logger.info(\"\\n🔍 Step 9: Window-Level Feature Extraction\")\n",
        "    run = init_wandb(run_name=f\"step9-feature-extraction-{pipeline_mode}\")\n",
        "    extractor = FeatureExtractor(run)\n",
        "    df = pd.read_parquet(f\"data/flagged_{pipeline_mode}.parquet\")\n",
        "    feature_matrix = extractor.extract_window_features(df, window_size=60, stride=30)\n",
        "    extractor.save_feature_matrix(feature_matrix, f\"features/windows_features_{pipeline_mode}.csv\")\n",
        "    run.finish()\n",
        "\n",
        "    # Step 10: Scaling & Normalization\n",
        "    logger.info(\"\\n⚖️ Step 10: Scaling & Normalization\")\n",
        "    run = init_wandb(run_name=f\"step10-scaling-{pipeline_mode}\")\n",
        "    features = pd.read_csv(f\"features/windows_features_{pipeline_mode}.csv\")\n",
        "\n",
        "    if pipeline_mode == \"train\":\n",
        "        # Fit scaler on training data\n",
        "        n_baseline = int(len(features) * 0.2)\n",
        "        baseline_features = features.iloc[:n_baseline]\n",
        "        scaler = FeatureScaler(run)\n",
        "        scaler.fit_scaler(baseline_features, method='robust')\n",
        "        scaled_features = scaler.apply_scaler(features)\n",
        "        scaled_baseline = scaled_features.iloc[:n_baseline]\n",
        "        scaler.validate_scaling(scaled_baseline, scaled_features)\n",
        "        scaler.save_scaler(\"models/robust_scaler.joblib\")\n",
        "    else:\n",
        "        # Load and apply existing scaler for test data\n",
        "        import joblib\n",
        "        scaler_data = joblib.load(\"models/robust_scaler.joblib\")\n",
        "        scaler = FeatureScaler(run)\n",
        "        scaler.scaler = scaler_data['scaler']\n",
        "        scaler.feature_names = scaler_data['feature_names']\n",
        "        scaler.scaling_stats = scaler_data['scaling_stats']\n",
        "        scaled_features = scaler.apply_scaler(features)\n",
        "\n",
        "    scaled_features.to_csv(f\"features/scaled_features_{pipeline_mode}.csv\", index=False)\n",
        "    artifact = wandb.Artifact(f\"scaled-features-{pipeline_mode}\", type=\"dataset\")\n",
        "    artifact.add_file(f\"features/scaled_features_{pipeline_mode}.csv\")\n",
        "    run.log_artifact(artifact)\n",
        "    run.finish()\n",
        "\n",
        "    # Step 11: Dimensionality Reduction & Feature Selection\n",
        "    logger.info(\"\\n📐 Step 11: Dimensionality Reduction & Feature Selection\")\n",
        "    run = init_wandb(run_name=f\"step11-feature-selection-{pipeline_mode}\")\n",
        "    features = pd.read_csv(f\"features/scaled_features_{pipeline_mode}.csv\")\n",
        "    selector = FeatureSelector(run)\n",
        "\n",
        "    if pipeline_mode == \"train\":\n",
        "        # Fit selection models on training data\n",
        "        reduced_features = selector.combine_selection_methods(\n",
        "            features, variance_threshold=0.01, mi_top_k=100, pca_variance=0.95\n",
        "        )\n",
        "        selector.save_models(\"models\")\n",
        "    else:\n",
        "        # Apply existing selection models for test data\n",
        "        import joblib\n",
        "        # Apply variance selector\n",
        "        if os.path.exists(\"models/variance_selector.pkl\"):\n",
        "            selector.models['variance_selector'] = joblib.load(\"models/variance_selector.pkl\")\n",
        "        # Apply PCA\n",
        "        if os.path.exists(\"models/pca.pkl\"):\n",
        "            selector.models['pca'] = joblib.load(\"models/pca.pkl\")\n",
        "\n",
        "        # Apply transformations\n",
        "        feature_cols = [col for col in features.columns if col not in ['window_id', 'window_start']]\n",
        "        if 'variance_selector' in selector.models:\n",
        "            selected_data = selector.models['variance_selector'].transform(features[feature_cols])\n",
        "            selected_features = [feature_cols[i] for i in range(len(feature_cols))\n",
        "                               if selector.models['variance_selector'].variances_[i] > 0.01]\n",
        "            features_var_filtered = pd.DataFrame(selected_data, columns=selected_features, index=features.index)\n",
        "            for col in features.columns:\n",
        "                if col not in feature_cols:\n",
        "                    features_var_filtered[col] = features[col]\n",
        "        else:\n",
        "            features_var_filtered = features\n",
        "\n",
        "        # Apply PCA\n",
        "        if 'pca' in selector.models:\n",
        "            from sklearn.impute import SimpleImputer\n",
        "            imputer = SimpleImputer(strategy='median')\n",
        "            feature_cols = [col for col in features_var_filtered.columns if col not in ['window_id', 'window_start']]\n",
        "            features_clean = pd.DataFrame(\n",
        "                imputer.fit_transform(features_var_filtered[feature_cols]),\n",
        "                columns=feature_cols,\n",
        "                index=features_var_filtered.index\n",
        "            )\n",
        "            pca_data = selector.models['pca'].transform(features_clean)\n",
        "            pca_cols = [f'pca_{i}' for i in range(pca_data.shape[1])]\n",
        "            reduced_features = pd.DataFrame(pca_data, columns=pca_cols, index=features_var_filtered.index)\n",
        "            reduced_features['window_id'] = features_var_filtered['window_id']\n",
        "            reduced_features['window_start'] = features_var_filtered['window_start']\n",
        "        else:\n",
        "            reduced_features = features_var_filtered\n",
        "\n",
        "    reduced_features.to_csv(f\"features/reduced_features_{pipeline_mode}.csv\", index=False)\n",
        "    artifact = wandb.Artifact(f\"reduced-features-{pipeline_mode}\", type=\"dataset\")\n",
        "    artifact.add_file(f\"features/reduced_features_{pipeline_mode}.csv\")\n",
        "    run.log_artifact(artifact)\n",
        "    run.finish()\n",
        "\n",
        "    # Step 12: Sequence Assembly & Model Input\n",
        "    logger.info(\"\\n🔢 Step 12: Sequence Assembly & Model Input\")\n",
        "    run = init_wandb(run_name=f\"step12-sequence-assembly-{pipeline_mode}\")\n",
        "    features = pd.read_csv(f\"features/reduced_features_{pipeline_mode}.csv\")\n",
        "    assembler = SequenceAssembler(run)\n",
        "\n",
        "    if pipeline_mode == \"train\":\n",
        "        # For training, create only train/val split (no test)\n",
        "        X_lstm, timestamps = assembler.create_sequences_lstm(features, sequence_length=10, overlap=0.5)\n",
        "        X_tabular = assembler.create_tabular_format(features)\n",
        "\n",
        "        # Modified split for training - only train and val\n",
        "        n_samples = len(X_lstm)\n",
        "        train_ratio = 0.8\n",
        "        train_end = int(n_samples * train_ratio)\n",
        "\n",
        "        lstm_splits = {\n",
        "            'X_train': X_lstm[:train_end],\n",
        "            'X_val': X_lstm[train_end:],\n",
        "            'timestamps_train': timestamps[:train_end],\n",
        "            'timestamps_val': timestamps[train_end:]\n",
        "        }\n",
        "\n",
        "        # Log split statistics\n",
        "        run.log({\n",
        "            \"X_train_samples\": len(lstm_splits['X_train']),\n",
        "            \"X_train_shape\": str(lstm_splits['X_train'].shape),\n",
        "            \"X_val_samples\": len(lstm_splits['X_val']),\n",
        "            \"X_val_shape\": str(lstm_splits['X_val'].shape)\n",
        "        })\n",
        "\n",
        "    else:\n",
        "        # For test, create full dataset for anomaly detection\n",
        "        X_lstm, timestamps = assembler.create_sequences_lstm(features, sequence_length=10, overlap=0.5)\n",
        "        lstm_splits = {\n",
        "            'X_test': X_lstm,\n",
        "            'timestamps_test': timestamps\n",
        "        }\n",
        "\n",
        "        run.log({\n",
        "            \"X_test_samples\": len(lstm_splits['X_test']),\n",
        "            \"X_test_shape\": str(lstm_splits['X_test'].shape)\n",
        "        })\n",
        "\n",
        "    assembler.save_sequences(lstm_splits, f\"data/{pipeline_mode}\")\n",
        "    run.finish()\n",
        "\n",
        "    # Step 13: Drift Monitoring (only for test data)\n",
        "    if pipeline_mode == \"test\":\n",
        "        logger.info(\"\\n📊 Step 13: Drift Monitoring\")\n",
        "        run = init_wandb(run_name=\"step13-drift-monitoring\")\n",
        "\n",
        "        # Load train and test features for comparison\n",
        "        train_features = pd.read_csv(\"features/reduced_features_train.csv\")\n",
        "        test_features = pd.read_csv(\"features/reduced_features_test.csv\")\n",
        "\n",
        "        monitor = DriftMonitor(run)\n",
        "        drift_results = monitor.monitor_feature_drift(train_features, test_features)\n",
        "        severity_results = monitor.evaluate_drift_severity(drift_results)\n",
        "        monitor.create_drift_dashboard(drift_results, severity_results)\n",
        "        retrain_needed = monitor.generate_retraining_sweep(severity_results)\n",
        "        monitor.save_drift_report(drift_results, severity_results, \"monitoring\")\n",
        "        run.finish()\n",
        "\n",
        "    return f\"data/{pipeline_mode}/model_inputs.h5\"\n",
        "\n",
        "def train_lstm_autoencoder(train_data_path: str):\n",
        "    \"\"\"\n",
        "    Step 14: Train LSTM Autoencoder on stable training data\n",
        "    \"\"\"\n",
        "    logger.info(\"\\n🤖 Step 14: LSTM Autoencoder Training on Stable Data\")\n",
        "    run = init_wandb(run_name=\"step14-lstm-training\")\n",
        "\n",
        "    # Load sequences\n",
        "    detector = AnomalyDetector(run)\n",
        "    sequences = detector.load_sequences(train_data_path)\n",
        "\n",
        "    # Get train/val splits\n",
        "    X_train = sequences['X_train']\n",
        "    X_val = sequences['X_val']\n",
        "\n",
        "    logger.info(f\"Training data shape: {X_train.shape}\")\n",
        "    logger.info(f\"Validation data shape: {X_val.shape}\")\n",
        "\n",
        "    # Create and train model\n",
        "    input_dim = X_train.shape[2]\n",
        "    config = {\n",
        "        'hidden_dim': 64,\n",
        "        'num_layers': 2,\n",
        "        'dropout': 0.2,\n",
        "        'learning_rate': 0.001\n",
        "    }\n",
        "\n",
        "    detector.create_model(input_dim, config)\n",
        "    detector.train_model(X_train, X_val, epochs=100, batch_size=32)\n",
        "\n",
        "    # Set threshold based on validation data (stable baseline)\n",
        "    detector.set_threshold(X_val, percentile=99)  # Higher percentile for stable data\n",
        "\n",
        "    # Create training visualizations\n",
        "    detector.create_training_visualizations()\n",
        "    detector.create_model_performance_summary(X_train, X_val, X_val)  # Using val as pseudo-test\n",
        "\n",
        "    # Save model\n",
        "    import torch\n",
        "    model_path = \"models/lstm_autoencoder.pth\"\n",
        "    torch.save({\n",
        "        'model_state_dict': detector.model.state_dict(),\n",
        "        'threshold': detector.threshold,\n",
        "        'training_history': detector.training_history,\n",
        "        'config': config,\n",
        "        'input_dim': input_dim\n",
        "    }, model_path)\n",
        "\n",
        "    # Log model as artifact\n",
        "    model_artifact = wandb.Artifact(\"lstm-autoencoder-trained\", type=\"model\")\n",
        "    model_artifact.add_file(model_path)\n",
        "    model_artifact.metadata = {\n",
        "        'threshold': float(detector.threshold),\n",
        "        'input_dim': input_dim,\n",
        "        'trained_on': 'stable_data'\n",
        "    }\n",
        "    run.log_artifact(model_artifact)\n",
        "\n",
        "    logger.info(f\"Model trained and saved. Threshold: {detector.threshold:.4f}\")\n",
        "\n",
        "    run.finish()\n",
        "    return detector.threshold\n",
        "\n",
        "def detect_anomalies_on_test(test_data_path: str, model_path: str, threshold: float):\n",
        "    \"\"\"\n",
        "    Apply trained LSTM Autoencoder to detect anomalies in test data\n",
        "    \"\"\"\n",
        "    logger.info(\"\\n🔍 Step 14: Anomaly Detection on Test Data\")\n",
        "    run = init_wandb(run_name=\"step14-anomaly-detection\")\n",
        "\n",
        "    # Load sequences\n",
        "    detector = AnomalyDetector(run)\n",
        "    sequences = detector.load_sequences(test_data_path)\n",
        "    X_test = sequences['X_test']\n",
        "    timestamps_test = sequences.get('timestamps_test', None)\n",
        "\n",
        "    logger.info(f\"Test data shape: {X_test.shape}\")\n",
        "\n",
        "    # Load trained model\n",
        "    import torch\n",
        "    checkpoint = torch.load(model_path)\n",
        "\n",
        "    # Recreate model with saved configuration\n",
        "    detector.create_model(checkpoint['input_dim'], checkpoint['config'])\n",
        "    detector.model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    detector.threshold = checkpoint['threshold']\n",
        "    detector.training_history = checkpoint['training_history']\n",
        "\n",
        "    # Detect anomalies\n",
        "    results = detector.detect_anomalies(X_test, timestamps_test)\n",
        "\n",
        "    # Create comprehensive visualizations\n",
        "    detector.create_anomaly_visualizations(results, X_test)\n",
        "    detector.create_predictions_table(results, num_samples=200)\n",
        "\n",
        "    # Save results\n",
        "    results_path = \"models/anomaly_results.npz\"\n",
        "    np.savez(results_path,\n",
        "            anomalies=results['anomalies'],\n",
        "            reconstruction_errors=results['reconstruction_errors'],\n",
        "            timestamps=results['timestamps'] if results['timestamps'] is not None else np.array([]))\n",
        "\n",
        "    # Create results artifact\n",
        "    results_artifact = wandb.Artifact(\"anomaly-detection-results\", type=\"dataset\")\n",
        "    results_artifact.add_file(results_path)\n",
        "    results_artifact.metadata = {\n",
        "        'anomalies_detected': int(results['statistics']['anomalies_detected']),\n",
        "        'anomaly_rate': float(results['statistics']['anomaly_rate']),\n",
        "        'total_samples': int(results['statistics']['total_samples'])\n",
        "    }\n",
        "    run.log_artifact(results_artifact)\n",
        "\n",
        "    logger.info(f\"\\n🎯 ANOMALY DETECTION COMPLETE\")\n",
        "    logger.info(f\"   Total Anomalies Detected: {results['statistics']['anomalies_detected']}\")\n",
        "    logger.info(f\"   Anomaly Rate: {results['statistics']['anomaly_rate']:.2%}\")\n",
        "\n",
        "    run.finish()\n",
        "    return results\n",
        "\n",
        "def run_complete_pipeline():\n",
        "    \"\"\"Execute the complete preprocessing and anomaly detection pipeline.\"\"\"\n",
        "\n",
        "    logger.info(\"🚀 Starting Predictive Maintenance Pipeline\")\n",
        "    pipeline_start = time.time()\n",
        "\n",
        "    # Step 0: Setup W&B\n",
        "    logger.info(\"\\n📊 Step 0: Setting up Weights & Biases...\")\n",
        "    setup_wandb_colab()\n",
        "\n",
        "    # Check for required input files\n",
        "    if not os.path.exists('data/train.csv'):\n",
        "        raise FileNotFoundError(\"train.csv not found in data/ directory. Please provide stable sensor data for training.\")\n",
        "\n",
        "    if not os.path.exists('data/test.csv'):\n",
        "        raise FileNotFoundError(\"test.csv not found in data/ directory. Please provide test sensor data with anomalies.\")\n",
        "\n",
        "    # Phase 1: Process training data (stable baseline)\n",
        "    logger.info(\"\\n\" + \"=\"*80)\n",
        "    logger.info(\"PHASE 1: PROCESSING TRAINING DATA (STABLE BASELINE)\")\n",
        "    logger.info(\"=\"*80)\n",
        "\n",
        "    train_data_path = run_preprocessing_pipeline(\"data/train.csv\", pipeline_mode=\"train\")\n",
        "\n",
        "    # Train LSTM Autoencoder\n",
        "    threshold = train_lstm_autoencoder(train_data_path)\n",
        "\n",
        "    # Phase 2: Process test data and detect anomalies\n",
        "    logger.info(\"\\n\" + \"=\"*80)\n",
        "    logger.info(\"PHASE 2: PROCESSING TEST DATA AND DETECTING ANOMALIES\")\n",
        "    logger.info(\"=\"*80)\n",
        "\n",
        "    test_data_path = run_preprocessing_pipeline(\"data/test.csv\", pipeline_mode=\"test\")\n",
        "\n",
        "    # Detect anomalies\n",
        "    results = detect_anomalies_on_test(test_data_path, \"models/lstm_autoencoder.pth\", threshold)\n",
        "\n",
        "    # Step 15: Post-Detection Monitoring\n",
        "    logger.info(\"\\n📈 Step 15: Post-Detection Monitoring\")\n",
        "    run = init_wandb(run_name=\"step15-post-detection-monitoring\")\n",
        "\n",
        "    post_monitor = PostDetectionMonitor(run)\n",
        "    post_monitor.analyze_reconstruction_errors(results)\n",
        "    post_monitor.track_anomaly_patterns(results)\n",
        "    post_monitor.create_temporal_analysis(results)\n",
        "    post_monitor.generate_summary_report(results)\n",
        "\n",
        "    run.finish()\n",
        "\n",
        "    # Step 16: Pipeline Summary\n",
        "    logger.info(\"\\n📋 Step 16: Pipeline Summary & Orchestration\")\n",
        "    pipeline_time = time.time() - pipeline_start\n",
        "    run = init_wandb(run_name=\"step16-pipeline-summary\")\n",
        "\n",
        "    # Log pipeline metrics\n",
        "    run.log({\n",
        "        \"pipeline_duration_minutes\": pipeline_time / 60,\n",
        "        \"pipeline_status\": \"completed\",\n",
        "        \"total_steps\": 16,\n",
        "        \"final_anomalies_detected\": results['statistics']['anomalies_detected'],\n",
        "        \"final_anomaly_rate\": results['statistics']['anomaly_rate'],\n",
        "        \"anomaly_threshold\": threshold\n",
        "    })\n",
        "\n",
        "    # Create summary visualization\n",
        "    import matplotlib.pyplot as plt\n",
        "    fig, ax = plt.subplots(figsize=(12, 10))\n",
        "\n",
        "    steps = [\n",
        "        \"Data Ingestion (Train)\", \"Quality Checks (Train)\", \"Time Features (Train)\",\n",
        "        \"Imputation (Train)\", \"Outlier Flagging (Train)\", \"Windowing (Train)\",\n",
        "        \"Baseline Cleaning (Train)\", \"Decomposition (Train)\", \"Feature Extraction (Train)\",\n",
        "        \"Scaling (Train)\", \"Feature Selection (Train)\", \"Sequence Assembly (Train)\",\n",
        "        \"LSTM Training\", \"Data Ingestion (Test)\", \"Quality Checks (Test)\",\n",
        "        \"Time Features (Test)\", \"Imputation (Test)\", \"Outlier Flagging (Test)\",\n",
        "        \"Windowing (Test)\", \"Baseline Cleaning (Test)\", \"Decomposition (Test)\",\n",
        "        \"Feature Extraction (Test)\", \"Scaling (Test)\", \"Feature Selection (Test)\",\n",
        "        \"Sequence Assembly (Test)\", \"Drift Monitoring\", \"Anomaly Detection\",\n",
        "        \"Post-Detection Monitoring\", \"Pipeline Summary\"\n",
        "    ]\n",
        "\n",
        "    y_pos = np.arange(len(steps))\n",
        "    ax.barh(y_pos, [1]*len(steps), color=['blue']*13 + ['green'] + ['orange']*12 + ['red'] + ['purple'] + ['gray'])\n",
        "    ax.set_yticks(y_pos)\n",
        "    ax.set_yticklabels(steps, fontsize=8)\n",
        "    ax.set_xlabel('Status')\n",
        "    ax.set_title('Pipeline Execution Status')\n",
        "    ax.set_xlim(0, 1.2)\n",
        "\n",
        "    for i, step in enumerate(steps):\n",
        "        ax.text(1.05, i, '✓', fontsize=16, color='green', va='center')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    run.log({\"pipeline_status_chart\": wandb.Image(fig)})\n",
        "    plt.close()\n",
        "\n",
        "    run.finish()\n",
        "\n",
        "    logger.info(f\"\\n✨ Pipeline execution completed in {pipeline_time/60:.2f} minutes!\")\n",
        "    logger.info(f\"🎯 Final Results: {results['statistics']['anomalies_detected']} anomalies detected\")\n",
        "    logger.info(f\"📊 Anomaly Rate: {results['statistics']['anomaly_rate']:.2%}\")\n",
        "    logger.info(f\"🎚️ Detection Threshold: {threshold:.4f}\")\n",
        "    logger.info(\"Check your W&B dashboard for detailed metrics and artifacts.\")\n",
        "\n",
        "    return results\n",
        "\n",
        "# Execute pipeline\n",
        "if __name__ == \"__main__\":\n",
        "    # Create necessary directories\n",
        "    os.makedirs('data', exist_ok=True)\n",
        "    os.makedirs('data/train', exist_ok=True)\n",
        "    os.makedirs('data/test', exist_ok=True)\n",
        "    os.makedirs('models', exist_ok=True)\n",
        "    os.makedirs('models/train', exist_ok=True)\n",
        "    os.makedirs('models/test', exist_ok=True)\n",
        "    os.makedirs('features', exist_ok=True)\n",
        "    os.makedirs('features/train', exist_ok=True)\n",
        "    os.makedirs('features/test', exist_ok=True)\n",
        "    os.makedirs('configs', exist_ok=True)\n",
        "    os.makedirs('monitoring', exist_ok=True)\n",
        "    os.makedirs('artifacts', exist_ok=True)\n",
        "\n",
        "    # Create default configs if they don't exist\n",
        "    '''if not os.path.exists('configs/limits.json'):\n",
        "        import json\n",
        "        limits = {\n",
        "            \"sensor_0\": {\"min\": 0, \"max\": 200},\n",
        "            \"sensor_1\": {\"min\": 10, \"max\": 210},\n",
        "            \"sensor_2\": {\"min\": 20, \"max\": 220},\n",
        "            \"sensor_3\": {\"min\": 30, \"max\": 230},\n",
        "            \"sensor_4\": {\"min\": 40, \"max\": 240}\n",
        "        }\n",
        "        with open('configs/limits.json', 'w') as f:\n",
        "            json.dump(limits, f, indent=2)'''\n",
        "\n",
        "    if not os.path.exists('configs/outlier_config.json'):\n",
        "        import json\n",
        "        outlier_config = {\n",
        "            \"zscore_threshold\": 3.5,\n",
        "            \"iqr_multiplier\": 1.5,\n",
        "            \"min_spike_duration\": 1,\n",
        "            \"use_rolling_stats\": True,\n",
        "            \"rolling_window\": 60\n",
        "        }\n",
        "        with open('configs/outlier_config.json', 'w') as f:\n",
        "            json.dump(outlier_config, f, indent=2)\n",
        "\n",
        "    # Run the pipeline\n",
        "    results = run_complete_pipeline()"
      ],
      "metadata": {
        "id": "ZXsele8GG88h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!chmod +x run_pipeline.py\n",
        "!python run_pipeline.py"
      ],
      "metadata": {
        "id": "5buwMJ7i4wp-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''%%writefile test/test_pipeline_steps.py\n",
        "import pytest\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import wandb\n",
        "from unittest.mock import Mock, patch\n",
        "\n",
        "# Test data ingestion\n",
        "def test_data_loader():\n",
        "    \"\"\"Test data loading and resampling.\"\"\"\n",
        "    from src.ingestion.loader import DataLoader\n",
        "\n",
        "    # Mock W&B run\n",
        "    mock_run = Mock()\n",
        "    loader = DataLoader(mock_run)\n",
        "\n",
        "    # Create test data\n",
        "    dates = pd.date_range('2024-01-01', periods=100, freq='1T')\n",
        "    df = pd.DataFrame({\n",
        "        'timestamp': dates,\n",
        "        'sensor_0': np.random.randn(100),\n",
        "        'sensor_1': np.random.randn(100)\n",
        "    })\n",
        "\n",
        "    # Test resampling\n",
        "    df_resampled = loader.resample_to_frequency(df.set_index('timestamp'), '5T')\n",
        "\n",
        "    assert len(df_resampled) == 20  # 100 minutes / 5 minutes\n",
        "    assert mock_run.log.called\n",
        "\n",
        "# Test quality checks\n",
        "def test_quality_checker():\n",
        "    \"\"\"Test quality checking functionality.\"\"\"\n",
        "    from src.cleaning.quality_checks import QualityChecker\n",
        "\n",
        "    mock_run = Mock()\n",
        "    checker = QualityChecker(mock_run)\n",
        "\n",
        "    # Create test data with duplicates\n",
        "    df = pd.DataFrame({\n",
        "        'sensor_0': [1, 2, 2, 3, 4],\n",
        "        'sensor_1': [5, 6, 7, 8, 9]\n",
        "    }, index=pd.date_range('2024-01-01', periods=5, freq='1T'))\n",
        "\n",
        "    # Duplicate the last row\n",
        "    df = pd.concat([df, df.iloc[[-1]]])\n",
        "\n",
        "    # Test duplicate removal\n",
        "    df_clean = checker.remove_duplicates(df)\n",
        "    assert len(df_clean) == 5\n",
        "\n",
        "# Test feature extraction\n",
        "def test_feature_extractor():\n",
        "    \"\"\"Test feature extraction from windows.\"\"\"\n",
        "    from src.features.extract import FeatureExtractor\n",
        "\n",
        "    mock_run = Mock()\n",
        "    extractor = FeatureExtractor(mock_run)\n",
        "\n",
        "    # Create test window\n",
        "    window = pd.DataFrame({\n",
        "        'sensor_0': np.sin(np.linspace(0, 2*np.pi, 60)),\n",
        "        'sensor_1': np.cos(np.linspace(0, 2*np.pi, 60))\n",
        "    }, index=pd.date_range('2024-01-01', periods=60, freq='1T'))\n",
        "\n",
        "    # Extract features\n",
        "    features = extractor.extract_features(window)\n",
        "\n",
        "    # Check feature extraction\n",
        "    assert 'sensor_0_mean' in features\n",
        "    assert 'sensor_0_std' in features\n",
        "    assert 'sensor_0_dominant_freq' in features\n",
        "    assert len(features) > 20  # Should have many features\n",
        "\n",
        "# Test drift monitoring\n",
        "def test_drift_monitor():\n",
        "    \"\"\"Test drift detection functionality.\"\"\"\n",
        "    from src.monitoring.drift import DriftMonitor\n",
        "\n",
        "    mock_run = Mock()\n",
        "    monitor = DriftMonitor(mock_run)\n",
        "\n",
        "    # Create baseline and current data\n",
        "    np.random.seed(42)\n",
        "    baseline = np.random.normal(0, 1, 1000)\n",
        "    current = np.random.normal(0.5, 1.2, 1000)  # Shifted distribution\n",
        "\n",
        "    # Test PSI calculation\n",
        "    psi = monitor.compute_psi(baseline, current)\n",
        "    assert psi > 0.1  # Should detect some drift\n",
        "\n",
        "    # Test KL divergence\n",
        "    kl = monitor.compute_kl_divergence(baseline, current)\n",
        "    assert kl > 0  # Should be positive\n",
        "\n",
        "# Test W&B artifact logging\n",
        "@patch('wandb.init')\n",
        "@patch('wandb.Artifact')\n",
        "def test_wandb_integration(mock_artifact, mock_init):\n",
        "    \"\"\"Test W&B artifact creation and logging.\"\"\"\n",
        "    from src.ingestion.loader import DataLoader\n",
        "\n",
        "    mock_run = Mock()\n",
        "    mock_init.return_value = mock_run\n",
        "\n",
        "    loader = DataLoader(mock_run)\n",
        "\n",
        "    # Check artifact creation\n",
        "    df = pd.DataFrame({'a': [1, 2, 3]})\n",
        "    loader.save_and_log_artifact(df, \"test.parquet\")\n",
        "\n",
        "    assert mock_artifact.called\n",
        "    assert mock_run.log_artifact.called\n",
        "\n",
        "# Test pipeline configuration\n",
        "def test_pipeline_config():\n",
        "    \"\"\"Test pipeline configuration loading.\"\"\"\n",
        "    from src.pipeline import DEFAULT_CONFIG\n",
        "\n",
        "    assert 'window_size' in DEFAULT_CONFIG\n",
        "    assert 'stride' in DEFAULT_CONFIG\n",
        "    assert DEFAULT_CONFIG['window_size'] > 0\n",
        "    assert DEFAULT_CONFIG['train_ratio'] + DEFAULT_CONFIG['val_ratio'] < 1.0\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    pytest.main([__file__, \"-v\"])'''"
      ],
      "metadata": {
        "id": "AQ0LxYZ-MyVC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!pytest -q tests/"
      ],
      "metadata": {
        "id": "II1DBevkNEXq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}